# Providers and Models

Lectic speaks to several providers. You pick a provider and a model in your
YAML header, or let Lectic choose a default based on which API keys are in
your environment.

## Picking a default provider

If you do not set `provider`, Lectic checks for keys in this order and uses
the first one it finds:

Anthropic → Gemini → OpenAI → OpenRouter.

Set one of these environment variables before you run Lectic:

- ANTHROPIC_API_KEY
- GEMINI_API_KEY
- OPENAI_API_KEY
- OPENROUTER_API_KEY

AWS credentials for Bedrock are not used for auto‑selection. If you want
Anthropic via Bedrock, set `provider: anthropic/bedrock` explicitly and make
sure your AWS environment is configured.

## Discover models

Not sure which models are available? Run:
by running:

```bash
lectic models
```

This queries each provider you have API keys for and prints the available
models. It's the easiest way to find valid model names for your
configuration.

::: {.callout-tip}
The LSP can also autocomplete model names as you type in the YAML header.
See [Editor Integration](./03_editor_integration.qmd).
:::

## OpenAI: two provider strings

OpenAI has two modes in Lectic today.

- `openai` selects the Responses API. Choose this when you want native tools
  like search and code.
- `openai/chat` selects the legacy Chat Completions API.

## Examples

These examples show the minimal configuration for each provider. You can
omit `provider` and `model` if you want Lectic to pick defaults based on
your environment.

**Anthropic** (direct API):

```yaml
interlocutor:
  name: Assistant
  prompt: You are a helpful assistant.
  provider: anthropic
```

**Anthropic via Bedrock**:

```yaml
interlocutor:
  name: Assistant
  prompt: You are a helpful assistant.
  provider: anthropic/bedrock
  model: anthropic.claude-3-haiku-20240307-v1:0
```

**OpenAI** (Responses API—use this for native tools):

```yaml
interlocutor:
  name: Assistant
  prompt: You are a helpful assistant.
  provider: openai
```

**OpenAI Chat Completions** (legacy API):

```yaml
interlocutor:
  name: Assistant
  prompt: You are a helpful assistant.
  provider: openai/chat
```

**Gemini**:

```yaml
interlocutor:
  name: Assistant
  prompt: You are a helpful assistant.
  provider: gemini
```

**OpenRouter**:

```yaml
interlocutor:
  name: Assistant
  prompt: You are a helpful assistant.
  provider: openrouter
  model: meta-llama/llama-3.1-70b-instruct
```

**Ollama** (local inference):

```yaml
interlocutor:
  name: Assistant
  prompt: You are a helpful assistant.
  provider: ollama
  model: llama3.1
```

## Capabilities and media

Providers differ in what they accept as input. Here's a rough guide:

| Provider | Text | Images | PDFs | Audio | Video |
|----------|------|--------|------|-------|-------|
| Anthropic | ✓ | ✓ | ✓ | ✗ | ✗ |
| Gemini | ✓ | ✓ | ✓ | ✓ | ✓ |
| OpenAI | ✓ | ✓ | ✓ | ✓* | ✗ |
| OpenRouter | varies by model | | | | |
| Ollama | ✓ | varies | ✗ | ✗ | ✗ |

\* OpenAI audio requires `provider: openai/chat` with an audio-capable
model.

Support changes quickly. Consult each provider's documentation for current
limits on formats, sizes, and rate limits.

In Lectic, you attach external content by linking files in the user message
body. Lectic packages these and sends them to the provider in a way that
fits that provider's API. See [External
Content](./context_management/01_external_content.qmd) for examples and tips.
