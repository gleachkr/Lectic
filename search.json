[
  {
    "objectID": "05_providers_and_models.html",
    "href": "05_providers_and_models.html",
    "title": "Providers and Models",
    "section": "",
    "text": "Lectic speaks to several providers. You pick a provider and a model in your YAML header, or let Lectic choose a default based on which API keys are in your environment.\n\n\nIf you do not set provider, Lectic checks for keys in this order and uses the first one it finds:\nAnthropic → Gemini → OpenAI → OpenRouter.\nSet one of these environment variables before you run Lectic:\n\nANTHROPIC_API_KEY\nGEMINI_API_KEY\nOPENAI_API_KEY\nOPENROUTER_API_KEY\n\nAWS credentials for Bedrock are not used for auto‑selection. If you want Anthropic via Bedrock, set provider: anthropic/bedrock explicitly and make sure your AWS environment is configured.\n\n\n\nOpenAI has two modes in Lectic today.\n\nopenai selects the Responses API. Choose this when you want native tools like search and code.\nopenai/chat selects the legacy Chat Completions API.\n\n\n\n\nAnthropic, direct API:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic\n  model: claude-3-haiku-20240307\nAnthropic via Bedrock:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic/bedrock\n  model: anthropic.claude-3-haiku-20240307-v1:0\nOpenAI Responses API:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: openai\n  model: gpt-4o-mini\nOpenAI Chat Completions:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: openai/chat\n  model: gpt-4o-mini\nGemini:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: gemini\n  model: gemini-2.5-flash\nOpenRouter:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: openrouter\n  model: meta-llama/llama-3.1-70b-instruct\nOllama (local inference):\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: ollama\n  model: llama3.1\n\n\n\nProviders differ in what they accept as input. Most accept plain text and images. Many accept PDFs and short audio clips. Support changes quickly, so consult each provider’s documentation for current limits on formats, sizes, page counts, and rate limits.\nIn Lectic, you attach external content by linking files in the user message body. Lectic will package these and send them to the provider in a way that fits that provider’s API. See External Content for examples and tips.",
    "crumbs": [
      "Providers and Models"
    ]
  },
  {
    "objectID": "05_providers_and_models.html#picking-a-default-provider",
    "href": "05_providers_and_models.html#picking-a-default-provider",
    "title": "Providers and Models",
    "section": "",
    "text": "If you do not set provider, Lectic checks for keys in this order and uses the first one it finds:\nAnthropic → Gemini → OpenAI → OpenRouter.\nSet one of these environment variables before you run Lectic:\n\nANTHROPIC_API_KEY\nGEMINI_API_KEY\nOPENAI_API_KEY\nOPENROUTER_API_KEY\n\nAWS credentials for Bedrock are not used for auto‑selection. If you want Anthropic via Bedrock, set provider: anthropic/bedrock explicitly and make sure your AWS environment is configured.",
    "crumbs": [
      "Providers and Models"
    ]
  },
  {
    "objectID": "05_providers_and_models.html#openai-two-provider-strings",
    "href": "05_providers_and_models.html#openai-two-provider-strings",
    "title": "Providers and Models",
    "section": "",
    "text": "OpenAI has two modes in Lectic today.\n\nopenai selects the Responses API. Choose this when you want native tools like search and code.\nopenai/chat selects the legacy Chat Completions API.",
    "crumbs": [
      "Providers and Models"
    ]
  },
  {
    "objectID": "05_providers_and_models.html#examples",
    "href": "05_providers_and_models.html#examples",
    "title": "Providers and Models",
    "section": "",
    "text": "Anthropic, direct API:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic\n  model: claude-3-haiku-20240307\nAnthropic via Bedrock:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic/bedrock\n  model: anthropic.claude-3-haiku-20240307-v1:0\nOpenAI Responses API:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: openai\n  model: gpt-4o-mini\nOpenAI Chat Completions:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: openai/chat\n  model: gpt-4o-mini\nGemini:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: gemini\n  model: gemini-2.5-flash\nOpenRouter:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: openrouter\n  model: meta-llama/llama-3.1-70b-instruct\nOllama (local inference):\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: ollama\n  model: llama3.1",
    "crumbs": [
      "Providers and Models"
    ]
  },
  {
    "objectID": "05_providers_and_models.html#capabilities-and-media",
    "href": "05_providers_and_models.html#capabilities-and-media",
    "title": "Providers and Models",
    "section": "",
    "text": "Providers differ in what they accept as input. Most accept plain text and images. Many accept PDFs and short audio clips. Support changes quickly, so consult each provider’s documentation for current limits on formats, sizes, page counts, and rate limits.\nIn Lectic, you attach external content by linking files in the user message body. Lectic will package these and send them to the provider in a way that fits that provider’s API. See External Content for examples and tips.",
    "crumbs": [
      "Providers and Models"
    ]
  },
  {
    "objectID": "automation/01_macros.html",
    "href": "automation/01_macros.html",
    "title": "Automation: Macros",
    "section": "",
    "text": "Lectic supports a simple but powerful macro system that allows you to define and reuse snippets of text. This is useful for saving frequently used prompts, automating repetitive workflows, and composing complex, multi-step commands.\nMacros are defined in your YAML configuration (either in a .lec file’s header or in an included configuration file).\n\n\nMacros are defined under the macros key. Each macro must have a name and an expansion.\nmacros:\n  - name: summarize\n    expansion: &gt;\n      Please provide a concise, single-paragraph summary of our\n      conversation so far, focusing on the key decisions made and\n      conclusions reached.\n\n  - name: commit_msg\n    expansion: |\n      Please write a Conventional Commit message for the following changes:\n      :cmd[git diff --staged]\n\n\nThe expansion field can be a simple string, or it can load its content from a file or from the output of a command, just like the prompt field. For full semantics of file: and exec:, see External Prompts (file:, exec:).\n\nFile Source: expansion: file:./prompts/summarize.txt\nCommand/Script Source:\n\nSingle line: expansion: exec:get-prompt-from-db --name summarize (executed directly, not via a shell)\nMulti‑line script: start with a shebang, e.g.\nexpansion: |\n  exec:#!/usr/bin/env bash\n  echo \"Hello, ${TARGET}!\"\nMulti‑line scripts are written to a temp file and executed with the interpreter given by the shebang.\n\n\n\n\n\n\nTo use a macro, you invoke it using a directive in your message: :macro[macro_name].\nWhen Lectic processes the file, it replaces the macro directive with the full text from its expansion field before processing any other directives (like :cmd).\nThis was a long and productive discussion. Could you wrap it up?\n\n:macro[summarize]\n\n\n\nYou can pass environment variables to a macro’s expansion by adding attributes to the :macro[...] directive. These attributes are injected into the environment of exec: expansions when they run.\n\n:macro[name]{FOO=\"bar\"} sets the variable FOO to bar.\n:macro[name]{EMPTY} sets the variable EMPTY to be undefined. If you need an empty string value, write :macro[name]{EMPTY=\"\"}.\n\nNotes: - Single‑line exec: commands are not run through a shell. If you need shell features, invoke a shell explicitly, e.g., exec: bash -c 'echo \"Hello, $TARGET\"'. - In single‑line commands, variables in the command string are expanded before execution. For multi‑line scripts, variables are available to the script via the environment.\n\n\nConfiguration:\nmacros:\n  - name: greet\n    expansion: exec: bash -c 'echo \"Hello, $TARGET!\"'\nConversation:\n:macro[greet]{TARGET=\"World\"}\nWhen Lectic processes this, the :macro directive will be replaced by the output of the exec command, which is “Hello, World!”.",
    "crumbs": [
      "Automation",
      "Macros"
    ]
  },
  {
    "objectID": "automation/01_macros.html#defining-macros",
    "href": "automation/01_macros.html#defining-macros",
    "title": "Automation: Macros",
    "section": "",
    "text": "Macros are defined under the macros key. Each macro must have a name and an expansion.\nmacros:\n  - name: summarize\n    expansion: &gt;\n      Please provide a concise, single-paragraph summary of our\n      conversation so far, focusing on the key decisions made and\n      conclusions reached.\n\n  - name: commit_msg\n    expansion: |\n      Please write a Conventional Commit message for the following changes:\n      :cmd[git diff --staged]\n\n\nThe expansion field can be a simple string, or it can load its content from a file or from the output of a command, just like the prompt field. For full semantics of file: and exec:, see External Prompts (file:, exec:).\n\nFile Source: expansion: file:./prompts/summarize.txt\nCommand/Script Source:\n\nSingle line: expansion: exec:get-prompt-from-db --name summarize (executed directly, not via a shell)\nMulti‑line script: start with a shebang, e.g.\nexpansion: |\n  exec:#!/usr/bin/env bash\n  echo \"Hello, ${TARGET}!\"\nMulti‑line scripts are written to a temp file and executed with the interpreter given by the shebang.",
    "crumbs": [
      "Automation",
      "Macros"
    ]
  },
  {
    "objectID": "automation/01_macros.html#using-macros",
    "href": "automation/01_macros.html#using-macros",
    "title": "Automation: Macros",
    "section": "",
    "text": "To use a macro, you invoke it using a directive in your message: :macro[macro_name].\nWhen Lectic processes the file, it replaces the macro directive with the full text from its expansion field before processing any other directives (like :cmd).\nThis was a long and productive discussion. Could you wrap it up?\n\n:macro[summarize]",
    "crumbs": [
      "Automation",
      "Macros"
    ]
  },
  {
    "objectID": "automation/01_macros.html#passing-environment-variables-to-expansions",
    "href": "automation/01_macros.html#passing-environment-variables-to-expansions",
    "title": "Automation: Macros",
    "section": "",
    "text": "You can pass environment variables to a macro’s expansion by adding attributes to the :macro[...] directive. These attributes are injected into the environment of exec: expansions when they run.\n\n:macro[name]{FOO=\"bar\"} sets the variable FOO to bar.\n:macro[name]{EMPTY} sets the variable EMPTY to be undefined. If you need an empty string value, write :macro[name]{EMPTY=\"\"}.\n\nNotes: - Single‑line exec: commands are not run through a shell. If you need shell features, invoke a shell explicitly, e.g., exec: bash -c 'echo \"Hello, $TARGET\"'. - In single‑line commands, variables in the command string are expanded before execution. For multi‑line scripts, variables are available to the script via the environment.\n\n\nConfiguration:\nmacros:\n  - name: greet\n    expansion: exec: bash -c 'echo \"Hello, $TARGET!\"'\nConversation:\n:macro[greet]{TARGET=\"World\"}\nWhen Lectic processes this, the :macro directive will be replaced by the output of the exec command, which is “Hello, World!”.",
    "crumbs": [
      "Automation",
      "Macros"
    ]
  },
  {
    "objectID": "tools/04_mcp.html",
    "href": "tools/04_mcp.html",
    "title": "Tools: Model Context Protocol (mcp)",
    "section": "",
    "text": "Lectic can act as a client for servers that implement the Model Context Protocol (MCP). This allows you to connect your LLM to a vast and growing ecosystem of pre-built tools and services.\nYou can find lists of available servers here: - Official MCP Server List - Awesome MCP Servers\n\n\nNote: The snippets below show only the tool definition. They assume you have an interlocutor with a valid prompt and model configuration. See Getting Started for a full header example.\nYou can connect to an MCP server in three ways: by running a local server as a command, or by connecting to a remote server over WebSockets or SSE.\n\n\nThis is the most common way to run an MCP server. You provide the command to start the server, and Lectic manages its lifecycle.\ntools:\n  - name: brave\n    mcp_command: npx\n    args:\n      - \"-y\"\n      - \"@modelcontextprotocol/server-brave-search\"\n    env:\n      BRAVE_API_KEY: \"your_key_here\"\n    roots:\n      - /home/user/research-docs/\n\n\n\nYou can also connect to running MCP servers.\n\nmcp_ws: The URL for a remote server using a WebSocket connection.\nmcp_sse: The URL for a remote server using Server-Sent Events.\nmcp_shttp: The URL for a remote server using Streamable HTTP.\n\nFor example:\ntools:\n  - name: documentation_search \n    mcp_shttp: https://mcp.context7.com/mcp\n\n\n\nIf you give an MCP tool a name (e.g., name: brave), you can access any resources it provides using a special content reference syntax. The scheme is the server’s name plus the resource type.\nFor example, to access a repo resource from a server named github: [README](github+repo://gleachkr/Lectic/contents/README.md)\nThe LLM is also given a tool to list the available resources from the server.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhile powerful, the MCP protocol carries significant security risks. Treat MCP integration as a high-trust capability. Never connect to untrusted servers; a malicious server could exfiltrate data or perform unwanted actions. Lectic’s safety mechanisms reduce mistakes from a well‑behaved LLM, not attacks from a hostile server.\n\n\n\n\nJust like with the exec tool, you can specify a confirm script. This script will be executed before every tool call dispatched to the MCP server, giving you a chance to approve or deny the action.\n\n\n\nFor local mcp_command tools, you can specify a sandbox script. This script will be used to launch the MCP server process in a controlled and isolated environment, limiting its access to your system.\nSee the documentation for the Exec Tool for more details on how confirmation and sandboxing scripts work.",
    "crumbs": [
      "Tools",
      "MCP"
    ]
  },
  {
    "objectID": "tools/04_mcp.html#configuration",
    "href": "tools/04_mcp.html#configuration",
    "title": "Tools: Model Context Protocol (mcp)",
    "section": "",
    "text": "Note: The snippets below show only the tool definition. They assume you have an interlocutor with a valid prompt and model configuration. See Getting Started for a full header example.\nYou can connect to an MCP server in three ways: by running a local server as a command, or by connecting to a remote server over WebSockets or SSE.\n\n\nThis is the most common way to run an MCP server. You provide the command to start the server, and Lectic manages its lifecycle.\ntools:\n  - name: brave\n    mcp_command: npx\n    args:\n      - \"-y\"\n      - \"@modelcontextprotocol/server-brave-search\"\n    env:\n      BRAVE_API_KEY: \"your_key_here\"\n    roots:\n      - /home/user/research-docs/\n\n\n\nYou can also connect to running MCP servers.\n\nmcp_ws: The URL for a remote server using a WebSocket connection.\nmcp_sse: The URL for a remote server using Server-Sent Events.\nmcp_shttp: The URL for a remote server using Streamable HTTP.\n\nFor example:\ntools:\n  - name: documentation_search \n    mcp_shttp: https://mcp.context7.com/mcp\n\n\n\nIf you give an MCP tool a name (e.g., name: brave), you can access any resources it provides using a special content reference syntax. The scheme is the server’s name plus the resource type.\nFor example, to access a repo resource from a server named github: [README](github+repo://gleachkr/Lectic/contents/README.md)\nThe LLM is also given a tool to list the available resources from the server.",
    "crumbs": [
      "Tools",
      "MCP"
    ]
  },
  {
    "objectID": "tools/04_mcp.html#safety-and-trust",
    "href": "tools/04_mcp.html#safety-and-trust",
    "title": "Tools: Model Context Protocol (mcp)",
    "section": "",
    "text": "Warning\n\n\n\nWhile powerful, the MCP protocol carries significant security risks. Treat MCP integration as a high-trust capability. Never connect to untrusted servers; a malicious server could exfiltrate data or perform unwanted actions. Lectic’s safety mechanisms reduce mistakes from a well‑behaved LLM, not attacks from a hostile server.\n\n\n\n\nJust like with the exec tool, you can specify a confirm script. This script will be executed before every tool call dispatched to the MCP server, giving you a chance to approve or deny the action.\n\n\n\nFor local mcp_command tools, you can specify a sandbox script. This script will be used to launch the MCP server process in a controlled and isolated environment, limiting its access to your system.\nSee the documentation for the Exec Tool for more details on how confirmation and sandboxing scripts work.",
    "crumbs": [
      "Tools",
      "MCP"
    ]
  },
  {
    "objectID": "tools/01_overview.html",
    "href": "tools/01_overview.html",
    "title": "Tools Overview",
    "section": "",
    "text": "Tools let your LLM do things. Instead of stopping at text, it can run a command, query a database, call another agent, or reach out to a service.\nIn Lectic, you configure tools for each interlocutor in the YAML frontmatter describing that interlocutor. A tool call follows a four-step process:\n\nUser Prompt: You ask a question or give an instruction that requires the use of a tool (e.g., “What is the current date?”).\nLLM Tool Call: The LLM, instead of answering directly, outputs a special block of text indicating which tool it wants to use and with what arguments.\nLectic Executes: Lectic detects this tool-use block, stops the LLM, executes the specified tool, and captures its output.\nLLM Final Response: Lectic sends the tool’s output back to the LLM, which then uses that information to formulate its final answer to you.\n\n\n\nLectic uses an XML block for tool calls. This is the canonical and only supported form. The opening tag names the tool, and the block contains two children: an &lt;arguments&gt; element and a &lt;results&gt; element.\n\n&lt;tool-call with=\"NAME\"&gt; starts the block.\n&lt;arguments&gt; holds one element per parameter in the tool’s schema. If a tool takes no inputs, &lt;arguments&gt; can be empty.\n&lt;results&gt; is filled by Lectic after execution. Each tool result is serialized as a &lt;result&gt; element inside &lt;results&gt;.\n&lt;/tool-call&gt; closes the block.\n\nYou will see these blocks in the assistant’s container directive. Lectic writes the block once the model asks for a tool, then appends results after running it.\n\n\nHere is a minimal configuration and a typical exchange. The tool returns the current date via the system date command.\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic\n  model: claude-3-haiku-20240307\n  tools:\n    - exec: date\n      name: get_date\n---\n\nWhat's the date today?\nAnd here is what the assistant block might look like after Lectic runs the call and records the result.\n:::Assistant\n\nI will call the date tool first.\n\n&lt;tool-call with=\"get_date\"&gt;\n  &lt;arguments&gt;\n    &lt;arguments&gt;\n      [ ]\n    &lt;/arguments&gt;\n  &lt;/arguments&gt;\n  &lt;results&gt;\n    &lt;result type=\"text\"&gt;&lt;stdout&gt;Fri Mar 15 14:35:18 PDT 2024\\n&lt;/stdout&gt;&lt;/result&gt;\n    &lt;result type=\"text\"&gt;&lt;exitCode&gt;0&lt;/exitCode&gt;&lt;/result&gt;\n  &lt;/results&gt;\n&lt;/tool-call&gt;\n\nToday's date is March 15th, 2024.\n\n:::\n\n\n\n\nTool calls are executed in parallel. If an LLM decides to use multiple tools in a single turn, Lectic will run all of them concurrently, rather than one after another. This significantly speeds up complex tasks that involve gathering information from multiple sources.\n\n\n\nLectic supports a variety of tool types, each with its own capabilities and configuration options. Read the individual tool guides for usage details and best practices:\n\nExec: Execute shell commands and scripts.\nSQLite: Query SQLite databases.\nMCP: Connect to Model Context Protocol (MCP) servers.\nAgent: Allow one interlocutor to call another as a tool.\nOther Tools: Includes think, serve, and native provider tools.",
    "crumbs": [
      "Tools",
      "Overview"
    ]
  },
  {
    "objectID": "tools/01_overview.html#tool-call-syntax",
    "href": "tools/01_overview.html#tool-call-syntax",
    "title": "Tools Overview",
    "section": "",
    "text": "Lectic uses an XML block for tool calls. This is the canonical and only supported form. The opening tag names the tool, and the block contains two children: an &lt;arguments&gt; element and a &lt;results&gt; element.\n\n&lt;tool-call with=\"NAME\"&gt; starts the block.\n&lt;arguments&gt; holds one element per parameter in the tool’s schema. If a tool takes no inputs, &lt;arguments&gt; can be empty.\n&lt;results&gt; is filled by Lectic after execution. Each tool result is serialized as a &lt;result&gt; element inside &lt;results&gt;.\n&lt;/tool-call&gt; closes the block.\n\nYou will see these blocks in the assistant’s container directive. Lectic writes the block once the model asks for a tool, then appends results after running it.\n\n\nHere is a minimal configuration and a typical exchange. The tool returns the current date via the system date command.\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic\n  model: claude-3-haiku-20240307\n  tools:\n    - exec: date\n      name: get_date\n---\n\nWhat's the date today?\nAnd here is what the assistant block might look like after Lectic runs the call and records the result.\n:::Assistant\n\nI will call the date tool first.\n\n&lt;tool-call with=\"get_date\"&gt;\n  &lt;arguments&gt;\n    &lt;arguments&gt;\n      [ ]\n    &lt;/arguments&gt;\n  &lt;/arguments&gt;\n  &lt;results&gt;\n    &lt;result type=\"text\"&gt;&lt;stdout&gt;Fri Mar 15 14:35:18 PDT 2024\\n&lt;/stdout&gt;&lt;/result&gt;\n    &lt;result type=\"text\"&gt;&lt;exitCode&gt;0&lt;/exitCode&gt;&lt;/result&gt;\n  &lt;/results&gt;\n&lt;/tool-call&gt;\n\nToday's date is March 15th, 2024.\n\n:::",
    "crumbs": [
      "Tools",
      "Overview"
    ]
  },
  {
    "objectID": "tools/01_overview.html#parallel-tool-execution",
    "href": "tools/01_overview.html#parallel-tool-execution",
    "title": "Tools Overview",
    "section": "",
    "text": "Tool calls are executed in parallel. If an LLM decides to use multiple tools in a single turn, Lectic will run all of them concurrently, rather than one after another. This significantly speeds up complex tasks that involve gathering information from multiple sources.",
    "crumbs": [
      "Tools",
      "Overview"
    ]
  },
  {
    "objectID": "tools/01_overview.html#available-tool-types",
    "href": "tools/01_overview.html#available-tool-types",
    "title": "Tools Overview",
    "section": "",
    "text": "Lectic supports a variety of tool types, each with its own capabilities and configuration options. Read the individual tool guides for usage details and best practices:\n\nExec: Execute shell commands and scripts.\nSQLite: Query SQLite databases.\nMCP: Connect to Model Context Protocol (MCP) servers.\nAgent: Allow one interlocutor to call another as a tool.\nOther Tools: Includes think, serve, and native provider tools.",
    "crumbs": [
      "Tools",
      "Overview"
    ]
  },
  {
    "objectID": "tools/03_sqlite.html",
    "href": "tools/03_sqlite.html",
    "title": "Tools: SQLite Query",
    "section": "",
    "text": "The sqlite tool gives your LLM the ability to query SQLite databases directly. This is a powerful way to provide access to structured data, allowing the LLM to perform data analysis, answer questions from a knowledge base, or check the state of an application.\n\n\nThe snippets below show only the tool definition. They assume you have an interlocutor with a valid prompt and model configuration. See Getting Started for a full header example.\nTo configure the tool, you must provide the path to the SQLite database file. The database schema is automatically introspected and provided to the LLM, so it knows what tables and columns are available.\ntools:\n  - sqlite: ./products.db\n    name: db_query\n    limit: 10000\n    details: &gt;\n      Contains the full product catalog and inventory levels. Use this to\n      answer questions about what is in stock.\n    extensions:\n      - ./lib/vector0\n      - ./lib/math\nThe path can include environment variables (for example, $DATA_DIR/main.db), which Lectic expands.\n\n\n\nsqlite: (Required) Path to the SQLite database file.\nname: A custom tool name.\nlimit: Maximum size of the serialized response in bytes. Large results raise an error instead of flooding the model.\ndetails: Extra high‑level context for the model. String, file:, or exec: are accepted. See External Prompts (file:, exec:).\nextensions: A list of SQLite extension libraries to load before queries.\n\n\n\n\nConfiguration:\ntools:\n  - sqlite: ./chinook.db\n    name: chinook\nConversation:\nWho are the top 5 artists by number of tracks?\n\n:::Assistant\n\nI will query the database to find out.\n\n&lt;tool-call with=\"chinook\"&gt;\n  &lt;arguments&gt;\n    &lt;query&gt;\nSELECT\n  ar.Name,\n  COUNT(t.TrackId) AS TrackCount\nFROM Artists ar\nJOIN Albums al ON ar.ArtistId = al.ArtistId\nJOIN Tracks t ON al.AlbumId = t.AlbumId\nGROUP BY ar.Name\nORDER BY TrackCount DESC\nLIMIT 5;\n    &lt;/query&gt;\n  &lt;/arguments&gt;\n  &lt;results&gt;\n    &lt;result type=\"text\"&gt;- Name: Iron Maiden\n  TrackCount: 213\n- Name: Led Zeppelin\n  TrackCount: 114\n- Name: Metallica\n  TrackCount: 112\n- Name: U2\n  TrackCount: 110\n- Name: Deep Purple\n  TrackCount: 92\n&lt;/result&gt;\n  &lt;/results&gt;\n&lt;/tool-call&gt;\n\nBased on the data, the top artists by track count are Iron Maiden, Led\nZeppelin, Metallica, U2, and Deep Purple.\n\n:::\n\n\n\n\nWrites are allowed by default. Each tool call runs inside a transaction and is atomic. If any statement in the call fails, Lectic rolls back the entire call, so the database is unchanged.\n\n\n\nThe limit parameter caps the size of the serialized YAML that Lectic returns. If a result exceeds the cap, the tool raises an error. Tighten your query (for example, add LIMIT or select fewer columns) to stay under the cap.\n\n\n\nYou can load extensions by path before queries run. On macOS, note that the system SQLite build may restrict loading extensions. Consult the Bun SQLite extension documentation if you hit issues.",
    "crumbs": [
      "Tools",
      "SQLite"
    ]
  },
  {
    "objectID": "tools/03_sqlite.html#configuration",
    "href": "tools/03_sqlite.html#configuration",
    "title": "Tools: SQLite Query",
    "section": "",
    "text": "The snippets below show only the tool definition. They assume you have an interlocutor with a valid prompt and model configuration. See Getting Started for a full header example.\nTo configure the tool, you must provide the path to the SQLite database file. The database schema is automatically introspected and provided to the LLM, so it knows what tables and columns are available.\ntools:\n  - sqlite: ./products.db\n    name: db_query\n    limit: 10000\n    details: &gt;\n      Contains the full product catalog and inventory levels. Use this to\n      answer questions about what is in stock.\n    extensions:\n      - ./lib/vector0\n      - ./lib/math\nThe path can include environment variables (for example, $DATA_DIR/main.db), which Lectic expands.\n\n\n\nsqlite: (Required) Path to the SQLite database file.\nname: A custom tool name.\nlimit: Maximum size of the serialized response in bytes. Large results raise an error instead of flooding the model.\ndetails: Extra high‑level context for the model. String, file:, or exec: are accepted. See External Prompts (file:, exec:).\nextensions: A list of SQLite extension libraries to load before queries.\n\n\n\n\nConfiguration:\ntools:\n  - sqlite: ./chinook.db\n    name: chinook\nConversation:\nWho are the top 5 artists by number of tracks?\n\n:::Assistant\n\nI will query the database to find out.\n\n&lt;tool-call with=\"chinook\"&gt;\n  &lt;arguments&gt;\n    &lt;query&gt;\nSELECT\n  ar.Name,\n  COUNT(t.TrackId) AS TrackCount\nFROM Artists ar\nJOIN Albums al ON ar.ArtistId = al.ArtistId\nJOIN Tracks t ON al.AlbumId = t.AlbumId\nGROUP BY ar.Name\nORDER BY TrackCount DESC\nLIMIT 5;\n    &lt;/query&gt;\n  &lt;/arguments&gt;\n  &lt;results&gt;\n    &lt;result type=\"text\"&gt;- Name: Iron Maiden\n  TrackCount: 213\n- Name: Led Zeppelin\n  TrackCount: 114\n- Name: Metallica\n  TrackCount: 112\n- Name: U2\n  TrackCount: 110\n- Name: Deep Purple\n  TrackCount: 92\n&lt;/result&gt;\n  &lt;/results&gt;\n&lt;/tool-call&gt;\n\nBased on the data, the top artists by track count are Iron Maiden, Led\nZeppelin, Metallica, U2, and Deep Purple.\n\n:::",
    "crumbs": [
      "Tools",
      "SQLite"
    ]
  },
  {
    "objectID": "tools/03_sqlite.html#writes-and-transactions",
    "href": "tools/03_sqlite.html#writes-and-transactions",
    "title": "Tools: SQLite Query",
    "section": "",
    "text": "Writes are allowed by default. Each tool call runs inside a transaction and is atomic. If any statement in the call fails, Lectic rolls back the entire call, so the database is unchanged.",
    "crumbs": [
      "Tools",
      "SQLite"
    ]
  },
  {
    "objectID": "tools/03_sqlite.html#limits-and-large-results",
    "href": "tools/03_sqlite.html#limits-and-large-results",
    "title": "Tools: SQLite Query",
    "section": "",
    "text": "The limit parameter caps the size of the serialized YAML that Lectic returns. If a result exceeds the cap, the tool raises an error. Tighten your query (for example, add LIMIT or select fewer columns) to stay under the cap.",
    "crumbs": [
      "Tools",
      "SQLite"
    ]
  },
  {
    "objectID": "tools/03_sqlite.html#extensions",
    "href": "tools/03_sqlite.html#extensions",
    "title": "Tools: SQLite Query",
    "section": "",
    "text": "You can load extensions by path before queries run. On macOS, note that the system SQLite build may restrict loading extensions. Consult the Bun SQLite extension documentation if you hit issues.",
    "crumbs": [
      "Tools",
      "SQLite"
    ]
  },
  {
    "objectID": "reference/02_configuration.html",
    "href": "reference/02_configuration.html",
    "title": "Reference: Configuration Keys",
    "section": "",
    "text": "This document provides a reference for all the keys available in Lectic’s YAML configuration, including the main .lec file frontmatter and any included configuration files.\n\n\n\ninterlocutor: A single object defining the primary LLM speaker.\ninterlocutors: A list of interlocutor objects for multiparty conversations.\nmacros: A list of macro definitions. See Macros.\nhooks: A list of hook definitions. See Hooks.\n\n\n\n\n\nAn interlocutor object defines a single LLM “personality” or configuration.\n\nname: (Required) The name of the speaker, used in the :::Name response blocks.\nprompt: (Required) The base system prompt that defines the LLM’s personality and instructions. The value can be a string, or it can be loaded from a file (file:./path.txt) or a command (exec:get-prompt). See External Sources (file:, exec:) for details and examples.\n\n\n\n\nprovider: The LLM provider to use. Supported values include anthropic, anthropic/bedrock, openai (Responses API), openai/chat (legacy Chat Completions), gemini, ollama, and openrouter.\nmodel: The specific model to use, e.g., claude-3-opus-20240229.\ntemperature: A number between 0 and 1 controlling the randomness of the output.\nmax_tokens: The maximum number of tokens to generate in a response.\nmax_tool_use: The maximum number of tool calls the LLM is allowed to make in a single turn.\n\n\n\nIf you don’t specify provider, Lectic picks a default based on your environment. It checks for known API keys in this order and uses the first one it finds:\n\nANTHROPIC_API_KEY\nGEMINI_API_KEY\nOPENAI_API_KEY\nOPENROUTER_API_KEY\n\nAWS credentials for Bedrock are not considered for auto‑selection. If you want Anthropic via Bedrock, set provider: anthropic/bedrock explicitly and ensure your AWS environment is configured.\nOpenAI has two provider options:\n\nopenai uses the Responses API. You’ll want this for native tools like search and code.\nopenai/chat uses the legacy Chat Completions API. You’ll need this for certain audio workflows that still require chat‑style models.\n\nFor a more detailed discussion of provider and model options, see Providers and Models.\n\n\n\n\n\nreminder: A string that is invisibly added to the user’s message on every turn. Useful for reinforcing key instructions without cluttering the conversation history.\n\n\n\n\n\ntools: A list of tool definitions that this interlocutor can use. The format of each object in the list depends on the tool type. See the Tools section for detailed configuration guides for each tool. ` —\n\n\n\n\n\n\nname: (Required) The name of the macro, used when invoking it with :macro[name].\nexpansion: (Required) The content to be expanded. Can be a string, or loaded via file: or exec: (see\n\n\n\n\n\n\non: (Required) A single event name or a list of event names to trigger the hook. Supported events are user_message, assistant_message, and error.\ndo: (Required) The command or inline script to run when the event occurs. If multi‑line, it must start with a shebang (e.g., #!/bin/bash). Event context is provided as environment variables. See the Hooks guide for details.",
    "crumbs": [
      "Reference",
      "Configuration"
    ]
  },
  {
    "objectID": "reference/02_configuration.html#top-level-keys",
    "href": "reference/02_configuration.html#top-level-keys",
    "title": "Reference: Configuration Keys",
    "section": "",
    "text": "interlocutor: A single object defining the primary LLM speaker.\ninterlocutors: A list of interlocutor objects for multiparty conversations.\nmacros: A list of macro definitions. See Macros.\nhooks: A list of hook definitions. See Hooks.",
    "crumbs": [
      "Reference",
      "Configuration"
    ]
  },
  {
    "objectID": "reference/02_configuration.html#the-interlocutor-object",
    "href": "reference/02_configuration.html#the-interlocutor-object",
    "title": "Reference: Configuration Keys",
    "section": "",
    "text": "An interlocutor object defines a single LLM “personality” or configuration.\n\nname: (Required) The name of the speaker, used in the :::Name response blocks.\nprompt: (Required) The base system prompt that defines the LLM’s personality and instructions. The value can be a string, or it can be loaded from a file (file:./path.txt) or a command (exec:get-prompt). See External Sources (file:, exec:) for details and examples.\n\n\n\n\nprovider: The LLM provider to use. Supported values include anthropic, anthropic/bedrock, openai (Responses API), openai/chat (legacy Chat Completions), gemini, ollama, and openrouter.\nmodel: The specific model to use, e.g., claude-3-opus-20240229.\ntemperature: A number between 0 and 1 controlling the randomness of the output.\nmax_tokens: The maximum number of tokens to generate in a response.\nmax_tool_use: The maximum number of tool calls the LLM is allowed to make in a single turn.\n\n\n\nIf you don’t specify provider, Lectic picks a default based on your environment. It checks for known API keys in this order and uses the first one it finds:\n\nANTHROPIC_API_KEY\nGEMINI_API_KEY\nOPENAI_API_KEY\nOPENROUTER_API_KEY\n\nAWS credentials for Bedrock are not considered for auto‑selection. If you want Anthropic via Bedrock, set provider: anthropic/bedrock explicitly and ensure your AWS environment is configured.\nOpenAI has two provider options:\n\nopenai uses the Responses API. You’ll want this for native tools like search and code.\nopenai/chat uses the legacy Chat Completions API. You’ll need this for certain audio workflows that still require chat‑style models.\n\nFor a more detailed discussion of provider and model options, see Providers and Models.\n\n\n\n\n\nreminder: A string that is invisibly added to the user’s message on every turn. Useful for reinforcing key instructions without cluttering the conversation history.\n\n\n\n\n\ntools: A list of tool definitions that this interlocutor can use. The format of each object in the list depends on the tool type. See the Tools section for detailed configuration guides for each tool. ` —",
    "crumbs": [
      "Reference",
      "Configuration"
    ]
  },
  {
    "objectID": "reference/02_configuration.html#the-macro-object",
    "href": "reference/02_configuration.html#the-macro-object",
    "title": "Reference: Configuration Keys",
    "section": "",
    "text": "name: (Required) The name of the macro, used when invoking it with :macro[name].\nexpansion: (Required) The content to be expanded. Can be a string, or loaded via file: or exec: (see",
    "crumbs": [
      "Reference",
      "Configuration"
    ]
  },
  {
    "objectID": "reference/02_configuration.html#the-hook-object",
    "href": "reference/02_configuration.html#the-hook-object",
    "title": "Reference: Configuration Keys",
    "section": "",
    "text": "on: (Required) A single event name or a list of event names to trigger the hook. Supported events are user_message, assistant_message, and error.\ndo: (Required) The command or inline script to run when the event occurs. If multi‑line, it must start with a shebang (e.g., #!/bin/bash). Event context is provided as environment variables. See the Hooks guide for details.",
    "crumbs": [
      "Reference",
      "Configuration"
    ]
  },
  {
    "objectID": "04_configuration.html",
    "href": "04_configuration.html",
    "title": "Lectic Configuration",
    "section": "",
    "text": "Lectic offers a flexible configuration system that lets you set global defaults, create per-project settings, and make conversation-specific overrides. This is managed through a hierarchy of YAML files.\n\n\nConfiguration settings are merged from multiple sources. Each source in the list below overrides the one before it, with the .lec file’s own header always having the final say.\n\nSystem Config Directory: Lectic first looks for a configuration file at lectic/lectic.yaml within your system’s standard config location (e.g., ~/.config/lectic/lectic.yaml on Linux). This is the ideal place for your global, user-level defaults.\nWorking Directory: Next, it looks for a lectic.yaml file in the current working directory. This is useful for project-level configuration that you might commit to a git repository.\n--Include (-I) Flag: You can specify an arbitrary YAML file to include using the --Include or -I command-line flag. This is handy for temporary or experimental settings. bash     lectic -I project_settings.yaml -f my_convo.lec\nLectic File Header: The YAML frontmatter within your .lec file is the final and highest-precedence source of configuration.\n\n\n\n\nYou can change the default locations for Lectic’s data directories by setting the following environment variables:\n\n$LECTIC_CONFIG: Overrides the base configuration directory path.\n$LECTIC_DATA: Overrides the data directory path.\n$LECTIC_CACHE: Overrides the cache directory path.\n$LECTIC_STATE: Overrides the state directory path.\n\nThese variables, along with $LECTIC_TEMP (a temporary directory) and $LECTIC_FILE (the path to the active .lec file), are automatically passed into the environment of any subprocesses Lectic spawns, such as exec tools. This ensures your custom scripts have access to the same context as the main process.\n\n\n\nWhen combining settings from multiple configuration files, Lectic follows specific rules:\n\nObjects (Mappings): Merged recursively. If a key exists in multiple sources, the value from the source with higher precedence wins.\nArrays (Lists): Merged based on the name attribute of their elements. If two objects in an array share the same name, they are merged. Otherwise, the elements are simply combined. This is especially useful for managing lists of tools and interlocutors.\nOther Values: For simple values (strings, numbers) or if the types don’t match, the value from the highest-precedence source is used without any merging.\n\n\n\nImagine you have a global config in ~/.config/lectic/lectic.yaml:\n# ~/.config/lectic/lectic.yaml\ninterlocutors:\n    - name: opus\n      provider: anthropic\n      model: claude-3-opus-20240229\nAnd a project-specific file, project.yaml:\n# ./project.yaml\ninterlocutor:\n    name: haiku\n    model: claude-3-haiku-20240307\n    tools:\n        - exec: bash\n        - agent: opus\nIf you run lectic -I project.yaml ..., the haiku interlocutor will be configured with the claude-3-haiku model, and it will have access to a bash tool and an agent tool that can call opus. You could then switch interlocutors to opus within the conversation if needed.",
    "crumbs": [
      "Configuration"
    ]
  },
  {
    "objectID": "04_configuration.html#configuration-hierarchy",
    "href": "04_configuration.html#configuration-hierarchy",
    "title": "Lectic Configuration",
    "section": "",
    "text": "Configuration settings are merged from multiple sources. Each source in the list below overrides the one before it, with the .lec file’s own header always having the final say.\n\nSystem Config Directory: Lectic first looks for a configuration file at lectic/lectic.yaml within your system’s standard config location (e.g., ~/.config/lectic/lectic.yaml on Linux). This is the ideal place for your global, user-level defaults.\nWorking Directory: Next, it looks for a lectic.yaml file in the current working directory. This is useful for project-level configuration that you might commit to a git repository.\n--Include (-I) Flag: You can specify an arbitrary YAML file to include using the --Include or -I command-line flag. This is handy for temporary or experimental settings. bash     lectic -I project_settings.yaml -f my_convo.lec\nLectic File Header: The YAML frontmatter within your .lec file is the final and highest-precedence source of configuration.",
    "crumbs": [
      "Configuration"
    ]
  },
  {
    "objectID": "04_configuration.html#overriding-default-directories",
    "href": "04_configuration.html#overriding-default-directories",
    "title": "Lectic Configuration",
    "section": "",
    "text": "You can change the default locations for Lectic’s data directories by setting the following environment variables:\n\n$LECTIC_CONFIG: Overrides the base configuration directory path.\n$LECTIC_DATA: Overrides the data directory path.\n$LECTIC_CACHE: Overrides the cache directory path.\n$LECTIC_STATE: Overrides the state directory path.\n\nThese variables, along with $LECTIC_TEMP (a temporary directory) and $LECTIC_FILE (the path to the active .lec file), are automatically passed into the environment of any subprocesses Lectic spawns, such as exec tools. This ensures your custom scripts have access to the same context as the main process.",
    "crumbs": [
      "Configuration"
    ]
  },
  {
    "objectID": "04_configuration.html#merging-logic",
    "href": "04_configuration.html#merging-logic",
    "title": "Lectic Configuration",
    "section": "",
    "text": "When combining settings from multiple configuration files, Lectic follows specific rules:\n\nObjects (Mappings): Merged recursively. If a key exists in multiple sources, the value from the source with higher precedence wins.\nArrays (Lists): Merged based on the name attribute of their elements. If two objects in an array share the same name, they are merged. Otherwise, the elements are simply combined. This is especially useful for managing lists of tools and interlocutors.\nOther Values: For simple values (strings, numbers) or if the types don’t match, the value from the highest-precedence source is used without any merging.\n\n\n\nImagine you have a global config in ~/.config/lectic/lectic.yaml:\n# ~/.config/lectic/lectic.yaml\ninterlocutors:\n    - name: opus\n      provider: anthropic\n      model: claude-3-opus-20240229\nAnd a project-specific file, project.yaml:\n# ./project.yaml\ninterlocutor:\n    name: haiku\n    model: claude-3-haiku-20240307\n    tools:\n        - exec: bash\n        - agent: opus\nIf you run lectic -I project.yaml ..., the haiku interlocutor will be configured with the claude-3-haiku model, and it will have access to a bash tool and an agent tool that can call opus. You could then switch interlocutors to opus within the conversation if needed.",
    "crumbs": [
      "Configuration"
    ]
  },
  {
    "objectID": "01_introduction.html",
    "href": "01_introduction.html",
    "title": "Introduction to Lectic",
    "section": "",
    "text": "Lectic is a conversational LLM client designed for users who are most comfortable working with text files and command-line tools. It treats every conversation as a simple markdown file, a design choice that unlocks a surprising number of benefits.\n\n\nBy making conversations plain text, Lectic ensures they are:\n\nPersistent by Default: Your conversations are saved just like any other document. There is no proprietary database or cloud service.\nVersion Controllable: You can use git and other version control systems to track the history of a conversation, branch off new ideas, and merge different lines of thought.\nSearchable and Portable: Standard tools like grep, sed, and awk can be used to search, analyze, and modify your conversations. They are easy to back up, sync, and move between machines.\nPrivate: Your conversations can stay entirely on your local machine if you are using a local LLM provider like Ollama.\n\n\n\n\nLectic is built to support deep, thoughtful work. It is for anyone who wants to use LLMs as a tool for:\n\nResearch and Analysis: Integrate data, run analyses, and discuss the results with an LLM, all within a single, reproducible document.\nReflection and Self-Study: Keep a running journal of your thoughts and explorations on a topic, using an LLM as a Socratic partner.\nDesign and Engineering: Draft code, review diffs, and document your process in a way that can be committed directly into your project’s repository.\n\nLectic makes it easy to manage conversational context using content references, integrate with external tools (for search, computation, database access, and more), and include multiple LLMs in a single conversation to bring a variety of perspectives to bear on a problem.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01_introduction.html#the-power-of-plain-text",
    "href": "01_introduction.html#the-power-of-plain-text",
    "title": "Introduction to Lectic",
    "section": "",
    "text": "By making conversations plain text, Lectic ensures they are:\n\nPersistent by Default: Your conversations are saved just like any other document. There is no proprietary database or cloud service.\nVersion Controllable: You can use git and other version control systems to track the history of a conversation, branch off new ideas, and merge different lines of thought.\nSearchable and Portable: Standard tools like grep, sed, and awk can be used to search, analyze, and modify your conversations. They are easy to back up, sync, and move between machines.\nPrivate: Your conversations can stay entirely on your local machine if you are using a local LLM provider like Ollama.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01_introduction.html#who-is-lectic-for",
    "href": "01_introduction.html#who-is-lectic-for",
    "title": "Introduction to Lectic",
    "section": "",
    "text": "Lectic is built to support deep, thoughtful work. It is for anyone who wants to use LLMs as a tool for:\n\nResearch and Analysis: Integrate data, run analyses, and discuss the results with an LLM, all within a single, reproducible document.\nReflection and Self-Study: Keep a running journal of your thoughts and explorations on a topic, using an LLM as a Socratic partner.\nDesign and Engineering: Draft code, review diffs, and document your process in a way that can be committed directly into your project’s repository.\n\nLectic makes it easy to manage conversational context using content references, integrate with external tools (for search, computation, database access, and more), and include multiple LLMs in a single conversation to bring a variety of perspectives to bear on a problem.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "context_management/03_external_prompts.html",
    "href": "context_management/03_external_prompts.html",
    "title": "External Prompts and Instructions: file: and exec:",
    "section": "",
    "text": "Many Lectic fields can load their text from outside the document. You can point a field at a file on disk, or run a command (or script) and use its output. This lets you keep prompts, usage text, and notes in one place, or compute them on demand.\nWhat supports external sources:\n\ninterlocutor.prompt\nmacros[].expansion\ntools[].usage (for tools that accept a usage string)\ntools[].details (for tools that provide extra details)\n\nEach of these accepts either a plain string, or a string beginning with one of the prefixes below.\n\n\nLoads the contents of PATH and uses that as the field value. Environment variables in the path are expanded before reading.\nExamples\ninterlocutor:\n  name: Assistant\n  prompt: file:./prompts/assistant.md\nmacros:\n  - name: summarize\n    expansion: file:$HOME/.config/lectic/prompts/summarize.txt\n\n\n\nRuns the command and uses its stdout as the field value. There are two forms:\n\nSingle line: executed directly, not through a shell. Shell features like globbing and command substitution do not work. If you need them, invoke a shell explicitly (for example, bash -lc '...').\nMulti‑line: treated as a script. The first line must be a shebang (for example, #!/usr/bin/env bash). The script is written to a temporary file and executed with the interpreter from the shebang.\n\nEnvironment variables in a single‑line command are expanded before running. For multi‑line scripts, variables are available via the process environment at runtime.\n\n\n\nSingle line\ninterlocutor:\n  name: Assistant\n  prompt: exec:echo \"You are a helpful assistant.\"\nMulti‑line script\ninterlocutor:\n  name: Assistant\n  prompt: |\n    exec:#!/usr/bin/env bash\n    cat &lt;&lt;'PREFACE'\n    You are a helpful assistant.\n    You will incorporate recent memory below.\n    PREFACE\n    echo\n    echo \"Recent memory:\"\n    sqlite3 \"$LECTIC_DATA/memory.sqlite3\" \\\n      \"SELECT printf('- %s (%s)', text, ts) FROM memory \\\n       ORDER BY ts DESC LIMIT 5;\"\n\n\n\n\nfile: and exec: resolve relative paths and run commands in the current working directory of the lectic process (the directory from which you invoked the command). If you used -f or -i, note that the working directory does not automatically switch to the .lec file’s directory for these expansions. Use absolute paths or cd if you need a different base.\nStandard Lectic environment variables are provided, including LECTIC_CONFIG, LECTIC_DATA, LECTIC_CACHE, LECTIC_STATE, LECTIC_TEMP, and LECTIC_FILE (when using -f or -i). Your shell environment is also passed through.\nMacro expansions can inject additional variables into exec: via directive attributes. See the Macros guide for details.\n\n\n\n\n\nThe value is recomputed on each run. This makes it easy to incorporate recent state (for example, “memory” from a local database) into a prompt.\nIf a file cannot be read or a command fails, Lectic reports an error and aborts the run. Fix the source and try again.\n\nSee also\n\nExternal Content for attaching files to user messages.\nMacros for passing variables into exec: expansions within macros.",
    "crumbs": [
      "Context Management",
      "External Prompts and Instructions"
    ]
  },
  {
    "objectID": "context_management/03_external_prompts.html#filepath",
    "href": "context_management/03_external_prompts.html#filepath",
    "title": "External Prompts and Instructions: file: and exec:",
    "section": "",
    "text": "Loads the contents of PATH and uses that as the field value. Environment variables in the path are expanded before reading.\nExamples\ninterlocutor:\n  name: Assistant\n  prompt: file:./prompts/assistant.md\nmacros:\n  - name: summarize\n    expansion: file:$HOME/.config/lectic/prompts/summarize.txt",
    "crumbs": [
      "Context Management",
      "External Prompts and Instructions"
    ]
  },
  {
    "objectID": "context_management/03_external_prompts.html#execcommand-or-execscript",
    "href": "context_management/03_external_prompts.html#execcommand-or-execscript",
    "title": "External Prompts and Instructions: file: and exec:",
    "section": "",
    "text": "Runs the command and uses its stdout as the field value. There are two forms:\n\nSingle line: executed directly, not through a shell. Shell features like globbing and command substitution do not work. If you need them, invoke a shell explicitly (for example, bash -lc '...').\nMulti‑line: treated as a script. The first line must be a shebang (for example, #!/usr/bin/env bash). The script is written to a temporary file and executed with the interpreter from the shebang.\n\nEnvironment variables in a single‑line command are expanded before running. For multi‑line scripts, variables are available via the process environment at runtime.",
    "crumbs": [
      "Context Management",
      "External Prompts and Instructions"
    ]
  },
  {
    "objectID": "context_management/03_external_prompts.html#examples",
    "href": "context_management/03_external_prompts.html#examples",
    "title": "External Prompts and Instructions: file: and exec:",
    "section": "",
    "text": "Single line\ninterlocutor:\n  name: Assistant\n  prompt: exec:echo \"You are a helpful assistant.\"\nMulti‑line script\ninterlocutor:\n  name: Assistant\n  prompt: |\n    exec:#!/usr/bin/env bash\n    cat &lt;&lt;'PREFACE'\n    You are a helpful assistant.\n    You will incorporate recent memory below.\n    PREFACE\n    echo\n    echo \"Recent memory:\"\n    sqlite3 \"$LECTIC_DATA/memory.sqlite3\" \\\n      \"SELECT printf('- %s (%s)', text, ts) FROM memory \\\n       ORDER BY ts DESC LIMIT 5;\"",
    "crumbs": [
      "Context Management",
      "External Prompts and Instructions"
    ]
  },
  {
    "objectID": "context_management/03_external_prompts.html#working-directory-and-environment",
    "href": "context_management/03_external_prompts.html#working-directory-and-environment",
    "title": "External Prompts and Instructions: file: and exec:",
    "section": "",
    "text": "file: and exec: resolve relative paths and run commands in the current working directory of the lectic process (the directory from which you invoked the command). If you used -f or -i, note that the working directory does not automatically switch to the .lec file’s directory for these expansions. Use absolute paths or cd if you need a different base.\nStandard Lectic environment variables are provided, including LECTIC_CONFIG, LECTIC_DATA, LECTIC_CACHE, LECTIC_STATE, LECTIC_TEMP, and LECTIC_FILE (when using -f or -i). Your shell environment is also passed through.\nMacro expansions can inject additional variables into exec: via directive attributes. See the Macros guide for details.",
    "crumbs": [
      "Context Management",
      "External Prompts and Instructions"
    ]
  },
  {
    "objectID": "context_management/03_external_prompts.html#behavior-and-errors",
    "href": "context_management/03_external_prompts.html#behavior-and-errors",
    "title": "External Prompts and Instructions: file: and exec:",
    "section": "",
    "text": "The value is recomputed on each run. This makes it easy to incorporate recent state (for example, “memory” from a local database) into a prompt.\nIf a file cannot be read or a command fails, Lectic reports an error and aborts the run. Fix the source and try again.\n\nSee also\n\nExternal Content for attaching files to user messages.\nMacros for passing variables into exec: expansions within macros.",
    "crumbs": [
      "Context Management",
      "External Prompts and Instructions"
    ]
  },
  {
    "objectID": "context_management/02_conversation_control.html",
    "href": "context_management/02_conversation_control.html",
    "title": "Managing Context: Conversation Control",
    "section": "",
    "text": "Beyond simply adding external data, Lectic provides directives that allow you to actively manage the flow of a conversation. You can switch between different LLM interlocutors and control the context window that is sent to the model.\n\n\nLectic allows you to define multiple interlocutors in the YAML frontmatter. This enables you to bring different “personalities” or models with different capabilities into a single conversation.\nTo do this, use the interlocutors key (instead of interlocutor) and provide a list of configurations.\n---\ninterlocutors:\n  - name: Boggle\n    provider: anthropic\n    model: claude-3-sonnet-20240229\n    prompt: You are an expert on personal finance.\n  - name: Oggle\n    provider: gemini\n    prompt: You are very skeptical of conventional financial advice.\n---\nOnce multiple interlocutors are defined, Lectic will continue with whichever one was last active. To direct your message to a specific interlocutor, you use the :ask and :aside directives.\n\n\nThe :ask[InterlocutorName] directive changes the active speaker. All subsequent conversation turns will be handled by this new interlocutor until another :ask directive is used.\n:ask[Boggle] What is the best way to save for retirement?\n\n:::Boggle\nThe most common advice is to invest in a diversified portfolio of low-cost\nindex funds.\n:::\n\n:ask[Oggle] What's your take on that?\n\n:::Oggle\n\"Common advice\" is often just dogma. That strategy leaves you completely\nexposed to market downturns.\n:::\n\n\n\nThe :aside[InterlocutorName] directive is for a one-off question or comment. It directs the current message to the specified interlocutor, but the conversation then reverts back to the previously active speaker for the next user message.\nThis is useful for quick interjections or for getting a second opinion without derailing the main flow of the conversation.\n\n\n\n\nAs a conversation grows longer, the context sent to the LLM on each turn also grows. This can increase costs and, in some cases, lead to the model getting “stuck” on earlier parts of the dialogue.\nThe :reset[] directive gives you manual control over the context window. When Lectic processes a message containing :reset[], it instructs the LLM to ignore all preceding conversation history. The context for that turn effectively begins with the message containing the directive.\n...a very long conversation happens here...\n\nOkay, let's start fresh. I'm going to :reset[] your context now. Please\nsummarize our previous discussion and then we can move on to the next topic.\n\n:::Assistant\nUnderstood. I have cleared my context.\n\nPreviously, we discussed the history of the Roman Empire, focusing on the\nreign of Augustus and the establishment of the Pax Romana. We concluded that\neconomic stability was a key factor in the era's success.\n\nI am ready for the next topic.\n:::\n\nExcellent. Now, how did this conversation begin?\n\n:::Assistant\nThis conversation began with you instructing me to reset my context and provide\na summary of our previous discussion about the Roman Empire.\n:::\nThis is a powerful tool for managing long-running conversations, allowing you to “compact” the context manually or with the help of the LLM.",
    "crumbs": [
      "Context Management",
      "Conversation Control"
    ]
  },
  {
    "objectID": "context_management/02_conversation_control.html#multiparty-conversations-with-ask-and-aside",
    "href": "context_management/02_conversation_control.html#multiparty-conversations-with-ask-and-aside",
    "title": "Managing Context: Conversation Control",
    "section": "",
    "text": "Lectic allows you to define multiple interlocutors in the YAML frontmatter. This enables you to bring different “personalities” or models with different capabilities into a single conversation.\nTo do this, use the interlocutors key (instead of interlocutor) and provide a list of configurations.\n---\ninterlocutors:\n  - name: Boggle\n    provider: anthropic\n    model: claude-3-sonnet-20240229\n    prompt: You are an expert on personal finance.\n  - name: Oggle\n    provider: gemini\n    prompt: You are very skeptical of conventional financial advice.\n---\nOnce multiple interlocutors are defined, Lectic will continue with whichever one was last active. To direct your message to a specific interlocutor, you use the :ask and :aside directives.\n\n\nThe :ask[InterlocutorName] directive changes the active speaker. All subsequent conversation turns will be handled by this new interlocutor until another :ask directive is used.\n:ask[Boggle] What is the best way to save for retirement?\n\n:::Boggle\nThe most common advice is to invest in a diversified portfolio of low-cost\nindex funds.\n:::\n\n:ask[Oggle] What's your take on that?\n\n:::Oggle\n\"Common advice\" is often just dogma. That strategy leaves you completely\nexposed to market downturns.\n:::\n\n\n\nThe :aside[InterlocutorName] directive is for a one-off question or comment. It directs the current message to the specified interlocutor, but the conversation then reverts back to the previously active speaker for the next user message.\nThis is useful for quick interjections or for getting a second opinion without derailing the main flow of the conversation.",
    "crumbs": [
      "Context Management",
      "Conversation Control"
    ]
  },
  {
    "objectID": "context_management/02_conversation_control.html#context-management-with-reset",
    "href": "context_management/02_conversation_control.html#context-management-with-reset",
    "title": "Managing Context: Conversation Control",
    "section": "",
    "text": "As a conversation grows longer, the context sent to the LLM on each turn also grows. This can increase costs and, in some cases, lead to the model getting “stuck” on earlier parts of the dialogue.\nThe :reset[] directive gives you manual control over the context window. When Lectic processes a message containing :reset[], it instructs the LLM to ignore all preceding conversation history. The context for that turn effectively begins with the message containing the directive.\n...a very long conversation happens here...\n\nOkay, let's start fresh. I'm going to :reset[] your context now. Please\nsummarize our previous discussion and then we can move on to the next topic.\n\n:::Assistant\nUnderstood. I have cleared my context.\n\nPreviously, we discussed the history of the Roman Empire, focusing on the\nreign of Augustus and the establishment of the Pax Romana. We concluded that\neconomic stability was a key factor in the era's success.\n\nI am ready for the next topic.\n:::\n\nExcellent. Now, how did this conversation begin?\n\n:::Assistant\nThis conversation began with you instructing me to reset my context and provide\na summary of our previous discussion about the Roman Empire.\n:::\nThis is a powerful tool for managing long-running conversations, allowing you to “compact” the context manually or with the help of the LLM.",
    "crumbs": [
      "Context Management",
      "Conversation Control"
    ]
  },
  {
    "objectID": "context_management/01_external_content.html",
    "href": "context_management/01_external_content.html",
    "title": "Managing Context: External Content",
    "section": "",
    "text": "Lectic aims to make it easy to pull external information into the conversation, providing the LLM with the context it needs to answer questions, analyze data, or perform tasks.\nThis is done in two primary ways: by referencing files and URIs using standard Markdown links, and by executing shell commands with the :cmd directive.\n\n\nYou can include local or remote content using the standard Markdown link syntax, [Title](URI). Lectic will fetch the content from the URI and include it in the context for the LLM.\nPlease summarize this local document: [Notes](./notes.md)\n\nAnalyze the data in this S3 bucket: [Dataset](s3://my_bucket/dataset.csv)\n\nWhat does this README say?\n[Repo](github+repo://gleachkr/Lectic/contents/README.md)\n\n\n\nText: Plain text files are included directly.\nImages: PNG, JPEG, GIF, and WebP images are supported.\nPDFs: Content from PDF files can be extracted (requires a provider that supports PDF ingestion, such as Anthropic, Gemini, or OpenAI).\nAudio: Gemini and OpenAI support audio inputs. For OpenAI, use provider: openai/chat with an audio‑capable model; supported formats include MP3, MPEG, and WAV. Gemini supports a broader set of audio types.\nVideo: Gemini supports video understanding. See supported formats in Google’s docs: https://ai.google.dev/gemini-api/docs/video-understanding#supported-formats\n\n\n\n\nLectic supports several URI schemes for referencing content:\n\nLocal Files: Simple relative paths like ./src/main.rs or absolute file:///path/to/file.txt URIs.\nRemote Content: http:// and https:// for web pages and other online resources.\nAmazon S3: s3:// for referencing objects in S3 buckets. This requires AWS credentials to be configured in your environment.\nMCP Resources: You can reference resources provided by an MCP server using a custom scheme, like github+repo://....\n\nA few convenience rules apply:\n\nFor local file references using file://, use absolute paths. A portable way to build these is with $PWD (e.g., file://$PWD/papers/some.pdf).\nEnvironment variables in URIs use the $VAR form; ${VAR} is not supported. Expansion happens before any globbing.\n\nYou can use glob patterns to include multiple files at once. This is useful for providing the entire source code of a project as context.\n[All source code](./src/**/*.ts)\n[All images in this directory](./images/*.jpg)\nLectic uses Bun’s Glob API for matching.\n\n\n\nUsing full file:// URIs for local content enables additional capabilities.\n\n\nLectic supports environment variable expansion in URIs. This helps in creating portable .lec files that don’t rely on hardcoded absolute paths.\n[My dataset](file://$DATA_ROOT/my_project/data.csv)\n[Log file](file://$PWD/logs/latest.log)\n\n\n\nWhen referencing a PDF, you can point to a specific page or a range of pages by adding a fragment to the URI. Page numbering starts at 1.\n\nSingle Page: [See page 5](file.pdf#page=5)\nPage Range: [See Chapter 2](book.pdf#pages=20-35)\n\nIf both page and pages are supplied, pages takes precedence. If a page or range is malformed or out of bounds, Lectic will surface an error that is visible to the LLM.\n\n\n\n\n\nFor dynamic content, you can use the :cmd[...] directive to execute a shell command and insert its standard output directly into your message.\nLectic executes the command using the fast and modern Bun shell.\n\n\nThis is a powerful way to provide the LLM with real-time information.\n\nSystem Information: What can you tell me about my system? :cmd[uname -a]\nProject State: Please write a commit message for these changes: :cmd[git diff --staged]\nData Analysis: Based on this data, what's the average? :cmd[cat data.csv | awk '...']\n\nThe output of the command is seamlessly included in the user message that is sent to the LLM.",
    "crumbs": [
      "Context Management",
      "External Content"
    ]
  },
  {
    "objectID": "context_management/01_external_content.html#content-references-via-markdown-links",
    "href": "context_management/01_external_content.html#content-references-via-markdown-links",
    "title": "Managing Context: External Content",
    "section": "",
    "text": "You can include local or remote content using the standard Markdown link syntax, [Title](URI). Lectic will fetch the content from the URI and include it in the context for the LLM.\nPlease summarize this local document: [Notes](./notes.md)\n\nAnalyze the data in this S3 bucket: [Dataset](s3://my_bucket/dataset.csv)\n\nWhat does this README say?\n[Repo](github+repo://gleachkr/Lectic/contents/README.md)\n\n\n\nText: Plain text files are included directly.\nImages: PNG, JPEG, GIF, and WebP images are supported.\nPDFs: Content from PDF files can be extracted (requires a provider that supports PDF ingestion, such as Anthropic, Gemini, or OpenAI).\nAudio: Gemini and OpenAI support audio inputs. For OpenAI, use provider: openai/chat with an audio‑capable model; supported formats include MP3, MPEG, and WAV. Gemini supports a broader set of audio types.\nVideo: Gemini supports video understanding. See supported formats in Google’s docs: https://ai.google.dev/gemini-api/docs/video-understanding#supported-formats\n\n\n\n\nLectic supports several URI schemes for referencing content:\n\nLocal Files: Simple relative paths like ./src/main.rs or absolute file:///path/to/file.txt URIs.\nRemote Content: http:// and https:// for web pages and other online resources.\nAmazon S3: s3:// for referencing objects in S3 buckets. This requires AWS credentials to be configured in your environment.\nMCP Resources: You can reference resources provided by an MCP server using a custom scheme, like github+repo://....\n\nA few convenience rules apply:\n\nFor local file references using file://, use absolute paths. A portable way to build these is with $PWD (e.g., file://$PWD/papers/some.pdf).\nEnvironment variables in URIs use the $VAR form; ${VAR} is not supported. Expansion happens before any globbing.\n\nYou can use glob patterns to include multiple files at once. This is useful for providing the entire source code of a project as context.\n[All source code](./src/**/*.ts)\n[All images in this directory](./images/*.jpg)\nLectic uses Bun’s Glob API for matching.\n\n\n\nUsing full file:// URIs for local content enables additional capabilities.\n\n\nLectic supports environment variable expansion in URIs. This helps in creating portable .lec files that don’t rely on hardcoded absolute paths.\n[My dataset](file://$DATA_ROOT/my_project/data.csv)\n[Log file](file://$PWD/logs/latest.log)\n\n\n\nWhen referencing a PDF, you can point to a specific page or a range of pages by adding a fragment to the URI. Page numbering starts at 1.\n\nSingle Page: [See page 5](file.pdf#page=5)\nPage Range: [See Chapter 2](book.pdf#pages=20-35)\n\nIf both page and pages are supplied, pages takes precedence. If a page or range is malformed or out of bounds, Lectic will surface an error that is visible to the LLM.",
    "crumbs": [
      "Context Management",
      "External Content"
    ]
  },
  {
    "objectID": "context_management/01_external_content.html#command-output-via-cmd-directive",
    "href": "context_management/01_external_content.html#command-output-via-cmd-directive",
    "title": "Managing Context: External Content",
    "section": "",
    "text": "For dynamic content, you can use the :cmd[...] directive to execute a shell command and insert its standard output directly into your message.\nLectic executes the command using the fast and modern Bun shell.\n\n\nThis is a powerful way to provide the LLM with real-time information.\n\nSystem Information: What can you tell me about my system? :cmd[uname -a]\nProject State: Please write a commit message for these changes: :cmd[git diff --staged]\nData Analysis: Based on this data, what's the average? :cmd[cat data.csv | awk '...']\n\nThe output of the command is seamlessly included in the user message that is sent to the LLM.",
    "crumbs": [
      "Context Management",
      "External Content"
    ]
  },
  {
    "objectID": "02_getting_started.html",
    "href": "02_getting_started.html",
    "title": "Getting Started with Lectic",
    "section": "",
    "text": "This short guide helps you install Lectic and run your first conversation. Along the way, you will verify your install, set an API key, and see a simple tool in action.\n\n\nChoose the method that fits your system.\n\n\nIf you use Nix, install directly from the repository:\nnix profile install github:gleachkr/lectic\n\n\n\nDownload the AppImage from the GitHub Releases page. Make it executable and put it on your PATH.\nchmod +x lectic-*.AppImage\nmv lectic-*.AppImage ~/.local/bin/lectic\n\n\n\nDownload the macOS binary from the GitHub Releases page and put it on your PATH.\n\n\n\n\nlectic --version\nIf you see a version number, you are ready to go.\n\n\n\n\n\nLectic talks to LLM providers. Put at least one provider key in your environment so Lectic can pick a default.\nexport ANTHROPIC_API_KEY=\"your-api-key-here\"\nLectic chooses a default provider by checking for keys in this order: Anthropic, then Gemini, then OpenAI, then OpenRouter. If you need Bedrock, set provider: anthropic/bedrock explicitly in your file and make sure your AWS credentials are configured. Bedrock is not auto‑selected.\nFinally, OpenAI has two provider choices. Use openai for the newer Responses API and native tools. Use openai/chat for the legacy Chat Completions API when you need it.\n\n\n\nMake a new file, for example my_convo.lec. The .lec extension helps with editor integration.\nAdd a minimal YAML header and your first user message:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic\n  model: claude-3-haiku-20240307\n---\n\nHello, world! What is a fun fact about the Rust programming language?\n\n\n\nFrom your terminal, run Lectic on the file. The -i flag updates the file in place.\nlectic -i my_convo.lec\nLectic sends your message to the model and appends its response in a new assistant block. You can add your next message under that block and run the command again to continue the conversation.\n\n\n\nNow add a very small tool to see the tool flow. This one exposes date.\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic\n  model: claude-3-haiku-20240307\n  tools:\n    - exec: date\n      name: get_date\n---\n\nWhat is today's date?\nRun Lectic again. The assistant block will now include an XML tool call and the recorded results. You will see tags like , , and  in the block.\n\n\n\n\n\n\nTip\n\n\n\nYou can load prompts from files or compute them with commands using file: and exec:. See External prompts (file:, exec:).\n\n\n\n\n\n\nThe intended workflow is to wire Lectic into your editor so you can update a conversation with a single key press.\n\n\nA full‑featured plugin is available in extra/lectic.nvim. It provides syntax, folding, and one‑shot submission.\n\n\n\nAn extension is available in extra/lectic.vscode.\n\n\n\nMost editors support sending the current buffer to an external command. You can configure yours to replace the buffer with the output of:\ncat your_file.lec | lectic",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "02_getting_started.html#installation",
    "href": "02_getting_started.html#installation",
    "title": "Getting Started with Lectic",
    "section": "",
    "text": "Choose the method that fits your system.\n\n\nIf you use Nix, install directly from the repository:\nnix profile install github:gleachkr/lectic\n\n\n\nDownload the AppImage from the GitHub Releases page. Make it executable and put it on your PATH.\nchmod +x lectic-*.AppImage\nmv lectic-*.AppImage ~/.local/bin/lectic\n\n\n\nDownload the macOS binary from the GitHub Releases page and put it on your PATH.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "02_getting_started.html#verify-the-install",
    "href": "02_getting_started.html#verify-the-install",
    "title": "Getting Started with Lectic",
    "section": "",
    "text": "lectic --version\nIf you see a version number, you are ready to go.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "02_getting_started.html#your-first-conversation",
    "href": "02_getting_started.html#your-first-conversation",
    "title": "Getting Started with Lectic",
    "section": "",
    "text": "Lectic talks to LLM providers. Put at least one provider key in your environment so Lectic can pick a default.\nexport ANTHROPIC_API_KEY=\"your-api-key-here\"\nLectic chooses a default provider by checking for keys in this order: Anthropic, then Gemini, then OpenAI, then OpenRouter. If you need Bedrock, set provider: anthropic/bedrock explicitly in your file and make sure your AWS credentials are configured. Bedrock is not auto‑selected.\nFinally, OpenAI has two provider choices. Use openai for the newer Responses API and native tools. Use openai/chat for the legacy Chat Completions API when you need it.\n\n\n\nMake a new file, for example my_convo.lec. The .lec extension helps with editor integration.\nAdd a minimal YAML header and your first user message:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic\n  model: claude-3-haiku-20240307\n---\n\nHello, world! What is a fun fact about the Rust programming language?\n\n\n\nFrom your terminal, run Lectic on the file. The -i flag updates the file in place.\nlectic -i my_convo.lec\nLectic sends your message to the model and appends its response in a new assistant block. You can add your next message under that block and run the command again to continue the conversation.\n\n\n\nNow add a very small tool to see the tool flow. This one exposes date.\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic\n  model: claude-3-haiku-20240307\n  tools:\n    - exec: date\n      name: get_date\n---\n\nWhat is today's date?\nRun Lectic again. The assistant block will now include an XML tool call and the recorded results. You will see tags like , , and  in the block.\n\n\n\n\n\n\nTip\n\n\n\nYou can load prompts from files or compute them with commands using file: and exec:. See External prompts (file:, exec:).",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "02_getting_started.html#editor-integration",
    "href": "02_getting_started.html#editor-integration",
    "title": "Getting Started with Lectic",
    "section": "",
    "text": "The intended workflow is to wire Lectic into your editor so you can update a conversation with a single key press.\n\n\nA full‑featured plugin is available in extra/lectic.nvim. It provides syntax, folding, and one‑shot submission.\n\n\n\nAn extension is available in extra/lectic.vscode.\n\n\n\nMost editors support sending the current buffer to an external command. You can configure yours to replace the buffer with the output of:\ncat your_file.lec | lectic",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "reference/01_cli.html",
    "href": "reference/01_cli.html",
    "title": "Reference: Command‑Line Interface",
    "section": "",
    "text": "The lectic command is the primary way to interact with Lectic. It can read from a file or from standard input, and it offers flags to control how the result is printed or saved.\n\n\nlectic [FLAGS] [OPTIONS]\n\n\n\n\n-v, --version Prints the version string.\n-f, --file &lt;PATH&gt; Path to the conversation file (.lec) to process. If omitted, Lectic reads from standard input.\n-i, --inplace &lt;PATH&gt; Read from the given file and update it in place. Mutually exclusive with --file.\n-s, --short Only emit the newly generated assistant message, not the full updated conversation.\n-S, --Short Like --short, but emits only the raw message text (without the :::Speaker wrapper).\n-H, --header Emit only the YAML frontmatter of the input. With --inplace, this will overwrite the file to contain only the header (effectively resetting the conversation).\n-I, --Include &lt;PATH&gt; Include an additional YAML file whose contents are merged into the active configuration. Higher precedence than system and working‑directory config, lower than the .lec header.\n-l, --log &lt;PATH&gt; Write detailed debug logs to the given file.\n-q, --quiet Suppress printing the assistant’s response to stdout.\n-h, --help Show help for all flags and options.\n\n\n\n\n\n–inplace cannot be combined with –file.\n–header cannot be combined with –short or –Short.\n–quiet cannot be combined with –short or –Short.\n\n\n\n\n\nGenerate the next message in a file and update it in place:\nlectic -i conversation.lec\nRead from stdin and write the full result to stdout:\ncat conversation.lec | lectic\nStream just the new assistant message:\nlectic -s -f conversation.lec\nAdd a message from the command line and update the file:\necho \"This is a new message.\" | lectic -i conversation.lec\nUse a project‑specific config file:\nlectic -I project.yaml -i conversation.lec",
    "crumbs": [
      "Reference",
      "Cli"
    ]
  },
  {
    "objectID": "reference/01_cli.html#usage",
    "href": "reference/01_cli.html#usage",
    "title": "Reference: Command‑Line Interface",
    "section": "",
    "text": "lectic [FLAGS] [OPTIONS]",
    "crumbs": [
      "Reference",
      "Cli"
    ]
  },
  {
    "objectID": "reference/01_cli.html#flags-and-options",
    "href": "reference/01_cli.html#flags-and-options",
    "title": "Reference: Command‑Line Interface",
    "section": "",
    "text": "-v, --version Prints the version string.\n-f, --file &lt;PATH&gt; Path to the conversation file (.lec) to process. If omitted, Lectic reads from standard input.\n-i, --inplace &lt;PATH&gt; Read from the given file and update it in place. Mutually exclusive with --file.\n-s, --short Only emit the newly generated assistant message, not the full updated conversation.\n-S, --Short Like --short, but emits only the raw message text (without the :::Speaker wrapper).\n-H, --header Emit only the YAML frontmatter of the input. With --inplace, this will overwrite the file to contain only the header (effectively resetting the conversation).\n-I, --Include &lt;PATH&gt; Include an additional YAML file whose contents are merged into the active configuration. Higher precedence than system and working‑directory config, lower than the .lec header.\n-l, --log &lt;PATH&gt; Write detailed debug logs to the given file.\n-q, --quiet Suppress printing the assistant’s response to stdout.\n-h, --help Show help for all flags and options.",
    "crumbs": [
      "Reference",
      "Cli"
    ]
  },
  {
    "objectID": "reference/01_cli.html#constraints",
    "href": "reference/01_cli.html#constraints",
    "title": "Reference: Command‑Line Interface",
    "section": "",
    "text": "–inplace cannot be combined with –file.\n–header cannot be combined with –short or –Short.\n–quiet cannot be combined with –short or –Short.",
    "crumbs": [
      "Reference",
      "Cli"
    ]
  },
  {
    "objectID": "reference/01_cli.html#common-examples",
    "href": "reference/01_cli.html#common-examples",
    "title": "Reference: Command‑Line Interface",
    "section": "",
    "text": "Generate the next message in a file and update it in place:\nlectic -i conversation.lec\nRead from stdin and write the full result to stdout:\ncat conversation.lec | lectic\nStream just the new assistant message:\nlectic -s -f conversation.lec\nAdd a message from the command line and update the file:\necho \"This is a new message.\" | lectic -i conversation.lec\nUse a project‑specific config file:\nlectic -I project.yaml -i conversation.lec",
    "crumbs": [
      "Reference",
      "Cli"
    ]
  },
  {
    "objectID": "tools/02_exec.html",
    "href": "tools/02_exec.html",
    "title": "Tools: Command Execution (exec)",
    "section": "",
    "text": "The exec tool is one of the most versatile tools in Lectic. It allows the LLM to execute commands and scripts, enabling it to interact directly with your system, run code, and interface with other command‑line applications.\n\n\nThe snippets below show only the tool definition. They assume you have an interlocutor with a valid prompt and model configuration. See Getting Started for a full header example.\nYou configure an exec tool by providing the command to be executed. You can also provide a custom name for the LLM to use, a usage guide, and optional parameters for security and execution control.\n\n\nThis configuration allows the LLM to run the python3 interpreter.\ntools:\n  - exec: python3\n    name: python\n    usage: &gt;\n      Use this to execute Python code. The code to be executed should be\n      written inside the tool call block.\n\n\n\nYou can also provide a multi‑line script in the YAML. The first line of the script must be a shebang (for example, #!/usr/bin/env bash) to choose the interpreter.\ntools:\n  - name: line_counter\n    usage: \"Counts the number of lines in a file. Takes one argument: path.\"\n    exec: |\n      #!/usr/bin/env bash\n      # A simple script to count the lines in a file\n      wc -l \"$1\"\n\n\n\n\nexec: (Required) The command or inline script to execute.\nname: An optional name for the tool.\nusage: A string with instructions for the LLM. It also accepts file: and exec: sources. See External Sources (file:, exec:) for semantics.\nsandbox: A path to a sandboxing script. See safety below.\nconfirm: A path to a confirmation script. See safety below.\ntimeoutSeconds: Seconds to wait before aborting a long‑running call.\nenv: Environment variables to set for the subprocess.\nschema: A map of parameter name → description. When present, the tool takes named string parameters (exposed as env vars). When absent, the tool instead takes a required arguments array of strings.\n\n\n\n\n\n\nNo shell is involved. The command (or script’s interpreter) is executed directly. Shell features like globbing or command substitution will not work unless you invoke a shell yourself.\nSingle‑line exec values have environment variables expanded before execution using the tool’s env plus standard Lectic variables.\nMulti‑line exec values must start with a shebang. Lectic writes the script to a temporary file and executes it with that interpreter.\n\n\n\nThe current working directory for exec is:\n\nIf you run with -f or -i: the directory containing the .lec file.\nOtherwise: the directory from which you invoked the lectic command.\n\nThis means relative paths in your commands and scripts resolve relative to that directory. Temporary scripts are written into the same working directory.\n\n\n\nLectic captures stdout and stderr separately and returns both to the model. It also includes the numeric exit code when it is non‑zero. You will see these serialized inside the tool call results as XML tags like , , and .\nIf a timeout occurs, Lectic kills the subprocess and throws an error that includes any partial stdout and stderr collected so far.\n\n\n\nYou might want to control what arguments your LLM can pass to a command or script, or offer a template for correct usage. If your configuration includes a schema, the LLM will be guided to provide specific parameters when calling the script or command. Each parameter is a string and Lectic exposes it to the subprocess via an environment variable with the same name.\nThis applies to both commands and scripts:\n\nFor scripts, parameters are available as $PARAM_NAME inside the script.\nFor commands, parameters are available in the subprocess environment.\n\nExample:\n# YAML configuration\ntools:\n  - name: greeter\n    exec: |\n      #!/usr/bin/env bash\n      echo \"Hello, ${NAME}! Today is ${DAY}.\"\n    schema:\n      NAME: The name to greet.\n      DAY: The day string to include.\nIf the LLM provides { NAME: \"Ada\", DAY: \"Friday\" } Lectic will fill in the results:\n&lt;tool-call with=\"greeter\"&gt;\n  &lt;arguments&gt;\n    &lt;NAME&gt;\"Ada\"&lt;/NAME&gt;\n    &lt;DAY&gt;\"Friday\"&lt;/DAY&gt;\n  &lt;/arguments&gt;\n  &lt;results&gt;\n    &lt;result type=\"text\"&gt;Hello, Ada! Today is Friday.\\n&lt;/result&gt;\n  &lt;/results&gt;\n&lt;/tool-call&gt;\n\n\n\n\nWhen Lectic runs your command or script, it sets a few helpful environment variables. In particular, LECTIC_INTERLOCUTOR is set to the name of the interlocutor who invoked the tool. This makes it easy to maintain per‑interlocutor state (for example, separate scratch directories or memory stores) in your scripts or sandbox wrappers.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nGranting an LLM the ability to execute commands can be dangerous. Treat every exec tool as a capability you are delegating. Combine human‑in‑the‑ loop confirmation and sandboxing to minimize risk. Do not expose sensitive files or networks unless you fully trust the tool and its usage.\n\n\nLectic provides two mechanisms to help you keep exec tools safe: confirmation scripts and sandboxing.\n\n\nIf a confirm script is provided, Lectic runs it before every call. The script receives two arguments: the tool’s name and a JSON string of the call arguments. If the confirmation script exits non‑zero, the call is cancelled.\nAn example script that uses a graphical dialog is in extra/confirm/zenity-confirm.sh.\n\n\n\nWhen a sandbox script is configured, a command call will actually execute the sandbox script, which will receive the command and the LLM provided parameters as arguments. The wrapper is responsible for creating a controlled environment to run the command.\nFor example, extra/sandbox/bwrap-sandbox.sh uses Bubblewrap to create a minimal, isolated environment with a temporary home directory.",
    "crumbs": [
      "Tools",
      "Exec"
    ]
  },
  {
    "objectID": "tools/02_exec.html#configuration",
    "href": "tools/02_exec.html#configuration",
    "title": "Tools: Command Execution (exec)",
    "section": "",
    "text": "The snippets below show only the tool definition. They assume you have an interlocutor with a valid prompt and model configuration. See Getting Started for a full header example.\nYou configure an exec tool by providing the command to be executed. You can also provide a custom name for the LLM to use, a usage guide, and optional parameters for security and execution control.\n\n\nThis configuration allows the LLM to run the python3 interpreter.\ntools:\n  - exec: python3\n    name: python\n    usage: &gt;\n      Use this to execute Python code. The code to be executed should be\n      written inside the tool call block.\n\n\n\nYou can also provide a multi‑line script in the YAML. The first line of the script must be a shebang (for example, #!/usr/bin/env bash) to choose the interpreter.\ntools:\n  - name: line_counter\n    usage: \"Counts the number of lines in a file. Takes one argument: path.\"\n    exec: |\n      #!/usr/bin/env bash\n      # A simple script to count the lines in a file\n      wc -l \"$1\"\n\n\n\n\nexec: (Required) The command or inline script to execute.\nname: An optional name for the tool.\nusage: A string with instructions for the LLM. It also accepts file: and exec: sources. See External Sources (file:, exec:) for semantics.\nsandbox: A path to a sandboxing script. See safety below.\nconfirm: A path to a confirmation script. See safety below.\ntimeoutSeconds: Seconds to wait before aborting a long‑running call.\nenv: Environment variables to set for the subprocess.\nschema: A map of parameter name → description. When present, the tool takes named string parameters (exposed as env vars). When absent, the tool instead takes a required arguments array of strings.",
    "crumbs": [
      "Tools",
      "Exec"
    ]
  },
  {
    "objectID": "tools/02_exec.html#command-execution-semantics",
    "href": "tools/02_exec.html#command-execution-semantics",
    "title": "Tools: Command Execution (exec)",
    "section": "",
    "text": "No shell is involved. The command (or script’s interpreter) is executed directly. Shell features like globbing or command substitution will not work unless you invoke a shell yourself.\nSingle‑line exec values have environment variables expanded before execution using the tool’s env plus standard Lectic variables.\nMulti‑line exec values must start with a shebang. Lectic writes the script to a temporary file and executes it with that interpreter.\n\n\n\nThe current working directory for exec is:\n\nIf you run with -f or -i: the directory containing the .lec file.\nOtherwise: the directory from which you invoked the lectic command.\n\nThis means relative paths in your commands and scripts resolve relative to that directory. Temporary scripts are written into the same working directory.\n\n\n\nLectic captures stdout and stderr separately and returns both to the model. It also includes the numeric exit code when it is non‑zero. You will see these serialized inside the tool call results as XML tags like , , and .\nIf a timeout occurs, Lectic kills the subprocess and throws an error that includes any partial stdout and stderr collected so far.\n\n\n\nYou might want to control what arguments your LLM can pass to a command or script, or offer a template for correct usage. If your configuration includes a schema, the LLM will be guided to provide specific parameters when calling the script or command. Each parameter is a string and Lectic exposes it to the subprocess via an environment variable with the same name.\nThis applies to both commands and scripts:\n\nFor scripts, parameters are available as $PARAM_NAME inside the script.\nFor commands, parameters are available in the subprocess environment.\n\nExample:\n# YAML configuration\ntools:\n  - name: greeter\n    exec: |\n      #!/usr/bin/env bash\n      echo \"Hello, ${NAME}! Today is ${DAY}.\"\n    schema:\n      NAME: The name to greet.\n      DAY: The day string to include.\nIf the LLM provides { NAME: \"Ada\", DAY: \"Friday\" } Lectic will fill in the results:\n&lt;tool-call with=\"greeter\"&gt;\n  &lt;arguments&gt;\n    &lt;NAME&gt;\"Ada\"&lt;/NAME&gt;\n    &lt;DAY&gt;\"Friday\"&lt;/DAY&gt;\n  &lt;/arguments&gt;\n  &lt;results&gt;\n    &lt;result type=\"text\"&gt;Hello, Ada! Today is Friday.\\n&lt;/result&gt;\n  &lt;/results&gt;\n&lt;/tool-call&gt;",
    "crumbs": [
      "Tools",
      "Exec"
    ]
  },
  {
    "objectID": "tools/02_exec.html#execution-environment",
    "href": "tools/02_exec.html#execution-environment",
    "title": "Tools: Command Execution (exec)",
    "section": "",
    "text": "When Lectic runs your command or script, it sets a few helpful environment variables. In particular, LECTIC_INTERLOCUTOR is set to the name of the interlocutor who invoked the tool. This makes it easy to maintain per‑interlocutor state (for example, separate scratch directories or memory stores) in your scripts or sandbox wrappers.",
    "crumbs": [
      "Tools",
      "Exec"
    ]
  },
  {
    "objectID": "tools/02_exec.html#safety-and-trust",
    "href": "tools/02_exec.html#safety-and-trust",
    "title": "Tools: Command Execution (exec)",
    "section": "",
    "text": "Warning\n\n\n\nGranting an LLM the ability to execute commands can be dangerous. Treat every exec tool as a capability you are delegating. Combine human‑in‑the‑ loop confirmation and sandboxing to minimize risk. Do not expose sensitive files or networks unless you fully trust the tool and its usage.\n\n\nLectic provides two mechanisms to help you keep exec tools safe: confirmation scripts and sandboxing.\n\n\nIf a confirm script is provided, Lectic runs it before every call. The script receives two arguments: the tool’s name and a JSON string of the call arguments. If the confirmation script exits non‑zero, the call is cancelled.\nAn example script that uses a graphical dialog is in extra/confirm/zenity-confirm.sh.\n\n\n\nWhen a sandbox script is configured, a command call will actually execute the sandbox script, which will receive the command and the LLM provided parameters as arguments. The wrapper is responsible for creating a controlled environment to run the command.\nFor example, extra/sandbox/bwrap-sandbox.sh uses Bubblewrap to create a minimal, isolated environment with a temporary home directory.",
    "crumbs": [
      "Tools",
      "Exec"
    ]
  },
  {
    "objectID": "tools/06_other_tools.html",
    "href": "tools/06_other_tools.html",
    "title": "Other Tools: think, serve, and native",
    "section": "",
    "text": "This document covers three distinct types of tools: a cognitive tool for the LLM, a simple web server, and a way to access the native, built-in capabilities of the model provider.\n\n\nThe think tool gives the LLM a private “scratch space” to pause and reason about a prompt before formulating its final response. This can improve the quality and thoughtfulness of the output, especially for complex or ambiguous questions.\nThis technique was inspired by a post on Anthropic’s engineering blog. The output of the think tool is hidden from the user by default in the editor plugins, though it is still present in the .lec file.\n\n\ntools:\n  - think_about: &gt;\n      What the user is really asking for, and what hidden assumptions they\n      might have.\n    name: scratchpad # Optional name\n\n\n\nWhat's the best city in the world?\n\n:::Assistant\n&lt;tool-call with=\"scratchpad\"&gt;\n  &lt;arguments&gt;\n    &lt;thought&gt;\"Best\" is subjective. The user could mean best for travel, for\n    food, for work, etc. I need to ask for clarification.&lt;/thought&gt;\n  &lt;/arguments&gt;\n  &lt;results&gt;\n    &lt;result type=\"text\"&gt;thought complete.&lt;/result&gt;\n  &lt;/results&gt;\n&lt;/tool-call&gt;\nThat depends on what you're looking for! Are you interested in the best city\nfor tourism, career opportunities, or something else?\n:::\n\n\n\n\nThe serve tool allows the LLM to spin up a simple, single-use web server to present content, such as an HTML file or a small web application it has generated.\nWhen the LLM uses this tool, Lectic starts a server on the specified port. It will then attempt to open the page in your default web browser. The server shuts down automatically after the first request is served. While the page is loading, Lectic waits for the first request—so the conversation resumes once your browser has loaded the page.\n\n\ntools:\n  - serve_on_port: 8080\n    name: web_server # Optional name\n\n\n\nGenerate a simple tic-tac-toe game in HTML and serve it to me.\n\n:::Assistant\n&lt;tool-call with=\"web_server\"&gt;\n  &lt;arguments&gt;\n    &lt;pageHtml&gt;\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;title&gt;Tic-Tac-Toe&lt;/title&gt;\n  ... (rest of the HTML/JS/CSS) ...\n&lt;/head&gt;\n&lt;body&gt;\n  ...\n&lt;/body&gt;\n&lt;/html&gt;\n    &lt;/pageHtml&gt;\n  &lt;/arguments&gt;\n  &lt;results&gt;\n    &lt;result type=\"text\"&gt;page is now available&lt;/result&gt;\n  &lt;/results&gt;\n&lt;/tool-call&gt;\nI have generated the game for you. It should be opening in your browser at\nhttp://localhost:8080.\n:::\n\n\n\n\nNative tools allow you to access functionality that is built directly into the LLM provider’s backend, such as web search or a code interpreter environment for data analysis.\nSupport for native tools varies by provider.\n\n\nYou enable native tools by specifying their type.\ntools:\n  - native: search # Enable the provider's built-in web search.\n  - native: code   # Enable the provider's built-in code interpreter.\n\n\n\n\nGemini: Supports both search and code. Note that the Gemini API has a limitation where you can only use one native tool at a time, and it cannot be combined with other (non-native) tools.\nAnthropic: Supports search only.\nOpenAI: Supports both search and code via the openai provider (not the legacy openai/chat provider).",
    "crumbs": [
      "Tools",
      "Other Tools"
    ]
  },
  {
    "objectID": "tools/06_other_tools.html#the-think-tool",
    "href": "tools/06_other_tools.html#the-think-tool",
    "title": "Other Tools: think, serve, and native",
    "section": "",
    "text": "The think tool gives the LLM a private “scratch space” to pause and reason about a prompt before formulating its final response. This can improve the quality and thoughtfulness of the output, especially for complex or ambiguous questions.\nThis technique was inspired by a post on Anthropic’s engineering blog. The output of the think tool is hidden from the user by default in the editor plugins, though it is still present in the .lec file.\n\n\ntools:\n  - think_about: &gt;\n      What the user is really asking for, and what hidden assumptions they\n      might have.\n    name: scratchpad # Optional name\n\n\n\nWhat's the best city in the world?\n\n:::Assistant\n&lt;tool-call with=\"scratchpad\"&gt;\n  &lt;arguments&gt;\n    &lt;thought&gt;\"Best\" is subjective. The user could mean best for travel, for\n    food, for work, etc. I need to ask for clarification.&lt;/thought&gt;\n  &lt;/arguments&gt;\n  &lt;results&gt;\n    &lt;result type=\"text\"&gt;thought complete.&lt;/result&gt;\n  &lt;/results&gt;\n&lt;/tool-call&gt;\nThat depends on what you're looking for! Are you interested in the best city\nfor tourism, career opportunities, or something else?\n:::",
    "crumbs": [
      "Tools",
      "Other Tools"
    ]
  },
  {
    "objectID": "tools/06_other_tools.html#the-serve-tool",
    "href": "tools/06_other_tools.html#the-serve-tool",
    "title": "Other Tools: think, serve, and native",
    "section": "",
    "text": "The serve tool allows the LLM to spin up a simple, single-use web server to present content, such as an HTML file or a small web application it has generated.\nWhen the LLM uses this tool, Lectic starts a server on the specified port. It will then attempt to open the page in your default web browser. The server shuts down automatically after the first request is served. While the page is loading, Lectic waits for the first request—so the conversation resumes once your browser has loaded the page.\n\n\ntools:\n  - serve_on_port: 8080\n    name: web_server # Optional name\n\n\n\nGenerate a simple tic-tac-toe game in HTML and serve it to me.\n\n:::Assistant\n&lt;tool-call with=\"web_server\"&gt;\n  &lt;arguments&gt;\n    &lt;pageHtml&gt;\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;title&gt;Tic-Tac-Toe&lt;/title&gt;\n  ... (rest of the HTML/JS/CSS) ...\n&lt;/head&gt;\n&lt;body&gt;\n  ...\n&lt;/body&gt;\n&lt;/html&gt;\n    &lt;/pageHtml&gt;\n  &lt;/arguments&gt;\n  &lt;results&gt;\n    &lt;result type=\"text\"&gt;page is now available&lt;/result&gt;\n  &lt;/results&gt;\n&lt;/tool-call&gt;\nI have generated the game for you. It should be opening in your browser at\nhttp://localhost:8080.\n:::",
    "crumbs": [
      "Tools",
      "Other Tools"
    ]
  },
  {
    "objectID": "tools/06_other_tools.html#native-tools",
    "href": "tools/06_other_tools.html#native-tools",
    "title": "Other Tools: think, serve, and native",
    "section": "",
    "text": "Native tools allow you to access functionality that is built directly into the LLM provider’s backend, such as web search or a code interpreter environment for data analysis.\nSupport for native tools varies by provider.\n\n\nYou enable native tools by specifying their type.\ntools:\n  - native: search # Enable the provider's built-in web search.\n  - native: code   # Enable the provider's built-in code interpreter.\n\n\n\n\nGemini: Supports both search and code. Note that the Gemini API has a limitation where you can only use one native tool at a time, and it cannot be combined with other (non-native) tools.\nAnthropic: Supports search only.\nOpenAI: Supports both search and code via the openai provider (not the legacy openai/chat provider).",
    "crumbs": [
      "Tools",
      "Other Tools"
    ]
  },
  {
    "objectID": "tools/05_agent.html",
    "href": "tools/05_agent.html",
    "title": "Tools: Agent",
    "section": "",
    "text": "The agent tool allows you to create sophisticated multi-LLM workflows by enabling one interlocutor to call another as a tool. The “agent” interlocutor receives the query from the “caller” with no other context, processes it, and returns its response as the tool’s output.\nThis is a powerful way to separate concerns. You can create specialized agents and then compose them into more complex systems. For an excellent overview of the philosophy behind this approach, see Anthropic’s blog post on their multi-agent research system.\n\n\nTo use the agent tool, you must have at least two interlocutors defined. In the configuration for one interlocutor, you add an agent tool that points to the name of the other.\n\nagent: (Required) The name of the interlocutor to be called as an agent.\nname: An optional name for the tool.\nusage: A string, file:, or exec: URI providing instructions for the calling LLM on when and how to use this agent.\nraw_output: A boolean value. Normally an agent’s output will be sanitized, so that raw tool call results are not visible to the interlocutor who called the agent. Setting raw_output to true puts the full output from the agent into the main interlocutor’s tool call results.\n\n\n\nIn this setup, Kirk is the main interlocutor. He has a tool named communicator which, when used, will call the Spock interlocutor. Spock has his own set of tools, including a think_about tool to encourage careful reasoning.\ninterlocutors:\n  - name: Kirk\n    prompt: You are Captain Kirk. You are bold and decisive.\n    tools:\n      - agent: Spock\n        name: communicator\n        usage: Use this to contact Spock for logical analysis and advice.\n\n  - name: Spock\n    prompt: &gt;\n      You are Mr. Spock. You respond with pure logic, suppressing all\n      emotion.\n    tools:\n      - think_about: how to logically solve the problem presented.\n\n\n\n\nUsing the configuration above, Captain Kirk can delegate complex analysis to Spock.\nWe've encountered an alien vessel of unknown origin. It is not responding to\nhails. What is the logical course of action?\n\n:::Kirk\n\nThis situation requires careful analysis. I will consult my science officer.\n\n&lt;tool-call with=\"communicator\"&gt;\n  &lt;arguments&gt;\n    &lt;content&gt;Alien vessel, unknown origin, unresponsive. Propose logical\n    course of action.&lt;/content&gt;\n  &lt;/arguments&gt;\n  &lt;results&gt;\n    &lt;result type=\"text\"&gt;Insufficient data. Recommend passive scans to gather\n    information on their technological capabilities before initiating\n    further contact. Avoid any action that could be perceived as\n    hostile.&lt;/result&gt;\n  &lt;/results&gt;\n&lt;/tool-call&gt;\n\nA logical approach. We will proceed with passive scans.\n\n:::",
    "crumbs": [
      "Tools",
      "Agent"
    ]
  },
  {
    "objectID": "tools/05_agent.html#configuration",
    "href": "tools/05_agent.html#configuration",
    "title": "Tools: Agent",
    "section": "",
    "text": "To use the agent tool, you must have at least two interlocutors defined. In the configuration for one interlocutor, you add an agent tool that points to the name of the other.\n\nagent: (Required) The name of the interlocutor to be called as an agent.\nname: An optional name for the tool.\nusage: A string, file:, or exec: URI providing instructions for the calling LLM on when and how to use this agent.\nraw_output: A boolean value. Normally an agent’s output will be sanitized, so that raw tool call results are not visible to the interlocutor who called the agent. Setting raw_output to true puts the full output from the agent into the main interlocutor’s tool call results.\n\n\n\nIn this setup, Kirk is the main interlocutor. He has a tool named communicator which, when used, will call the Spock interlocutor. Spock has his own set of tools, including a think_about tool to encourage careful reasoning.\ninterlocutors:\n  - name: Kirk\n    prompt: You are Captain Kirk. You are bold and decisive.\n    tools:\n      - agent: Spock\n        name: communicator\n        usage: Use this to contact Spock for logical analysis and advice.\n\n  - name: Spock\n    prompt: &gt;\n      You are Mr. Spock. You respond with pure logic, suppressing all\n      emotion.\n    tools:\n      - think_about: how to logically solve the problem presented.",
    "crumbs": [
      "Tools",
      "Agent"
    ]
  },
  {
    "objectID": "tools/05_agent.html#example-conversation",
    "href": "tools/05_agent.html#example-conversation",
    "title": "Tools: Agent",
    "section": "",
    "text": "Using the configuration above, Captain Kirk can delegate complex analysis to Spock.\nWe've encountered an alien vessel of unknown origin. It is not responding to\nhails. What is the logical course of action?\n\n:::Kirk\n\nThis situation requires careful analysis. I will consult my science officer.\n\n&lt;tool-call with=\"communicator\"&gt;\n  &lt;arguments&gt;\n    &lt;content&gt;Alien vessel, unknown origin, unresponsive. Propose logical\n    course of action.&lt;/content&gt;\n  &lt;/arguments&gt;\n  &lt;results&gt;\n    &lt;result type=\"text\"&gt;Insufficient data. Recommend passive scans to gather\n    information on their technological capabilities before initiating\n    further contact. Avoid any action that could be perceived as\n    hostile.&lt;/result&gt;\n  &lt;/results&gt;\n&lt;/tool-call&gt;\n\nA logical approach. We will proceed with passive scans.\n\n:::",
    "crumbs": [
      "Tools",
      "Agent"
    ]
  },
  {
    "objectID": "03_conversation_format.html",
    "href": "03_conversation_format.html",
    "title": "The Lectic Conversation Format",
    "section": "",
    "text": "Lectic conversations are stored in plain markdown files, typically with a .lec extension. They use a superset of CommonMark, adding two specific conventions: a YAML frontmatter block for configuration and “container directives” for assistant responses.\n\n\nEvery Lectic file begins with a YAML frontmatter block, enclosed by three dashes (---). This is where you configure the conversation, defining the interlocutor(s), their models, prompts, and any tools they might use.\nA minimal header looks like this:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic\n  model: claude-3-haiku-20240307\n---\nThe frontmatter can be closed with either three dashes (---) or three periods (...). For a complete guide to all available options, see the Configuration page.\n\n\n\nAnything in the file that is not part of the YAML frontmatter or an assistant response block is considered a user message. You write your prompts, questions, and instructions here as plain text or standard markdown.\nThis is a user message.\n\nSo is this. You can include any markdown you like, such as **bold text** or\n`inline code`.\n\n\n\nLectic uses “container directives” to represent messages from the LLM. These are fenced blocks that start with a run of colons, followed immediately by the name of the interlocutor.\nThe canonical form is exactly three colons on open and close, like this:\n:::Name\n\nSome content.\n\n:::\nMarkdown code fences inside assistant blocks can also use three backticks.\n\n\nHere is a complete, simple conversation file showing all the parts together:\n---\ninterlocutor:\n  name: Oggle\n  prompt: You are a skeptical assistant.\n---\n\nI'd like to know more about container directives.\n\n:::Oggle\n\nAre you sure? It seems like a rather niche topic. They are part of a\nproposed extension to CommonMark that allows for custom block-level\nelements.\n\nInside one of these blocks, standard markdown is still supported:\n\n```python\n# This is a regular code block\nprint(\"Hello from inside a directive!\")\n```\n\nIs that all you wanted to know?\n\n:::\nWhen you run lectic, it reads the entire file, sends the content to the LLM, and then appends the next assistant response in a new directive block.",
    "crumbs": [
      "Conversation Format"
    ]
  },
  {
    "objectID": "03_conversation_format.html#yaml-frontmatter",
    "href": "03_conversation_format.html#yaml-frontmatter",
    "title": "The Lectic Conversation Format",
    "section": "",
    "text": "Every Lectic file begins with a YAML frontmatter block, enclosed by three dashes (---). This is where you configure the conversation, defining the interlocutor(s), their models, prompts, and any tools they might use.\nA minimal header looks like this:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic\n  model: claude-3-haiku-20240307\n---\nThe frontmatter can be closed with either three dashes (---) or three periods (...). For a complete guide to all available options, see the Configuration page.",
    "crumbs": [
      "Conversation Format"
    ]
  },
  {
    "objectID": "03_conversation_format.html#user-messages",
    "href": "03_conversation_format.html#user-messages",
    "title": "The Lectic Conversation Format",
    "section": "",
    "text": "Anything in the file that is not part of the YAML frontmatter or an assistant response block is considered a user message. You write your prompts, questions, and instructions here as plain text or standard markdown.\nThis is a user message.\n\nSo is this. You can include any markdown you like, such as **bold text** or\n`inline code`.",
    "crumbs": [
      "Conversation Format"
    ]
  },
  {
    "objectID": "03_conversation_format.html#assistant-responses",
    "href": "03_conversation_format.html#assistant-responses",
    "title": "The Lectic Conversation Format",
    "section": "",
    "text": "Lectic uses “container directives” to represent messages from the LLM. These are fenced blocks that start with a run of colons, followed immediately by the name of the interlocutor.\nThe canonical form is exactly three colons on open and close, like this:\n:::Name\n\nSome content.\n\n:::\nMarkdown code fences inside assistant blocks can also use three backticks.\n\n\nHere is a complete, simple conversation file showing all the parts together:\n---\ninterlocutor:\n  name: Oggle\n  prompt: You are a skeptical assistant.\n---\n\nI'd like to know more about container directives.\n\n:::Oggle\n\nAre you sure? It seems like a rather niche topic. They are part of a\nproposed extension to CommonMark that allows for custom block-level\nelements.\n\nInside one of these blocks, standard markdown is still supported:\n\n```python\n# This is a regular code block\nprint(\"Hello from inside a directive!\")\n```\n\nIs that all you wanted to know?\n\n:::\nWhen you run lectic, it reads the entire file, sends the content to the LLM, and then appends the next assistant response in a new directive block.",
    "crumbs": [
      "Conversation Format"
    ]
  },
  {
    "objectID": "automation/02_hooks.html",
    "href": "automation/02_hooks.html",
    "title": "Automation: Hooks",
    "section": "",
    "text": "Hooks are a powerful automation feature that let you run custom commands and scripts in response to events in Lectic’s lifecycle. Use them for logging, notifications, post‑processing, or integrating with other tools and workflows.\nHooks are defined in your YAML configuration under the hooks key.\n\n\nA hook has two fields:\n\non: A single event name or a list of event names to listen for.\ndo: The command or inline script to run when the event fires.\n\nhooks:\n  - on: [assistant_message, user_message]\n    do: ./log-activity.sh\nIf do contains multiple lines, it is treated as a script and must begin with a shebang (e.g., #!/bin/bash). If it is a single line, it is treated as a command. Commands are executed directly (not through a shell), so shell features like command substitution will not work.\nHook commands run synchronously. Their stdout, stderr, and exit status are ignored by Lectic. Use your own logging inside the hook if you want persistent records.\n\n\n\nLectic emits three hook events. When an event fires, the hook process receives its context as environment variables. No positional arguments are passed.\n\nuser_message\n\nEnvironment:\n\nUSER_MESSAGE: The text of the most recent user message.\nStandard Lectic variables like LECTIC_FILE, LECTIC_CONFIG, LECTIC_DATA, LECTIC_CACHE, LECTIC_STATE, and LECTIC_TEMP are also set when available.\n\nWhen: Just before the request is sent to the LLM provider.\n\nassistant_message\n\nEnvironment:\n\nASSISTANT_MESSAGE: The full text of the assistant’s response that was just produced.\nLECTIC_INTERLOCUTOR: The name of the interlocutor who spoke.\nStandard Lectic variables as above.\n\nWhen: Immediately after the assistant’s message is streamed.\n\nerror\n\nEnvironment:\n\nERROR_MESSAGE: A descriptive error message.\nStandard Lectic variables as above.\n\nWhen: Whenever an uncaught error is encountered.\n\n\n\n\n\nThis example persists every user and assistant message to an SQLite database located in your Lectic data directory. You can later query this for personal memory, project history, or analytics.\nConfiguration:\nhooks:\n  - on: [user_message, assistant_message]\n    do: |\n      #!/usr/bin/env bash\n      set -euo pipefail\n      DB_ROOT=\"${LECTIC_DATA:-$HOME/.local/share/lectic}\"\n      DB_PATH=\"${DB_ROOT}/memory.sqlite3\"\n      mkdir -p \"${DB_ROOT}\"\n\n      # Determine role and text from available variables\n      if [[ -n \"${ASSISTANT_MESSAGE:-}\" ]]; then\n        ROLE=\"assistant\"\n        TEXT=\"$ASSISTANT_MESSAGE\"\n      else\n        ROLE=\"user\"\n        TEXT=\"${USER_MESSAGE:-}\"\n      fi\n\n      # Basic sanitizer for single quotes for SQL literal\n      esc_sq() { printf %s \"$1\" | sed \"s/'/''/g\"; }\n\n      TS=$(date -Is)\n      FILE_PATH=\"${LECTIC_FILE:-}\"\n      NAME=\"${LECTIC_INTERLOCUTOR:-}\"\n\n      sqlite3 \"$DB_PATH\" &lt;&lt;SQL\n      CREATE TABLE IF NOT EXISTS memory (\n        id INTEGER PRIMARY KEY,\n        ts TEXT NOT NULL,\n        role TEXT NOT NULL,\n        interlocutor TEXT,\n        file TEXT,\n        text TEXT NOT NULL\n      );\n      INSERT INTO memory(ts, role, interlocutor, file, text)\n      VALUES ('${TS}', '${ROLE}', '$(esc_sq \"$NAME\")',\n              '$(esc_sq \"$FILE_PATH\")', '$(esc_sq \"$TEXT\")');\n      SQL\nNotes:\n\nRequires the sqlite3 command-line tool to be installed and on your PATH.\nThe hook inspects which variable is set to decide whether the event was a user or assistant message.\nLECTIC_FILE is populated when using -f/-i and may be empty when streaming from stdin.\nAdjust the table schema to suit your use case.",
    "crumbs": [
      "Automation",
      "Hooks"
    ]
  },
  {
    "objectID": "automation/02_hooks.html#hook-configuration",
    "href": "automation/02_hooks.html#hook-configuration",
    "title": "Automation: Hooks",
    "section": "",
    "text": "A hook has two fields:\n\non: A single event name or a list of event names to listen for.\ndo: The command or inline script to run when the event fires.\n\nhooks:\n  - on: [assistant_message, user_message]\n    do: ./log-activity.sh\nIf do contains multiple lines, it is treated as a script and must begin with a shebang (e.g., #!/bin/bash). If it is a single line, it is treated as a command. Commands are executed directly (not through a shell), so shell features like command substitution will not work.\nHook commands run synchronously. Their stdout, stderr, and exit status are ignored by Lectic. Use your own logging inside the hook if you want persistent records.",
    "crumbs": [
      "Automation",
      "Hooks"
    ]
  },
  {
    "objectID": "automation/02_hooks.html#available-events-and-environment",
    "href": "automation/02_hooks.html#available-events-and-environment",
    "title": "Automation: Hooks",
    "section": "",
    "text": "Lectic emits three hook events. When an event fires, the hook process receives its context as environment variables. No positional arguments are passed.\n\nuser_message\n\nEnvironment:\n\nUSER_MESSAGE: The text of the most recent user message.\nStandard Lectic variables like LECTIC_FILE, LECTIC_CONFIG, LECTIC_DATA, LECTIC_CACHE, LECTIC_STATE, and LECTIC_TEMP are also set when available.\n\nWhen: Just before the request is sent to the LLM provider.\n\nassistant_message\n\nEnvironment:\n\nASSISTANT_MESSAGE: The full text of the assistant’s response that was just produced.\nLECTIC_INTERLOCUTOR: The name of the interlocutor who spoke.\nStandard Lectic variables as above.\n\nWhen: Immediately after the assistant’s message is streamed.\n\nerror\n\nEnvironment:\n\nERROR_MESSAGE: A descriptive error message.\nStandard Lectic variables as above.\n\nWhen: Whenever an uncaught error is encountered.",
    "crumbs": [
      "Automation",
      "Hooks"
    ]
  },
  {
    "objectID": "automation/02_hooks.html#example-log-messages-to-sqlite-for-longterm-memory",
    "href": "automation/02_hooks.html#example-log-messages-to-sqlite-for-longterm-memory",
    "title": "Automation: Hooks",
    "section": "",
    "text": "This example persists every user and assistant message to an SQLite database located in your Lectic data directory. You can later query this for personal memory, project history, or analytics.\nConfiguration:\nhooks:\n  - on: [user_message, assistant_message]\n    do: |\n      #!/usr/bin/env bash\n      set -euo pipefail\n      DB_ROOT=\"${LECTIC_DATA:-$HOME/.local/share/lectic}\"\n      DB_PATH=\"${DB_ROOT}/memory.sqlite3\"\n      mkdir -p \"${DB_ROOT}\"\n\n      # Determine role and text from available variables\n      if [[ -n \"${ASSISTANT_MESSAGE:-}\" ]]; then\n        ROLE=\"assistant\"\n        TEXT=\"$ASSISTANT_MESSAGE\"\n      else\n        ROLE=\"user\"\n        TEXT=\"${USER_MESSAGE:-}\"\n      fi\n\n      # Basic sanitizer for single quotes for SQL literal\n      esc_sq() { printf %s \"$1\" | sed \"s/'/''/g\"; }\n\n      TS=$(date -Is)\n      FILE_PATH=\"${LECTIC_FILE:-}\"\n      NAME=\"${LECTIC_INTERLOCUTOR:-}\"\n\n      sqlite3 \"$DB_PATH\" &lt;&lt;SQL\n      CREATE TABLE IF NOT EXISTS memory (\n        id INTEGER PRIMARY KEY,\n        ts TEXT NOT NULL,\n        role TEXT NOT NULL,\n        interlocutor TEXT,\n        file TEXT,\n        text TEXT NOT NULL\n      );\n      INSERT INTO memory(ts, role, interlocutor, file, text)\n      VALUES ('${TS}', '${ROLE}', '$(esc_sq \"$NAME\")',\n              '$(esc_sq \"$FILE_PATH\")', '$(esc_sq \"$TEXT\")');\n      SQL\nNotes:\n\nRequires the sqlite3 command-line tool to be installed and on your PATH.\nThe hook inspects which variable is set to decide whether the event was a user or assistant message.\nLECTIC_FILE is populated when using -f/-i and may be empty when streaming from stdin.\nAdjust the table schema to suit your use case.",
    "crumbs": [
      "Automation",
      "Hooks"
    ]
  }
]