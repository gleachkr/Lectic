[
  {
    "objectID": "cookbook/06_custom_sandboxing.html",
    "href": "cookbook/06_custom_sandboxing.html",
    "title": "Recipe: Custom Sandboxing",
    "section": "",
    "text": "Giving an LLM access to exec tools is powerful, but it carries risk. While Lectic provides a sandbox configuration option, it doesn’t enforce a specific technology. Instead, it delegates execution to a script you control.\nThis recipe walks you through the sandbox script mechanism and helps you write wrappers to isolate tool execution.\n\n\nWhen you configure a sandbox in your lectic.yaml, Lectic wraps the execution of exec tools and local mcp_command tools. Instead of executing the tool directly, it executes your sandbox script and passes the tool’s command and arguments as arguments to that script.\nAn exec tool without sandbox: Lectic runs: ls -la\nWith sandbox: ./wrapper.sh: Lectic runs: ./wrapper.sh ls -la\nThe command to launch an MCP server is wrapped by the sandbox script in a similar way.\nYour sandbox script is responsible for:\n\nSetting up the environment.\nExecuting the command (passed in $@).\nCleaning up.\nReturning the exit code.\n\n\n\n\nBefore trying to isolate the filesystem, let’s make a wrapper that simply logs every command the assistant tries to run. This is useful for auditing.\nCreate ~/.config/lectic/audit.sh:\n#!/bin/bash\n# Append timestamp and command to a log file\necho \"[$(date)] Executing: $*\" &gt;&gt; \"$HOME/.lectic_audit_log\"\n\n# Run the actual command\nexec \"$@\"\nMake it executable:\nchmod +x ~/.config/lectic/audit.sh\nConfigure it in lectic.yaml:\ninterlocutor:\n  name: Assistant\n  # Apply to all exec/local MCP tools for this interlocutor\n  sandbox: ~/.config/lectic/audit.sh\n  tools:\n    - exec: ls\n\n\n\nFor actual safety, we can use Bubblewrap (bwrap). This tool creates a new namespace for the process, allowing you to control exactly which parts of your filesystem the assistant can see or write to.\nHere is a simplified version of the lectic-bwrap script found in the Lectic repository. It creates a read-only view of the system but gives the assistant a temporary, empty home directory.\nCreate ~/.config/lectic/safe-run.sh:\n#!/bin/bash\nset -euo pipefail\n\n# Create a temporary directory for the assistant's \"home\"\nFAKE_HOME=$(mktemp -d)\n\n# Ensure we clean up the temp dir when the script exits\ntrap 'rm -rf \"$FAKE_HOME\"' EXIT\n\n# Run bwrap with specific permissions\nbwrap \\\n  --ro-bind / / \\                 # Mount the root as read-only\n  --dev /dev \\                    # legitimate devices\n  --proc /proc \\                  # legitimate processes\n  --bind \"$PWD\" \"$PWD\" \\          # Allow read-write access to current project\n  --bind \"$FAKE_HOME\" \"$HOME\" \\   # Fake the home directory\n  --unshare-net \\                 # Disable network access (optional)\n  --die-with-parent \\             # Kill process if lectic dies\n  \"$@\"\n\n\n\nRead-only Root: The assistant cannot modify system files (/usr, /bin, etc.).\nFake Home: If the assistant runs rm -rf ~, it only deletes the temporary directory, not your actual home folder.\nProject Access: The script explicitly binds $PWD, so the assistant can still read and write files in the directory where you ran Lectic.\nNo Network: The --unshare-net flag prevents the assistant from making outbound connections (remove this if you want it to use curl or something similar).\n\n\n\n\n\nIsolating the filesystem by copying the project to a temporary directory is a great way to protect your work. However, simple scripts that create a new directory for every command will break stateful workflows (e.g., git init followed by git commit won’t work if they run in different directories).\nTo fix this, we need a stateful sandbox that persists across tool calls. We can use environment variables provided by Lectic to identify the session.\nCreate ~/.config/lectic/shadow-run.sh:\n#!/bin/bash\nset -euo pipefail\n\n# 1. Generate a stable path for this project + interlocutor\n# Using a hash of the current directory ensures we get a unique sandbox per project\nPROJ_HASH=$(echo -n \"$PWD\" | md5sum | awk '{print $1}')\nSANDBOX_ROOT=\"${TMPDIR:-/tmp}/lectic-sandbox-${PROJ_HASH}\"\n# LECTIC_INTERLOCUTOR is provided by Lectic\nSANDBOX_DIR=\"$SANDBOX_ROOT/${LECTIC_INTERLOCUTOR:-default}\"\n\n# 2. Initialize the sandbox if it's new\nif [[ ! -d \"$SANDBOX_DIR\" ]]; then\n  echo \"Initializing sandbox at $SANDBOX_DIR...\" &gt;&2\n  mkdir -p \"$SANDBOX_DIR\"\n  # Copy the project to the sandbox\n  cp -r . \"$SANDBOX_DIR\"\nfi\n\ncd \"$SANDBOX_DIR\"\n\n# 3. Run the command\n\"$@\"\nThis script creates a “shadow” copy of your project that persists as long as you don’t delete the temporary directory. The assistant can make changes, run builds, and edit files without affecting your real project. If you like the results, you can manually copy them back.\n\n\n\nYou can apply sandboxes globally to an interlocutor or to specific tools.\nGlobal (Recommended): This ensures every exec tool the assistant uses is wrapped.\ninterlocutor:\n  name: Assistant\n  sandbox: ~/.config/lectic/safe-run.sh\n  tools:\n    - exec: bash\n    - exec: python3\nPer-Tool: Useful if you have a specific “dangerous” tool that needs isolation while others (like ls) can run natively.\ntools:\n  - exec: rm\n    name: delete_files\n    sandbox: ~/.config/lectic/safe-run.sh\n  - exec: ls\n    name: list_files\n    # No sandbox\n\n\n\n\nEnvironment variables: Lectic passes variables like LECTIC_INTERLOCUTOR into sandboxed commands. Use these for per-interlocutor state (for example, separate scratch directories). See Exec Tool and Configuration Reference.\nQuoting and arguments: A sandbox is a command string. If you need complex quoting or structured options, prefer writing a wrapper script.\nPerformance matters: Tools can be called in tight loops. Heavy sandboxes like docker run can add significant latency. Prefer docker exec into a long-running container if you go the Docker route.\nTest your sandbox: Verify it blocks what you think it blocks. Try to access files outside the allowed roots and confirm it fails.",
    "crumbs": [
      "Cookbook",
      "Custom Sandboxes"
    ]
  },
  {
    "objectID": "cookbook/06_custom_sandboxing.html#the-sandbox-protocol",
    "href": "cookbook/06_custom_sandboxing.html#the-sandbox-protocol",
    "title": "Recipe: Custom Sandboxing",
    "section": "",
    "text": "When you configure a sandbox in your lectic.yaml, Lectic wraps the execution of exec tools and local mcp_command tools. Instead of executing the tool directly, it executes your sandbox script and passes the tool’s command and arguments as arguments to that script.\nAn exec tool without sandbox: Lectic runs: ls -la\nWith sandbox: ./wrapper.sh: Lectic runs: ./wrapper.sh ls -la\nThe command to launch an MCP server is wrapped by the sandbox script in a similar way.\nYour sandbox script is responsible for:\n\nSetting up the environment.\nExecuting the command (passed in $@).\nCleaning up.\nReturning the exit code.",
    "crumbs": [
      "Cookbook",
      "Custom Sandboxes"
    ]
  },
  {
    "objectID": "cookbook/06_custom_sandboxing.html#level-1-observability-wrapper",
    "href": "cookbook/06_custom_sandboxing.html#level-1-observability-wrapper",
    "title": "Recipe: Custom Sandboxing",
    "section": "",
    "text": "Before trying to isolate the filesystem, let’s make a wrapper that simply logs every command the assistant tries to run. This is useful for auditing.\nCreate ~/.config/lectic/audit.sh:\n#!/bin/bash\n# Append timestamp and command to a log file\necho \"[$(date)] Executing: $*\" &gt;&gt; \"$HOME/.lectic_audit_log\"\n\n# Run the actual command\nexec \"$@\"\nMake it executable:\nchmod +x ~/.config/lectic/audit.sh\nConfigure it in lectic.yaml:\ninterlocutor:\n  name: Assistant\n  # Apply to all exec/local MCP tools for this interlocutor\n  sandbox: ~/.config/lectic/audit.sh\n  tools:\n    - exec: ls",
    "crumbs": [
      "Cookbook",
      "Custom Sandboxes"
    ]
  },
  {
    "objectID": "cookbook/06_custom_sandboxing.html#level-2-filesystem-isolation-bubblewrap",
    "href": "cookbook/06_custom_sandboxing.html#level-2-filesystem-isolation-bubblewrap",
    "title": "Recipe: Custom Sandboxing",
    "section": "",
    "text": "For actual safety, we can use Bubblewrap (bwrap). This tool creates a new namespace for the process, allowing you to control exactly which parts of your filesystem the assistant can see or write to.\nHere is a simplified version of the lectic-bwrap script found in the Lectic repository. It creates a read-only view of the system but gives the assistant a temporary, empty home directory.\nCreate ~/.config/lectic/safe-run.sh:\n#!/bin/bash\nset -euo pipefail\n\n# Create a temporary directory for the assistant's \"home\"\nFAKE_HOME=$(mktemp -d)\n\n# Ensure we clean up the temp dir when the script exits\ntrap 'rm -rf \"$FAKE_HOME\"' EXIT\n\n# Run bwrap with specific permissions\nbwrap \\\n  --ro-bind / / \\                 # Mount the root as read-only\n  --dev /dev \\                    # legitimate devices\n  --proc /proc \\                  # legitimate processes\n  --bind \"$PWD\" \"$PWD\" \\          # Allow read-write access to current project\n  --bind \"$FAKE_HOME\" \"$HOME\" \\   # Fake the home directory\n  --unshare-net \\                 # Disable network access (optional)\n  --die-with-parent \\             # Kill process if lectic dies\n  \"$@\"\n\n\n\nRead-only Root: The assistant cannot modify system files (/usr, /bin, etc.).\nFake Home: If the assistant runs rm -rf ~, it only deletes the temporary directory, not your actual home folder.\nProject Access: The script explicitly binds $PWD, so the assistant can still read and write files in the directory where you ran Lectic.\nNo Network: The --unshare-net flag prevents the assistant from making outbound connections (remove this if you want it to use curl or something similar).",
    "crumbs": [
      "Cookbook",
      "Custom Sandboxes"
    ]
  },
  {
    "objectID": "cookbook/06_custom_sandboxing.html#level-3-stateful-isolation-the-shadow-workspace",
    "href": "cookbook/06_custom_sandboxing.html#level-3-stateful-isolation-the-shadow-workspace",
    "title": "Recipe: Custom Sandboxing",
    "section": "",
    "text": "Isolating the filesystem by copying the project to a temporary directory is a great way to protect your work. However, simple scripts that create a new directory for every command will break stateful workflows (e.g., git init followed by git commit won’t work if they run in different directories).\nTo fix this, we need a stateful sandbox that persists across tool calls. We can use environment variables provided by Lectic to identify the session.\nCreate ~/.config/lectic/shadow-run.sh:\n#!/bin/bash\nset -euo pipefail\n\n# 1. Generate a stable path for this project + interlocutor\n# Using a hash of the current directory ensures we get a unique sandbox per project\nPROJ_HASH=$(echo -n \"$PWD\" | md5sum | awk '{print $1}')\nSANDBOX_ROOT=\"${TMPDIR:-/tmp}/lectic-sandbox-${PROJ_HASH}\"\n# LECTIC_INTERLOCUTOR is provided by Lectic\nSANDBOX_DIR=\"$SANDBOX_ROOT/${LECTIC_INTERLOCUTOR:-default}\"\n\n# 2. Initialize the sandbox if it's new\nif [[ ! -d \"$SANDBOX_DIR\" ]]; then\n  echo \"Initializing sandbox at $SANDBOX_DIR...\" &gt;&2\n  mkdir -p \"$SANDBOX_DIR\"\n  # Copy the project to the sandbox\n  cp -r . \"$SANDBOX_DIR\"\nfi\n\ncd \"$SANDBOX_DIR\"\n\n# 3. Run the command\n\"$@\"\nThis script creates a “shadow” copy of your project that persists as long as you don’t delete the temporary directory. The assistant can make changes, run builds, and edit files without affecting your real project. If you like the results, you can manually copy them back.",
    "crumbs": [
      "Cookbook",
      "Custom Sandboxes"
    ]
  },
  {
    "objectID": "cookbook/06_custom_sandboxing.html#configuration-usage",
    "href": "cookbook/06_custom_sandboxing.html#configuration-usage",
    "title": "Recipe: Custom Sandboxing",
    "section": "",
    "text": "You can apply sandboxes globally to an interlocutor or to specific tools.\nGlobal (Recommended): This ensures every exec tool the assistant uses is wrapped.\ninterlocutor:\n  name: Assistant\n  sandbox: ~/.config/lectic/safe-run.sh\n  tools:\n    - exec: bash\n    - exec: python3\nPer-Tool: Useful if you have a specific “dangerous” tool that needs isolation while others (like ls) can run natively.\ntools:\n  - exec: rm\n    name: delete_files\n    sandbox: ~/.config/lectic/safe-run.sh\n  - exec: ls\n    name: list_files\n    # No sandbox",
    "crumbs": [
      "Cookbook",
      "Custom Sandboxes"
    ]
  },
  {
    "objectID": "cookbook/06_custom_sandboxing.html#tips",
    "href": "cookbook/06_custom_sandboxing.html#tips",
    "title": "Recipe: Custom Sandboxing",
    "section": "",
    "text": "Environment variables: Lectic passes variables like LECTIC_INTERLOCUTOR into sandboxed commands. Use these for per-interlocutor state (for example, separate scratch directories). See Exec Tool and Configuration Reference.\nQuoting and arguments: A sandbox is a command string. If you need complex quoting or structured options, prefer writing a wrapper script.\nPerformance matters: Tools can be called in tight loops. Heavy sandboxes like docker run can add significant latency. Prefer docker exec into a long-running container if you go the Docker route.\nTest your sandbox: Verify it blocks what you think it blocks. Try to access files outside the allowed roots and confirm it fails.",
    "crumbs": [
      "Cookbook",
      "Custom Sandboxes"
    ]
  },
  {
    "objectID": "cookbook/04_memory.html",
    "href": "cookbook/04_memory.html",
    "title": "Recipe: Conversation Memory",
    "section": "",
    "text": "This recipe shows two approaches to giving your assistant memory across conversations: explicit recall via a tool, and automatic context via hooks. These serve different purposes and have different tradeoffs.\n\n\nIn this approach, everything is recorded automatically, but the assistant must explicitly search for relevant memories. This keeps the prompt lean and preserves cache efficiency.\n\n\nAdd this hook to save every message:\nhooks:\n  - on: [user_message, assistant_message]\n    do: |\n      #!/bin/bash\n      set -euo pipefail\n      \n      DB=\"${LECTIC_DATA}/memory.sqlite3\"\n      \n      # Initialize database if needed\n      sqlite3 \"$DB\" &lt;&lt;'SQL'\n      CREATE TABLE IF NOT EXISTS messages (\n        id INTEGER PRIMARY KEY,\n        timestamp TEXT NOT NULL,\n        role TEXT NOT NULL,\n        interlocutor TEXT,\n        file TEXT,\n        content TEXT NOT NULL\n      );\n      CREATE INDEX IF NOT EXISTS idx_timestamp ON messages(timestamp);\n      SQL\n      \n      # Determine role and content\n      if [[ -n \"${ASSISTANT_MESSAGE:-}\" ]]; then\n        ROLE=\"assistant\"\n        CONTENT=\"$ASSISTANT_MESSAGE\"\n        NAME=\"${LECTIC_INTERLOCUTOR:-}\"\n      else\n        ROLE=\"user\"\n        CONTENT=\"${USER_MESSAGE:-}\"\n        NAME=\"\"\n      fi\n      \n      # Escape single quotes for SQL\n      CONTENT_ESC=\"${CONTENT//\\'/\\'\\'}\"\n      NAME_ESC=\"${NAME//\\'/\\'\\'}\"\n      FILE_ESC=\"${LECTIC_FILE//\\'/\\'\\'}\"\n      \n      sqlite3 \"$DB\" &lt;&lt;SQL\n      INSERT INTO messages (timestamp, role, interlocutor, file, content)\n      VALUES (datetime('now'), '$ROLE', '$NAME_ESC', '$FILE_ESC', '$CONTENT_ESC');\n      SQL\n\n\n\nGive the assistant a tool to search memory:\ntools:\n  - name: search_memory\n    usage: |\n      Search past conversation history. Use this when the user\n      references something from a previous conversation or when\n      context from past discussions would be helpful.\n    exec: |\n      #!/bin/bash\n      sqlite3 \"${LECTIC_DATA}/memory.sqlite3\" &lt;&lt;SQL\n      SELECT printf('[%s] %s: %s', timestamp, role, content)\n      FROM messages\n      WHERE content LIKE '%${QUERY}%'\n      ORDER BY timestamp DESC\n      LIMIT 10;\n      SQL\n    schema:\n      QUERY: The search term to look for in past messages.\n\n\n\n\nLong-running assistants where most turns don’t need memory\nWhen you want the assistant to decide what’s relevant\nWhen cache efficiency matters (the prompt stays constant)\n\n\n\n\n\nIn this approach, relevant memories are automatically injected into every prompt. Nothing is recorded automatically — instead, the assistant has a tool to explicitly remember things.\n\n\ntools:\n  - name: remember\n    usage: |\n      Store important information for future reference. Use this when\n      the user shares preferences, makes decisions, or provides context\n      that should persist across conversations.\n    exec: |\n      #!/bin/bash\n      set -euo pipefail\n      \n      DB=\"${LECTIC_DATA}/memory.sqlite3\"\n      \n      sqlite3 \"$DB\" &lt;&lt;'SQL'\n      CREATE TABLE IF NOT EXISTS memories (\n        id INTEGER PRIMARY KEY,\n        timestamp TEXT NOT NULL,\n        content TEXT NOT NULL\n      );\n      SQL\n      \n      CONTENT_ESC=\"${CONTENT//\\'/\\'\\'}\"\n      \n      sqlite3 \"$DB\" &lt;&lt;SQL\n      INSERT INTO memories (timestamp, content)\n      VALUES (datetime('now'), '$CONTENT_ESC');\n      SQL\n      \n      echo \"Remembered.\"\n    schema:\n      CONTENT: The information to remember.\n\n\n\nUse an exec: prompt to inject stored memories:\ninterlocutor:\n  name: Assistant\n  prompt: |\n    exec:#!/bin/bash\n    cat &lt;&lt;'PROMPT'\n    You are a helpful assistant with access to stored memories.\n    \n    Things you've been asked to remember:\n    PROMPT\n    \n    DB=\"${LECTIC_DATA}/memory.sqlite3\"\n    if [[ -f \"$DB\" ]]; then\n      sqlite3 \"$DB\" &lt;&lt;'SQL'\n      SELECT printf('- %s', content)\n      FROM memories\n      ORDER BY timestamp DESC\n      LIMIT 20;\n      SQL\n    else\n      echo \"(No memories yet)\"\n    fi\n    \n    echo \"\"\n    echo \"Use the remember tool to store new information.\"\n\n\n\n\nWhen you want explicit control over what’s remembered\nFor preferences and decisions rather than conversation history\nWhen memories are small and frequently relevant\n\n\n\n\n\n\nDon’t mix these approaches carelessly. Recording everything and injecting it into the prompt will bloat your context and hurt cache performance.\nBe selective about what you store. Tool call results and verbose outputs can bloat the database quickly.\nConsider privacy. Memory persists across sessions. Don’t store sensitive information you wouldn’t want retrieved later.\nMonitor database size. Add a periodic cleanup job or a forget tool for manual pruning.\n\n\n\ntools:\n  - name: forget\n    usage: Remove a specific memory by its content.\n    exec: |\n      #!/bin/bash\n      sqlite3 \"${LECTIC_DATA}/memory.sqlite3\" &lt;&lt;SQL\n      DELETE FROM memories WHERE content LIKE '%${PATTERN}%';\n      SELECT 'Deleted ' || changes() || ' memories';\n      SQL\n    schema:\n      PATTERN: Pattern to match memories to delete.",
    "crumbs": [
      "Cookbook",
      "Conversation Memory"
    ]
  },
  {
    "objectID": "cookbook/04_memory.html#approach-1-memory-as-a-tool",
    "href": "cookbook/04_memory.html#approach-1-memory-as-a-tool",
    "title": "Recipe: Conversation Memory",
    "section": "",
    "text": "In this approach, everything is recorded automatically, but the assistant must explicitly search for relevant memories. This keeps the prompt lean and preserves cache efficiency.\n\n\nAdd this hook to save every message:\nhooks:\n  - on: [user_message, assistant_message]\n    do: |\n      #!/bin/bash\n      set -euo pipefail\n      \n      DB=\"${LECTIC_DATA}/memory.sqlite3\"\n      \n      # Initialize database if needed\n      sqlite3 \"$DB\" &lt;&lt;'SQL'\n      CREATE TABLE IF NOT EXISTS messages (\n        id INTEGER PRIMARY KEY,\n        timestamp TEXT NOT NULL,\n        role TEXT NOT NULL,\n        interlocutor TEXT,\n        file TEXT,\n        content TEXT NOT NULL\n      );\n      CREATE INDEX IF NOT EXISTS idx_timestamp ON messages(timestamp);\n      SQL\n      \n      # Determine role and content\n      if [[ -n \"${ASSISTANT_MESSAGE:-}\" ]]; then\n        ROLE=\"assistant\"\n        CONTENT=\"$ASSISTANT_MESSAGE\"\n        NAME=\"${LECTIC_INTERLOCUTOR:-}\"\n      else\n        ROLE=\"user\"\n        CONTENT=\"${USER_MESSAGE:-}\"\n        NAME=\"\"\n      fi\n      \n      # Escape single quotes for SQL\n      CONTENT_ESC=\"${CONTENT//\\'/\\'\\'}\"\n      NAME_ESC=\"${NAME//\\'/\\'\\'}\"\n      FILE_ESC=\"${LECTIC_FILE//\\'/\\'\\'}\"\n      \n      sqlite3 \"$DB\" &lt;&lt;SQL\n      INSERT INTO messages (timestamp, role, interlocutor, file, content)\n      VALUES (datetime('now'), '$ROLE', '$NAME_ESC', '$FILE_ESC', '$CONTENT_ESC');\n      SQL\n\n\n\nGive the assistant a tool to search memory:\ntools:\n  - name: search_memory\n    usage: |\n      Search past conversation history. Use this when the user\n      references something from a previous conversation or when\n      context from past discussions would be helpful.\n    exec: |\n      #!/bin/bash\n      sqlite3 \"${LECTIC_DATA}/memory.sqlite3\" &lt;&lt;SQL\n      SELECT printf('[%s] %s: %s', timestamp, role, content)\n      FROM messages\n      WHERE content LIKE '%${QUERY}%'\n      ORDER BY timestamp DESC\n      LIMIT 10;\n      SQL\n    schema:\n      QUERY: The search term to look for in past messages.\n\n\n\n\nLong-running assistants where most turns don’t need memory\nWhen you want the assistant to decide what’s relevant\nWhen cache efficiency matters (the prompt stays constant)",
    "crumbs": [
      "Cookbook",
      "Conversation Memory"
    ]
  },
  {
    "objectID": "cookbook/04_memory.html#approach-2-automatic-context-injection",
    "href": "cookbook/04_memory.html#approach-2-automatic-context-injection",
    "title": "Recipe: Conversation Memory",
    "section": "",
    "text": "In this approach, relevant memories are automatically injected into every prompt. Nothing is recorded automatically — instead, the assistant has a tool to explicitly remember things.\n\n\ntools:\n  - name: remember\n    usage: |\n      Store important information for future reference. Use this when\n      the user shares preferences, makes decisions, or provides context\n      that should persist across conversations.\n    exec: |\n      #!/bin/bash\n      set -euo pipefail\n      \n      DB=\"${LECTIC_DATA}/memory.sqlite3\"\n      \n      sqlite3 \"$DB\" &lt;&lt;'SQL'\n      CREATE TABLE IF NOT EXISTS memories (\n        id INTEGER PRIMARY KEY,\n        timestamp TEXT NOT NULL,\n        content TEXT NOT NULL\n      );\n      SQL\n      \n      CONTENT_ESC=\"${CONTENT//\\'/\\'\\'}\"\n      \n      sqlite3 \"$DB\" &lt;&lt;SQL\n      INSERT INTO memories (timestamp, content)\n      VALUES (datetime('now'), '$CONTENT_ESC');\n      SQL\n      \n      echo \"Remembered.\"\n    schema:\n      CONTENT: The information to remember.\n\n\n\nUse an exec: prompt to inject stored memories:\ninterlocutor:\n  name: Assistant\n  prompt: |\n    exec:#!/bin/bash\n    cat &lt;&lt;'PROMPT'\n    You are a helpful assistant with access to stored memories.\n    \n    Things you've been asked to remember:\n    PROMPT\n    \n    DB=\"${LECTIC_DATA}/memory.sqlite3\"\n    if [[ -f \"$DB\" ]]; then\n      sqlite3 \"$DB\" &lt;&lt;'SQL'\n      SELECT printf('- %s', content)\n      FROM memories\n      ORDER BY timestamp DESC\n      LIMIT 20;\n      SQL\n    else\n      echo \"(No memories yet)\"\n    fi\n    \n    echo \"\"\n    echo \"Use the remember tool to store new information.\"\n\n\n\n\nWhen you want explicit control over what’s remembered\nFor preferences and decisions rather than conversation history\nWhen memories are small and frequently relevant",
    "crumbs": [
      "Cookbook",
      "Conversation Memory"
    ]
  },
  {
    "objectID": "cookbook/04_memory.html#tips",
    "href": "cookbook/04_memory.html#tips",
    "title": "Recipe: Conversation Memory",
    "section": "",
    "text": "Don’t mix these approaches carelessly. Recording everything and injecting it into the prompt will bloat your context and hurt cache performance.\nBe selective about what you store. Tool call results and verbose outputs can bloat the database quickly.\nConsider privacy. Memory persists across sessions. Don’t store sensitive information you wouldn’t want retrieved later.\nMonitor database size. Add a periodic cleanup job or a forget tool for manual pruning.\n\n\n\ntools:\n  - name: forget\n    usage: Remove a specific memory by its content.\n    exec: |\n      #!/bin/bash\n      sqlite3 \"${LECTIC_DATA}/memory.sqlite3\" &lt;&lt;SQL\n      DELETE FROM memories WHERE content LIKE '%${PATTERN}%';\n      SELECT 'Deleted ' || changes() || ' memories';\n      SQL\n    schema:\n      PATTERN: Pattern to match memories to delete.",
    "crumbs": [
      "Cookbook",
      "Conversation Memory"
    ]
  },
  {
    "objectID": "cookbook/03_research_perspectives.html",
    "href": "cookbook/03_research_perspectives.html",
    "title": "Recipe: Research with Multiple Perspectives",
    "section": "",
    "text": "This recipe shows how to use multiple interlocutors to explore a topic from different angles. One interlocutor does research, another provides critique, and you can quickly get second opinions without derailing the main conversation.\n\n\n---\ninterlocutors:\n  - name: Researcher\n    prompt: |\n      You are a thorough researcher. When exploring a topic:\n      - Consider multiple sources and viewpoints\n      - Note uncertainties and limitations\n      - Suggest follow-up questions\n    provider: anthropic\n    model: claude-sonnet-4-20250514\n    tools:\n      - native: search\n      - think_about: &gt;\n          What are the key questions here? What might I be missing?\n          What assumptions am I making?\n\n  - name: Critic  \n    prompt: |\n      You are a skeptical critic. Your job is to:\n      - Challenge assumptions and weak arguments\n      - Point out missing evidence or alternative explanations\n      - Steelman opposing viewpoints\n      Be constructive but rigorous.\n    provider: anthropic\n    model: claude-sonnet-4-20250514\n\n  - name: Synthesizer\n    prompt: |\n      You synthesize discussions into clear summaries. Focus on:\n      - Key points of agreement and disagreement\n      - Open questions that remain\n      - Actionable conclusions\n    provider: anthropic\n    model: claude-3-haiku-20240307\n---\n\n\n\n\n\n:ask[Researcher] I'm trying to understand the tradeoffs between\nmicroservices and monolithic architectures for a team of 5 developers\nbuilding a B2B SaaS product.\nThe Researcher will explore the topic, potentially using web search and their thinking tool.\n\n\n\nUse :aside to get feedback without switching the main conversation:\n:aside[Critic] What's wrong with this analysis?\nThe Critic responds, then the next message goes back to the Researcher automatically.\n\n\n\n:ask[Critic] Let's dig into the claim about \"complexity tax.\" What's\nthe actual evidence here?\nNow you’re in a conversation with the Critic until you switch again.\n\n\n\n:ask[Synthesizer] Summarize this discussion. What did we learn? What\nshould we do next?\n\n\n\n\n\n\ninterlocutors:\n  - name: Legal\n    prompt: You are a legal expert. Focus on regulatory compliance...\n  - name: Technical\n    prompt: You are a senior engineer. Focus on implementation...\n  - name: Business\n    prompt: You are a business strategist. Focus on market fit...\n\n\n\nHave one interlocutor call another as a tool:\ninterlocutors:\n  - name: Lead\n    prompt: You coordinate research. Delegate to specialists.\n    tools:\n      - agent: Researcher\n        name: research\n        usage: Get detailed research on a specific topic.\n      - agent: Critic\n        name: critique\n        usage: Get critical analysis of a claim or argument.\n  \n  - name: Researcher\n    prompt: ...\n    tools:\n      - native: search\n  \n  - name: Critic\n    prompt: ...\nNow the Lead can autonomously decide when to delegate:\n:ask[Lead] Evaluate whether we should migrate from PostgreSQL to\nCockroachDB for our multi-region deployment.\n\n\n\nUse cheaper/faster models for quick checks:\ninterlocutors:\n  - name: Deep\n    model: claude-sonnet-4-20250514  # For complex analysis\n    \n  - name: Quick\n    model: claude-3-haiku-20240307  # For quick sanity checks\n\n\n\n\n\nUse :aside liberally. It’s cheap to get a second opinion without losing your place in the main conversation.\nGive each interlocutor a distinct voice. The prompts should produce noticeably different responses, otherwise there’s no point in having multiple speakers.\nUse :reset[] when switching topics. If you’re starting a new line of inquiry, reset context to avoid confusion from earlier discussion.",
    "crumbs": [
      "Cookbook",
      "Research Perspectives"
    ]
  },
  {
    "objectID": "cookbook/03_research_perspectives.html#the-setup",
    "href": "cookbook/03_research_perspectives.html#the-setup",
    "title": "Recipe: Research with Multiple Perspectives",
    "section": "",
    "text": "---\ninterlocutors:\n  - name: Researcher\n    prompt: |\n      You are a thorough researcher. When exploring a topic:\n      - Consider multiple sources and viewpoints\n      - Note uncertainties and limitations\n      - Suggest follow-up questions\n    provider: anthropic\n    model: claude-sonnet-4-20250514\n    tools:\n      - native: search\n      - think_about: &gt;\n          What are the key questions here? What might I be missing?\n          What assumptions am I making?\n\n  - name: Critic  \n    prompt: |\n      You are a skeptical critic. Your job is to:\n      - Challenge assumptions and weak arguments\n      - Point out missing evidence or alternative explanations\n      - Steelman opposing viewpoints\n      Be constructive but rigorous.\n    provider: anthropic\n    model: claude-sonnet-4-20250514\n\n  - name: Synthesizer\n    prompt: |\n      You synthesize discussions into clear summaries. Focus on:\n      - Key points of agreement and disagreement\n      - Open questions that remain\n      - Actionable conclusions\n    provider: anthropic\n    model: claude-3-haiku-20240307\n---",
    "crumbs": [
      "Cookbook",
      "Research Perspectives"
    ]
  },
  {
    "objectID": "cookbook/03_research_perspectives.html#usage",
    "href": "cookbook/03_research_perspectives.html#usage",
    "title": "Recipe: Research with Multiple Perspectives",
    "section": "",
    "text": ":ask[Researcher] I'm trying to understand the tradeoffs between\nmicroservices and monolithic architectures for a team of 5 developers\nbuilding a B2B SaaS product.\nThe Researcher will explore the topic, potentially using web search and their thinking tool.\n\n\n\nUse :aside to get feedback without switching the main conversation:\n:aside[Critic] What's wrong with this analysis?\nThe Critic responds, then the next message goes back to the Researcher automatically.\n\n\n\n:ask[Critic] Let's dig into the claim about \"complexity tax.\" What's\nthe actual evidence here?\nNow you’re in a conversation with the Critic until you switch again.\n\n\n\n:ask[Synthesizer] Summarize this discussion. What did we learn? What\nshould we do next?",
    "crumbs": [
      "Cookbook",
      "Research Perspectives"
    ]
  },
  {
    "objectID": "cookbook/03_research_perspectives.html#variations",
    "href": "cookbook/03_research_perspectives.html#variations",
    "title": "Recipe: Research with Multiple Perspectives",
    "section": "",
    "text": "interlocutors:\n  - name: Legal\n    prompt: You are a legal expert. Focus on regulatory compliance...\n  - name: Technical\n    prompt: You are a senior engineer. Focus on implementation...\n  - name: Business\n    prompt: You are a business strategist. Focus on market fit...\n\n\n\nHave one interlocutor call another as a tool:\ninterlocutors:\n  - name: Lead\n    prompt: You coordinate research. Delegate to specialists.\n    tools:\n      - agent: Researcher\n        name: research\n        usage: Get detailed research on a specific topic.\n      - agent: Critic\n        name: critique\n        usage: Get critical analysis of a claim or argument.\n  \n  - name: Researcher\n    prompt: ...\n    tools:\n      - native: search\n  \n  - name: Critic\n    prompt: ...\nNow the Lead can autonomously decide when to delegate:\n:ask[Lead] Evaluate whether we should migrate from PostgreSQL to\nCockroachDB for our multi-region deployment.\n\n\n\nUse cheaper/faster models for quick checks:\ninterlocutors:\n  - name: Deep\n    model: claude-sonnet-4-20250514  # For complex analysis\n    \n  - name: Quick\n    model: claude-3-haiku-20240307  # For quick sanity checks",
    "crumbs": [
      "Cookbook",
      "Research Perspectives"
    ]
  },
  {
    "objectID": "cookbook/03_research_perspectives.html#tips",
    "href": "cookbook/03_research_perspectives.html#tips",
    "title": "Recipe: Research with Multiple Perspectives",
    "section": "",
    "text": "Use :aside liberally. It’s cheap to get a second opinion without losing your place in the main conversation.\nGive each interlocutor a distinct voice. The prompts should produce noticeably different responses, otherwise there’s no point in having multiple speakers.\nUse :reset[] when switching topics. If you’re starting a new line of inquiry, reset context to avoid confusion from earlier discussion.",
    "crumbs": [
      "Cookbook",
      "Research Perspectives"
    ]
  },
  {
    "objectID": "cookbook/05_context_compaction.html",
    "href": "cookbook/05_context_compaction.html",
    "title": "Recipe: Context Compaction",
    "section": "",
    "text": "This recipe shows how to automatically handle long conversations by summarizing and resetting context when token usage gets high.\n\n\nLong conversations hit context limits. Before that happens, you want to:\n\nSummarize what’s been discussed\nReset the context window\nContinue with the summary as the new starting point\n\n\n\n\nThis hook monitors token usage and triggers compaction when a threshold is reached:\nhooks:\n  - on: assistant_message\n    inline: true\n    do: |\n      #!/bin/bash\n      \n      # Configuration\n      LIMIT=80000        # Trigger compaction at this token count\n      \n      TOTAL=\"${TOKEN_USAGE_TOTAL:-0}\"\n      \n      if [[ $TOTAL -lt $LIMIT ]]; then\n        exit 0  # No output, no action\n      fi\n      \n      # The conversation body comes in on stdin.\n      # Indent it as a code block for the summarizer.\n      CONVERSATION=$(sed 's/^/    /')\n      \n      # Generate summary using a separate lectic call\n      SUMMARY=$(echo \"$CONVERSATION\" | lectic -f ~/.config/lectic/summarize.lec -S)\n      \n      # Output with reset header\n      echo \"LECTIC:reset\"\n      echo \"LECTIC:final\"\n      echo \"\"\n      echo \"**Context compacted at $TOTAL tokens.**\"\n      echo \"\"\n      echo \"Summary of previous discussion:\"\n      echo \"$SUMMARY\"\nThe LECTIC:reset header clears prior context. The LECTIC:final header prevents the assistant from responding to the compaction notice itself.\n\n\n\nCreate ~/.config/lectic/summarize.lec:\n---\ninterlocutor:\n  name: Summarizer\n  prompt: |\n    Summarize the following conversation concisely. Include:\n    - Key decisions made\n    - Important information established\n    - Current task or question being addressed\n    - Any pending items or open threads\n    \n    Be thorough but concise. This summary will be the only context\n    available for continuing the conversation.\n  model: claude-3-haiku-20240307\n  max_tokens: 1000\n---\n\nSummarize this conversation:\nThe conversation text is piped to stdin and appended to the prompt file’s content.\n\n\n\n\nAfter each assistant message, the hook checks TOKEN_USAGE_TOTAL\nThe conversation body so far is available on stdin\nIf over the limit, the hook pipes the conversation to a separate lectic instance for summarization\nThe summary is output with the reset header, clearing old context\nThe next turn starts fresh with only the summary as context\n\n\n\n\n\n\nInstead of automatic triggering, add a macro:\nmacros:\n  - name: compact\n    expansion: |\n      exec:#!/bin/bash\n      echo \":reset[]\"\n      echo \"\"\n      echo \"Please summarize our conversation so far, focusing on key\"\n      echo \"decisions, important context, and any open questions.\"\nUsage: :compact[]\nThis asks the current assistant to summarize before resetting, rather than using a separate summarization call.\n\n\n\nSave the full conversation before resetting:\nhooks:\n  - on: assistant_message\n    inline: true\n    do: |\n      #!/bin/bash\n      TOTAL=\"${TOKEN_USAGE_TOTAL:-0}\"\n      LIMIT=80000\n      \n      if [[ $TOTAL -lt $LIMIT ]]; then\n        exit 0\n      fi\n      \n      # Archive the current conversation\n      ARCHIVE_DIR=\"${LECTIC_DATA}/archives\"\n      mkdir -p \"$ARCHIVE_DIR\"\n      \n      TIMESTAMP=$(date +%Y%m%d-%H%M%S)\n      BASE=$(basename \"${LECTIC_FILE:-.lec}\" .lec)\n      ARCHIVE_FILE=\"$ARCHIVE_DIR/$BASE-$TIMESTAMP.lec\"\n      \n      # Save stdin (the conversation) to the archive\n      cat &gt; \"$ARCHIVE_FILE\"\n      \n      # Now generate summary (re-read from archive since stdin is consumed)\n      SUMMARY=$(sed 's/^/    /' \"$ARCHIVE_FILE\" | \\\n                lectic -f ~/.config/lectic/summarize.lec -S)\n      \n      echo \"LECTIC:reset\"\n      echo \"LECTIC:final\"\n      echo \"\"\n      echo \"**Context compacted. Archived to: $ARCHIVE_FILE**\"\n      echo \"\"\n      echo \"Summary:\"\n      echo \"$SUMMARY\"\n\n\n\nGive a warning at 70% capacity, compact at 90%:\nhooks:\n  - on: assistant_message\n    inline: true\n    do: |\n      #!/bin/bash\n      TOTAL=\"${TOKEN_USAGE_TOTAL:-0}\"\n      WARN_LIMIT=70000\n      HARD_LIMIT=90000\n      \n      if [[ $TOTAL -gt $HARD_LIMIT ]]; then\n        # Full compaction\n        CONVERSATION=$(sed 's/^/    /')\n        SUMMARY=$(echo \"$CONVERSATION\" | \\\n                  lectic -f ~/.config/lectic/summarize.lec -S)\n        echo \"LECTIC:reset\"\n        echo \"LECTIC:final\"\n        echo \"\"\n        echo \"**Context limit reached. Compacted.**\"\n        echo \"\"\n        echo \"$SUMMARY\"\n      elif [[ $TOTAL -gt $WARN_LIMIT ]]; then\n        # Just warn\n        cat &gt; /dev/null  # Consume stdin\n        echo \"LECTIC:final\"\n        echo \"\"\n        echo \"*Note: Context at ${TOTAL} tokens. Consider* :reset[] *soon.*\"\n      fi\n\n\n\n\n\nTest your summarizer. A bad summary loses important context. Try it manually on a few conversations first.\nUse a fast model for summarization. Haiku or similar is fine for summaries and keeps compaction quick.\nConsider what to preserve. Code snippets, file paths, and specific decisions are often more important than general discussion.\nMonitor compaction frequency. If you’re compacting every few messages, your conversations might be too verbose or you might need a larger context model.",
    "crumbs": [
      "Cookbook",
      "Context Compaction"
    ]
  },
  {
    "objectID": "cookbook/05_context_compaction.html#the-problem",
    "href": "cookbook/05_context_compaction.html#the-problem",
    "title": "Recipe: Context Compaction",
    "section": "",
    "text": "Long conversations hit context limits. Before that happens, you want to:\n\nSummarize what’s been discussed\nReset the context window\nContinue with the summary as the new starting point",
    "crumbs": [
      "Cookbook",
      "Context Compaction"
    ]
  },
  {
    "objectID": "cookbook/05_context_compaction.html#automatic-compaction-hook",
    "href": "cookbook/05_context_compaction.html#automatic-compaction-hook",
    "title": "Recipe: Context Compaction",
    "section": "",
    "text": "This hook monitors token usage and triggers compaction when a threshold is reached:\nhooks:\n  - on: assistant_message\n    inline: true\n    do: |\n      #!/bin/bash\n      \n      # Configuration\n      LIMIT=80000        # Trigger compaction at this token count\n      \n      TOTAL=\"${TOKEN_USAGE_TOTAL:-0}\"\n      \n      if [[ $TOTAL -lt $LIMIT ]]; then\n        exit 0  # No output, no action\n      fi\n      \n      # The conversation body comes in on stdin.\n      # Indent it as a code block for the summarizer.\n      CONVERSATION=$(sed 's/^/    /')\n      \n      # Generate summary using a separate lectic call\n      SUMMARY=$(echo \"$CONVERSATION\" | lectic -f ~/.config/lectic/summarize.lec -S)\n      \n      # Output with reset header\n      echo \"LECTIC:reset\"\n      echo \"LECTIC:final\"\n      echo \"\"\n      echo \"**Context compacted at $TOTAL tokens.**\"\n      echo \"\"\n      echo \"Summary of previous discussion:\"\n      echo \"$SUMMARY\"\nThe LECTIC:reset header clears prior context. The LECTIC:final header prevents the assistant from responding to the compaction notice itself.",
    "crumbs": [
      "Cookbook",
      "Context Compaction"
    ]
  },
  {
    "objectID": "cookbook/05_context_compaction.html#the-summarization-prompt",
    "href": "cookbook/05_context_compaction.html#the-summarization-prompt",
    "title": "Recipe: Context Compaction",
    "section": "",
    "text": "Create ~/.config/lectic/summarize.lec:\n---\ninterlocutor:\n  name: Summarizer\n  prompt: |\n    Summarize the following conversation concisely. Include:\n    - Key decisions made\n    - Important information established\n    - Current task or question being addressed\n    - Any pending items or open threads\n    \n    Be thorough but concise. This summary will be the only context\n    available for continuing the conversation.\n  model: claude-3-haiku-20240307\n  max_tokens: 1000\n---\n\nSummarize this conversation:\nThe conversation text is piped to stdin and appended to the prompt file’s content.",
    "crumbs": [
      "Cookbook",
      "Context Compaction"
    ]
  },
  {
    "objectID": "cookbook/05_context_compaction.html#how-it-works",
    "href": "cookbook/05_context_compaction.html#how-it-works",
    "title": "Recipe: Context Compaction",
    "section": "",
    "text": "After each assistant message, the hook checks TOKEN_USAGE_TOTAL\nThe conversation body so far is available on stdin\nIf over the limit, the hook pipes the conversation to a separate lectic instance for summarization\nThe summary is output with the reset header, clearing old context\nThe next turn starts fresh with only the summary as context",
    "crumbs": [
      "Cookbook",
      "Context Compaction"
    ]
  },
  {
    "objectID": "cookbook/05_context_compaction.html#variations",
    "href": "cookbook/05_context_compaction.html#variations",
    "title": "Recipe: Context Compaction",
    "section": "",
    "text": "Instead of automatic triggering, add a macro:\nmacros:\n  - name: compact\n    expansion: |\n      exec:#!/bin/bash\n      echo \":reset[]\"\n      echo \"\"\n      echo \"Please summarize our conversation so far, focusing on key\"\n      echo \"decisions, important context, and any open questions.\"\nUsage: :compact[]\nThis asks the current assistant to summarize before resetting, rather than using a separate summarization call.\n\n\n\nSave the full conversation before resetting:\nhooks:\n  - on: assistant_message\n    inline: true\n    do: |\n      #!/bin/bash\n      TOTAL=\"${TOKEN_USAGE_TOTAL:-0}\"\n      LIMIT=80000\n      \n      if [[ $TOTAL -lt $LIMIT ]]; then\n        exit 0\n      fi\n      \n      # Archive the current conversation\n      ARCHIVE_DIR=\"${LECTIC_DATA}/archives\"\n      mkdir -p \"$ARCHIVE_DIR\"\n      \n      TIMESTAMP=$(date +%Y%m%d-%H%M%S)\n      BASE=$(basename \"${LECTIC_FILE:-.lec}\" .lec)\n      ARCHIVE_FILE=\"$ARCHIVE_DIR/$BASE-$TIMESTAMP.lec\"\n      \n      # Save stdin (the conversation) to the archive\n      cat &gt; \"$ARCHIVE_FILE\"\n      \n      # Now generate summary (re-read from archive since stdin is consumed)\n      SUMMARY=$(sed 's/^/    /' \"$ARCHIVE_FILE\" | \\\n                lectic -f ~/.config/lectic/summarize.lec -S)\n      \n      echo \"LECTIC:reset\"\n      echo \"LECTIC:final\"\n      echo \"\"\n      echo \"**Context compacted. Archived to: $ARCHIVE_FILE**\"\n      echo \"\"\n      echo \"Summary:\"\n      echo \"$SUMMARY\"\n\n\n\nGive a warning at 70% capacity, compact at 90%:\nhooks:\n  - on: assistant_message\n    inline: true\n    do: |\n      #!/bin/bash\n      TOTAL=\"${TOKEN_USAGE_TOTAL:-0}\"\n      WARN_LIMIT=70000\n      HARD_LIMIT=90000\n      \n      if [[ $TOTAL -gt $HARD_LIMIT ]]; then\n        # Full compaction\n        CONVERSATION=$(sed 's/^/    /')\n        SUMMARY=$(echo \"$CONVERSATION\" | \\\n                  lectic -f ~/.config/lectic/summarize.lec -S)\n        echo \"LECTIC:reset\"\n        echo \"LECTIC:final\"\n        echo \"\"\n        echo \"**Context limit reached. Compacted.**\"\n        echo \"\"\n        echo \"$SUMMARY\"\n      elif [[ $TOTAL -gt $WARN_LIMIT ]]; then\n        # Just warn\n        cat &gt; /dev/null  # Consume stdin\n        echo \"LECTIC:final\"\n        echo \"\"\n        echo \"*Note: Context at ${TOTAL} tokens. Consider* :reset[] *soon.*\"\n      fi",
    "crumbs": [
      "Cookbook",
      "Context Compaction"
    ]
  },
  {
    "objectID": "cookbook/05_context_compaction.html#tips",
    "href": "cookbook/05_context_compaction.html#tips",
    "title": "Recipe: Context Compaction",
    "section": "",
    "text": "Test your summarizer. A bad summary loses important context. Try it manually on a few conversations first.\nUse a fast model for summarization. Haiku or similar is fine for summaries and keeps compaction quick.\nConsider what to preserve. Code snippets, file paths, and specific decisions are often more important than general discussion.\nMonitor compaction frequency. If you’re compacting every few messages, your conversations might be too verbose or you might need a larger context model.",
    "crumbs": [
      "Cookbook",
      "Context Compaction"
    ]
  },
  {
    "objectID": "automation/03_custom_subcommands.html",
    "href": "automation/03_custom_subcommands.html",
    "title": "Automation: Custom Subcommands",
    "section": "",
    "text": "Lectic’s CLI is extensible through “git-style” custom subcommands. If you create an executable named lectic-&lt;command&gt;, or lectic-&lt;command&gt;.&lt;file-extension&gt; and place it in your configuration directory, data directory, or PATH, you can invoke it as lectic &lt;command&gt;.\nThis allows you to wrap common workflows, build project-specific tools, and create shortcuts for complex Lectic invocations.\n\n\nWhen you run lectic foo args..., Lectic searches for an executable named lectic-foo or lectic-foo.* in the following locations, in order:\n\nConfiguration Directory: $LECTIC_CONFIG (defaults to ~/.config/lectic on Linux)\nData Directory: $LECTIC_DATA (defaults to ~/.local/share/lectic on Linux)\nSystem PATH: Any directory in your $PATH.\n\nThe first match found is executed. The subprocess receives the remaining arguments, inherits the standard input, output, and error streams, and has access to Lectic’s environment variables.\n\n\n\n\n\nCreate a file named lectic-hello in ~/.config/lectic/:\n#!/bin/bash\necho \"Hello from a custom subcommand!\"\necho \"My config dir is: $LECTIC_CONFIG\"\nMake it executable: chmod +x ~/.config/lectic/lectic-hello\nRun it:\nlectic hello\n\n\n\nLectic includes a built-in bun runtime for running JavaScript and TypeScript files via lectic script. You can use this to write subcommands in JS or TS even if you don’t have bun installed globally.\nCreate ~/.config/lectic/lectic-calc:\n#!/usr/bin/env -S lectic script\n\nconst args = process.argv.slice(2);\nif (args.length === 0) {\n  console.error(\"Usage: lectic calc &lt;expression&gt;\");\n  process.exit(1);\n}\n\n// Access standard Lectic environment variables\nconst configDir = process.env.LECTIC_CONFIG;\n\ntry {\n  console.log(eval(args.join(\" \")));\n} catch (e) {\n  console.error(\"Error:\", e.message);\n}\nMake it executable and run:\nlectic calc 1 + 2\nBun has built in support for all sorts of things, including YAML parsing and serialization, running simple webservers, interfacing with databases, rich networking primitives, and lots more. All these are accessible via lectic script.\n\n\n\n\nSubcommands receive the standard set of Lectic environment variables:\n\nLECTIC_CONFIG: Path to the configuration directory.\nLECTIC_DATA: Path to the data directory.\nLECTIC_CACHE: Path to the cache directory.\nLECTIC_STATE: Path to the state directory.\nLECTIC_TEMP: Path to the temporary directory.\n\nThese ensure your subcommands respect the user’s directory configuration.\n\n\n\nYou can add tab completion for your custom subcommands. The completion system supports plugging in custom completion functions.\n\n\nFirst, ensure you have enabled tab completion by sourcing the completion script in your shell configuration (e.g., ~/.bashrc):\nsource /path/to/lectic/extra/tab_complete/lectic_completion.bash\n(The path depends on how you installed Lectic. If you installed via Nix or an AppImage, you may need to locate this file in the repository or extract it.)\n\n\n\nTo provide completions for a subcommand lectic-foo, create a bash script that defines a completion function and registers it.\nThe script can be placed in: 1. ~/.config/lectic/completions/ 2. ~/.local/share/lectic/completions/ 3. Or alongside the executable itself, named lectic-foo.completion.bash.\nExample:\nCreate ~/.config/lectic/completions/foo.bash:\n_lectic_complete_foo() {\n  local cur\n  cur=\"${COMP_WORDS[COMP_CWORD]}\"\n  # Suggest 'bar' and 'baz'\n  COMPREPLY=( $(compgen -W \"bar baz\" -- \"${cur}\") )\n}\n\n# Register the function for the 'foo' subcommand\nlectic_register_completion foo _lectic_complete_foo\nNow, typing lectic foo &lt;TAB&gt; will suggest bar and baz.\n\n\n\n\n\n\nTip\n\n\n\nFor performance, define completions in a separate .completion.bash file rather than inside the subcommand script itself. This allows the shell to load completions without executing the subcommand.",
    "crumbs": [
      "Automation",
      "Subcommands"
    ]
  },
  {
    "objectID": "automation/03_custom_subcommands.html#how-it-works",
    "href": "automation/03_custom_subcommands.html#how-it-works",
    "title": "Automation: Custom Subcommands",
    "section": "",
    "text": "When you run lectic foo args..., Lectic searches for an executable named lectic-foo or lectic-foo.* in the following locations, in order:\n\nConfiguration Directory: $LECTIC_CONFIG (defaults to ~/.config/lectic on Linux)\nData Directory: $LECTIC_DATA (defaults to ~/.local/share/lectic on Linux)\nSystem PATH: Any directory in your $PATH.\n\nThe first match found is executed. The subprocess receives the remaining arguments, inherits the standard input, output, and error streams, and has access to Lectic’s environment variables.",
    "crumbs": [
      "Automation",
      "Subcommands"
    ]
  },
  {
    "objectID": "automation/03_custom_subcommands.html#examples",
    "href": "automation/03_custom_subcommands.html#examples",
    "title": "Automation: Custom Subcommands",
    "section": "",
    "text": "Create a file named lectic-hello in ~/.config/lectic/:\n#!/bin/bash\necho \"Hello from a custom subcommand!\"\necho \"My config dir is: $LECTIC_CONFIG\"\nMake it executable: chmod +x ~/.config/lectic/lectic-hello\nRun it:\nlectic hello\n\n\n\nLectic includes a built-in bun runtime for running JavaScript and TypeScript files via lectic script. You can use this to write subcommands in JS or TS even if you don’t have bun installed globally.\nCreate ~/.config/lectic/lectic-calc:\n#!/usr/bin/env -S lectic script\n\nconst args = process.argv.slice(2);\nif (args.length === 0) {\n  console.error(\"Usage: lectic calc &lt;expression&gt;\");\n  process.exit(1);\n}\n\n// Access standard Lectic environment variables\nconst configDir = process.env.LECTIC_CONFIG;\n\ntry {\n  console.log(eval(args.join(\" \")));\n} catch (e) {\n  console.error(\"Error:\", e.message);\n}\nMake it executable and run:\nlectic calc 1 + 2\nBun has built in support for all sorts of things, including YAML parsing and serialization, running simple webservers, interfacing with databases, rich networking primitives, and lots more. All these are accessible via lectic script.",
    "crumbs": [
      "Automation",
      "Subcommands"
    ]
  },
  {
    "objectID": "automation/03_custom_subcommands.html#environment-variables",
    "href": "automation/03_custom_subcommands.html#environment-variables",
    "title": "Automation: Custom Subcommands",
    "section": "",
    "text": "Subcommands receive the standard set of Lectic environment variables:\n\nLECTIC_CONFIG: Path to the configuration directory.\nLECTIC_DATA: Path to the data directory.\nLECTIC_CACHE: Path to the cache directory.\nLECTIC_STATE: Path to the state directory.\nLECTIC_TEMP: Path to the temporary directory.\n\nThese ensure your subcommands respect the user’s directory configuration.",
    "crumbs": [
      "Automation",
      "Subcommands"
    ]
  },
  {
    "objectID": "automation/03_custom_subcommands.html#tab-completion",
    "href": "automation/03_custom_subcommands.html#tab-completion",
    "title": "Automation: Custom Subcommands",
    "section": "",
    "text": "You can add tab completion for your custom subcommands. The completion system supports plugging in custom completion functions.\n\n\nFirst, ensure you have enabled tab completion by sourcing the completion script in your shell configuration (e.g., ~/.bashrc):\nsource /path/to/lectic/extra/tab_complete/lectic_completion.bash\n(The path depends on how you installed Lectic. If you installed via Nix or an AppImage, you may need to locate this file in the repository or extract it.)\n\n\n\nTo provide completions for a subcommand lectic-foo, create a bash script that defines a completion function and registers it.\nThe script can be placed in: 1. ~/.config/lectic/completions/ 2. ~/.local/share/lectic/completions/ 3. Or alongside the executable itself, named lectic-foo.completion.bash.\nExample:\nCreate ~/.config/lectic/completions/foo.bash:\n_lectic_complete_foo() {\n  local cur\n  cur=\"${COMP_WORDS[COMP_CWORD]}\"\n  # Suggest 'bar' and 'baz'\n  COMPREPLY=( $(compgen -W \"bar baz\" -- \"${cur}\") )\n}\n\n# Register the function for the 'foo' subcommand\nlectic_register_completion foo _lectic_complete_foo\nNow, typing lectic foo &lt;TAB&gt; will suggest bar and baz.\n\n\n\n\n\n\nTip\n\n\n\nFor performance, define completions in a separate .completion.bash file rather than inside the subcommand script itself. This allows the shell to load completions without executing the subcommand.",
    "crumbs": [
      "Automation",
      "Subcommands"
    ]
  },
  {
    "objectID": "automation/01_macros.html",
    "href": "automation/01_macros.html",
    "title": "Automation: Macros",
    "section": "",
    "text": "Lectic supports a simple but powerful macro system that allows you to define and reuse snippets of text. This is useful for saving frequently used prompts, automating repetitive workflows, and composing complex, multi-step commands.\nMacros are defined in your YAML configuration (either in a .lec file’s header or in an included configuration file).\n\n\nMacros are defined under the macros key. Each macro must have a name and an expansion. You can optionally provide an env map to set default environment variables for the expansion.\nmacros:\n  - name: summarize\n    expansion: &gt;\n      Please provide a concise, single-paragraph summary of our\n      conversation so far, focusing on the key decisions made and\n      conclusions reached.\n\n  - name: build\n    env:\n      BUILD_DIR: ./dist\n    expansion: exec:echo \"Building in $BUILD_DIR\"\n\n\nThe expansion field can be a simple string, or it can load its content from a file or from the output of a command, just like the prompt field. For full semantics of file: and exec:, see External Prompts.\n\nFile Source: expansion: file:./prompts/summarize.txt\nCommand/Script Source:\n\nSingle line: expansion: exec:get-prompt-from-db --name summarize (executed directly, not via a shell)\nMulti‑line script: start with a shebang, e.g.\nexpansion: |\n  exec:#!/usr/bin/env bash\n  echo \"Hello, ${TARGET}!\"\nMulti‑line scripts are written to a temp file and executed with the interpreter given by the shebang.\n\n\n\n\n\n\nTo use a macro, you invoke it by writing the macro name as the directive name:\n\n:name[] expands the macro.\n:name[args] expands the macro and also passes args to the expansion as the ARG environment variable.\n\nWhen Lectic processes the file, it replaces the macro directive with the full text from its expansion field before processing any other directives (like :cmd).\nThis was a long and productive discussion. Could you wrap it up?\n\n:summarize[]\n\n\n\nWhen a macro expands via exec, the script being executed can be pased information via environment variables.\n\n\nThe text inside the directive brackets is passed to the macro expansion as the ARG environment variable.\nThis works for both single-line exec: commands and multi-line exec: scripts.\n\n:name[hello] sets ARG=hello.\nIf you explicitly set an ARG attribute, it overrides the bracket content: :name[hello]{ARG=\"override\"}.\n\n\n\n\nYou can pass environment variables to a macro’s expansion by adding attributes to the macro directive. These attributes are injected into the environment of exec: expansions when they run.\n\n:name[]{FOO=\"bar\"} sets the variable FOO to bar.\n:name[]{EMPTY} sets the variable EMPTY to be undefined. If you need an empty string value, write :name[]{EMPTY=\"\"}.\n\nNotes: - Single‑line exec: commands are not run through a shell. If you need shell features, invoke a shell explicitly, e.g., exec: bash -c 'echo \"Hello, $TARGET\"'. - In single‑line commands, variables in the command string are expanded before execution. For multi‑line scripts, variables are available to the script via the environment.\n\n\nConfiguration:\nmacros:\n  - name: greet\n    expansion: exec: bash -c 'echo \"Hello, $TARGET!\"'\nConversation:\n:greet[]{TARGET=\"World\"}\nWhen Lectic processes this, the directive will be replaced by the output of the exec command, which is “Hello, World!”.\n\n\n\n\nA few other environment variables are available by default.\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nMESSAGE_INDEX\nIndex (starting from one) of the message containing the macro\n\n\nMESSAGES_LENGTH\nTotal number of messages in the conversation\n\n\n\nThese might be useful for conditionally running only if the macro is, e.g. part of the most recent user message.\n\n\n\n\nMacros can interact with each other recursively. To support complex workflows, macros can define two separate expansion phases: pre and post.\n\npre: Expanded when the macro is first encountered (pre-order traversal). If pre returns content, the expansion stops there: the macro is replaced by the result of pre and that result is then recursively expanded. The original children of the macro are discarded.\npost: Expanded after the macro’s children have been processed (post-order traversal). The processed output of the children is passed to post as the ARG variable.\n\nIf you define a macro with just expansion, it is treated as a post phase macro.\n\n\nIf the pre script runs but produces no output (an empty string), Lectic treats this as a “pass-through”. The macro is NOT replaced; instead, Lectic proceeds to process the macro’s children and then runs the post phase.\nThis makes it easy to implement cache checks or conditional logic.\n\n\n\n\n\n\nTip\n\n\n\nIf you explicitly want to delete a node during the pre phase (stopping recursion and producing no output), you cannot return an empty string. Instead, return an empty HTML comment: &lt;!-- --&gt;. This stops recursion and renders as nothing.\n\n\n\n\n\nThis design allows for powerful compositions, such as a caching macro that wraps expensive operations.\nmacros:\n  - name: cache\n    # Check for cache hit. If found, cat the file.\n    # If not found, the script produces no output (empty string),\n    # so Lectic proceeds to expand the children.\n    pre: |\n      exec:#!/bin/bash\n      HASH=$(echo \"$ARG\" | md5sum | cut -d' ' -f1)\n      if [ -f \"/tmp/cache/$HASH\" ]; then\n        cat \"/tmp/cache/$HASH\"\n      fi\n    # If we reached post, it means pre didn't return anything (cache miss).\n    # We now have the result of the children in ARG. Save it and output it.\n    post: |\n      exec:#!/bin/bash\n      HASH=$(echo \"$ARG\" | md5sum | cut -d' ' -f1)\n      mkdir -p /tmp/cache\n      echo \"$ARG\" &gt; \"/tmp/cache/$HASH\"\n      echo \"$ARG\"\nUsage:\n:cache[:summarize[:cat[file.txt]]]\n\n:cache’s pre runs. If the cache exists for the raw text of the children, it returns the cached summary. Lectic replaces the :cache block with this text and is done.\nIf pre returns nothing (cache miss), Lectic enters the children.\n:cat expands to the file content.\n:summarize processes that content.\nFinally, :cache’s post runs. ARG contains the summary. It writes ARG to the cache and outputs it.",
    "crumbs": [
      "Automation",
      "Macros"
    ]
  },
  {
    "objectID": "automation/01_macros.html#defining-macros",
    "href": "automation/01_macros.html#defining-macros",
    "title": "Automation: Macros",
    "section": "",
    "text": "Macros are defined under the macros key. Each macro must have a name and an expansion. You can optionally provide an env map to set default environment variables for the expansion.\nmacros:\n  - name: summarize\n    expansion: &gt;\n      Please provide a concise, single-paragraph summary of our\n      conversation so far, focusing on the key decisions made and\n      conclusions reached.\n\n  - name: build\n    env:\n      BUILD_DIR: ./dist\n    expansion: exec:echo \"Building in $BUILD_DIR\"\n\n\nThe expansion field can be a simple string, or it can load its content from a file or from the output of a command, just like the prompt field. For full semantics of file: and exec:, see External Prompts.\n\nFile Source: expansion: file:./prompts/summarize.txt\nCommand/Script Source:\n\nSingle line: expansion: exec:get-prompt-from-db --name summarize (executed directly, not via a shell)\nMulti‑line script: start with a shebang, e.g.\nexpansion: |\n  exec:#!/usr/bin/env bash\n  echo \"Hello, ${TARGET}!\"\nMulti‑line scripts are written to a temp file and executed with the interpreter given by the shebang.",
    "crumbs": [
      "Automation",
      "Macros"
    ]
  },
  {
    "objectID": "automation/01_macros.html#using-macros",
    "href": "automation/01_macros.html#using-macros",
    "title": "Automation: Macros",
    "section": "",
    "text": "To use a macro, you invoke it by writing the macro name as the directive name:\n\n:name[] expands the macro.\n:name[args] expands the macro and also passes args to the expansion as the ARG environment variable.\n\nWhen Lectic processes the file, it replaces the macro directive with the full text from its expansion field before processing any other directives (like :cmd).\nThis was a long and productive discussion. Could you wrap it up?\n\n:summarize[]",
    "crumbs": [
      "Automation",
      "Macros"
    ]
  },
  {
    "objectID": "automation/01_macros.html#the-macro-expansion-environment",
    "href": "automation/01_macros.html#the-macro-expansion-environment",
    "title": "Automation: Macros",
    "section": "",
    "text": "When a macro expands via exec, the script being executed can be pased information via environment variables.\n\n\nThe text inside the directive brackets is passed to the macro expansion as the ARG environment variable.\nThis works for both single-line exec: commands and multi-line exec: scripts.\n\n:name[hello] sets ARG=hello.\nIf you explicitly set an ARG attribute, it overrides the bracket content: :name[hello]{ARG=\"override\"}.\n\n\n\n\nYou can pass environment variables to a macro’s expansion by adding attributes to the macro directive. These attributes are injected into the environment of exec: expansions when they run.\n\n:name[]{FOO=\"bar\"} sets the variable FOO to bar.\n:name[]{EMPTY} sets the variable EMPTY to be undefined. If you need an empty string value, write :name[]{EMPTY=\"\"}.\n\nNotes: - Single‑line exec: commands are not run through a shell. If you need shell features, invoke a shell explicitly, e.g., exec: bash -c 'echo \"Hello, $TARGET\"'. - In single‑line commands, variables in the command string are expanded before execution. For multi‑line scripts, variables are available to the script via the environment.\n\n\nConfiguration:\nmacros:\n  - name: greet\n    expansion: exec: bash -c 'echo \"Hello, $TARGET!\"'\nConversation:\n:greet[]{TARGET=\"World\"}\nWhen Lectic processes this, the directive will be replaced by the output of the exec command, which is “Hello, World!”.\n\n\n\n\nA few other environment variables are available by default.\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nMESSAGE_INDEX\nIndex (starting from one) of the message containing the macro\n\n\nMESSAGES_LENGTH\nTotal number of messages in the conversation\n\n\n\nThese might be useful for conditionally running only if the macro is, e.g. part of the most recent user message.",
    "crumbs": [
      "Automation",
      "Macros"
    ]
  },
  {
    "objectID": "automation/01_macros.html#advanced-macros-phases-and-recursion",
    "href": "automation/01_macros.html#advanced-macros-phases-and-recursion",
    "title": "Automation: Macros",
    "section": "",
    "text": "Macros can interact with each other recursively. To support complex workflows, macros can define two separate expansion phases: pre and post.\n\npre: Expanded when the macro is first encountered (pre-order traversal). If pre returns content, the expansion stops there: the macro is replaced by the result of pre and that result is then recursively expanded. The original children of the macro are discarded.\npost: Expanded after the macro’s children have been processed (post-order traversal). The processed output of the children is passed to post as the ARG variable.\n\nIf you define a macro with just expansion, it is treated as a post phase macro.\n\n\nIf the pre script runs but produces no output (an empty string), Lectic treats this as a “pass-through”. The macro is NOT replaced; instead, Lectic proceeds to process the macro’s children and then runs the post phase.\nThis makes it easy to implement cache checks or conditional logic.\n\n\n\n\n\n\nTip\n\n\n\nIf you explicitly want to delete a node during the pre phase (stopping recursion and producing no output), you cannot return an empty string. Instead, return an empty HTML comment: &lt;!-- --&gt;. This stops recursion and renders as nothing.\n\n\n\n\n\nThis design allows for powerful compositions, such as a caching macro that wraps expensive operations.\nmacros:\n  - name: cache\n    # Check for cache hit. If found, cat the file.\n    # If not found, the script produces no output (empty string),\n    # so Lectic proceeds to expand the children.\n    pre: |\n      exec:#!/bin/bash\n      HASH=$(echo \"$ARG\" | md5sum | cut -d' ' -f1)\n      if [ -f \"/tmp/cache/$HASH\" ]; then\n        cat \"/tmp/cache/$HASH\"\n      fi\n    # If we reached post, it means pre didn't return anything (cache miss).\n    # We now have the result of the children in ARG. Save it and output it.\n    post: |\n      exec:#!/bin/bash\n      HASH=$(echo \"$ARG\" | md5sum | cut -d' ' -f1)\n      mkdir -p /tmp/cache\n      echo \"$ARG\" &gt; \"/tmp/cache/$HASH\"\n      echo \"$ARG\"\nUsage:\n:cache[:summarize[:cat[file.txt]]]\n\n:cache’s pre runs. If the cache exists for the raw text of the children, it returns the cached summary. Lectic replaces the :cache block with this text and is done.\nIf pre returns nothing (cache miss), Lectic enters the children.\n:cat expands to the file content.\n:summarize processes that content.\nFinally, :cache’s post runs. ARG contains the summary. It writes ARG to the cache and outputs it.",
    "crumbs": [
      "Automation",
      "Macros"
    ]
  },
  {
    "objectID": "tools/05_agent.html",
    "href": "tools/05_agent.html",
    "title": "Tools: Agent",
    "section": "",
    "text": "The agent tool allows you to create sophisticated multi-LLM workflows by enabling one interlocutor to call another as a tool. The “agent” interlocutor receives the query from the “caller” with no other context, processes it, and returns its response as the tool’s output.\nThis is a powerful way to separate concerns. You can create specialized agents and then compose them into more complex systems. For an excellent overview of the philosophy behind this approach, see Anthropic’s blog post on their multi-agent research system.\n\n\nTo use the agent tool, you must have at least two interlocutors defined. In the configuration for one interlocutor, you add an agent tool that points to the name of the other.\n\nagent: (Required) The name of the interlocutor to be called as an agent.\nname: An optional name for the tool.\nusage: A string, file:, or exec: URI providing instructions for the calling LLM on when and how to use this agent.\nraw_output: A boolean value. Normally an agent’s output will be sanitized, so that raw tool call results are not visible to the interlocutor who called the agent. Setting raw_output to true puts the full output from the agent into the main interlocutor’s tool call results.\n\n\n\nIn this setup, Kirk is the main interlocutor. He has a tool named communicator which, when used, will call the Spock interlocutor. Spock has his own set of tools, including a think_about tool to encourage careful reasoning.\ninterlocutors:\n  - name: Kirk\n    prompt: You are Captain Kirk. You are bold and decisive.\n    tools:\n      - agent: Spock\n        name: communicator\n        usage: Use this to contact Spock for logical analysis and advice.\n\n  - name: Spock\n    prompt: &gt;\n      You are Mr. Spock. You respond with pure logic, suppressing all\n      emotion.\n    tools:\n      - think_about: how to logically solve the problem presented.\n\n\n\n\nUsing the configuration above, Captain Kirk can delegate complex analysis to Spock.\nWe've encountered an alien vessel of unknown origin. It is not responding to\nhails. What is the logical course of action?\n\n:::Kirk\n\nThis situation requires careful analysis. I will consult my science officer.\n\n&lt;tool-call with=\"communicator\"&gt;\n&lt;arguments&gt;\n&lt;content&gt;\n┆Alien vessel, unknown origin, unresponsive. Propose logical course of action.\n&lt;/content&gt;\n&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆Insufficient data. Recommend passive scans to gather\n┆information on their technological capabilities before initiating\n┆further contact. Avoid any action that could be perceived as\n┆hostile.\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\nA logical approach. We will proceed with passive scans.\n\n:::",
    "crumbs": [
      "Tools",
      "Agent"
    ]
  },
  {
    "objectID": "tools/05_agent.html#configuration",
    "href": "tools/05_agent.html#configuration",
    "title": "Tools: Agent",
    "section": "",
    "text": "To use the agent tool, you must have at least two interlocutors defined. In the configuration for one interlocutor, you add an agent tool that points to the name of the other.\n\nagent: (Required) The name of the interlocutor to be called as an agent.\nname: An optional name for the tool.\nusage: A string, file:, or exec: URI providing instructions for the calling LLM on when and how to use this agent.\nraw_output: A boolean value. Normally an agent’s output will be sanitized, so that raw tool call results are not visible to the interlocutor who called the agent. Setting raw_output to true puts the full output from the agent into the main interlocutor’s tool call results.\n\n\n\nIn this setup, Kirk is the main interlocutor. He has a tool named communicator which, when used, will call the Spock interlocutor. Spock has his own set of tools, including a think_about tool to encourage careful reasoning.\ninterlocutors:\n  - name: Kirk\n    prompt: You are Captain Kirk. You are bold and decisive.\n    tools:\n      - agent: Spock\n        name: communicator\n        usage: Use this to contact Spock for logical analysis and advice.\n\n  - name: Spock\n    prompt: &gt;\n      You are Mr. Spock. You respond with pure logic, suppressing all\n      emotion.\n    tools:\n      - think_about: how to logically solve the problem presented.",
    "crumbs": [
      "Tools",
      "Agent"
    ]
  },
  {
    "objectID": "tools/05_agent.html#example-conversation",
    "href": "tools/05_agent.html#example-conversation",
    "title": "Tools: Agent",
    "section": "",
    "text": "Using the configuration above, Captain Kirk can delegate complex analysis to Spock.\nWe've encountered an alien vessel of unknown origin. It is not responding to\nhails. What is the logical course of action?\n\n:::Kirk\n\nThis situation requires careful analysis. I will consult my science officer.\n\n&lt;tool-call with=\"communicator\"&gt;\n&lt;arguments&gt;\n&lt;content&gt;\n┆Alien vessel, unknown origin, unresponsive. Propose logical course of action.\n&lt;/content&gt;\n&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆Insufficient data. Recommend passive scans to gather\n┆information on their technological capabilities before initiating\n┆further contact. Avoid any action that could be perceived as\n┆hostile.\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\nA logical approach. We will proceed with passive scans.\n\n:::",
    "crumbs": [
      "Tools",
      "Agent"
    ]
  },
  {
    "objectID": "tools/06_other_tools.html",
    "href": "tools/06_other_tools.html",
    "title": "Other Tools: think, serve, and native",
    "section": "",
    "text": "This document covers three distinct types of tools: a cognitive tool for the LLM, a simple web server, and a way to access the native, built-in capabilities of the model provider.\n\n\nThe think tool gives the LLM a private “scratch space” to pause and reason about a prompt before formulating its final response. This can improve the quality and thoughtfulness of the output, especially for complex or ambiguous questions.\nThis technique was inspired by a post on Anthropic’s engineering blog. The output of the think tool is hidden from the user by default in the editor plugins, though it is still present in the .lec file.\n\n\ntools:\n  - think_about: &gt;\n      What the user is really asking for, and what hidden assumptions they\n      might have.\n    name: scratchpad # Optional name\n\n\n\nWhat's the best city in the world?\n\n:::Assistant\n\n&lt;tool-call with=\"scratchpad\"&gt;\n&lt;arguments&gt;\n&lt;thought&gt;\n┆\"Best\" is subjective. The user could mean best for travel, for\n┆food, for work, etc. I need to ask for clarification.\n&lt;/thought&gt;\n&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆thought complete.\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\nThat depends on what you're looking for! Are you interested in the best city\nfor tourism, career opportunities, or something else?\n:::\n\n\n\n\nThe serve tool allows the LLM to spin up a simple, single-use web server to present content, such as an HTML file or a small web application it has generated.\nWhen the LLM uses this tool, Lectic starts a server on the specified port. It will then attempt to open the page in your default web browser. The server shuts down automatically after the first request is served. While the page is loading, Lectic waits for the first request—so the conversation resumes once your browser has loaded the page.\n\n\ntools:\n  - serve_on_port: 8080\n    name: web_server # Optional name\n\n\n\nGenerate a simple tic-tac-toe game in HTML and serve it to me.\n\n:::Assistant\n\n&lt;tool-call with=\"web_server\"&gt;\n&lt;arguments&gt;\n&lt;pageHtml&gt;\n┆&lt;!DOCTYPE html&gt;\n┆&lt;html&gt;\n┆&lt;head&gt;\n┆&lt;title&gt;Tic-Tac-Toe&lt;/title&gt;\n┆... (rest of the HTML/JS/CSS) ...\n┆&lt;/head&gt;\n┆&lt;body&gt;\n┆...\n┆&lt;/body&gt;\n┆&lt;/html&gt;\n&lt;/pageHtml&gt;\n&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆page is now available\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\n\nI have generated the game for you. It should be opening in your browser at\nhttp://localhost:8080.\n:::\n\n\n\n\nNative tools allow you to access functionality that is built directly into the LLM provider’s backend, such as web search or a code interpreter environment for data analysis.\nSupport for native tools varies by provider.\n\n\nYou enable native tools by specifying their type.\ntools:\n  - native: search # Enable the provider's built-in web search.\n  - native: code   # Enable the provider's built-in code interpreter.\n\n\n\n\nGemini: Supports both search and code. Note that the Gemini API has a limitation where you can only use one native tool at a time, and it cannot be combined with other (non-native) tools.\nAnthropic: Supports search only.\nOpenAI: Supports both search and code via the openai provider (not the legacy openai/chat provider).",
    "crumbs": [
      "Tools",
      "Other Tools"
    ]
  },
  {
    "objectID": "tools/06_other_tools.html#the-think-tool",
    "href": "tools/06_other_tools.html#the-think-tool",
    "title": "Other Tools: think, serve, and native",
    "section": "",
    "text": "The think tool gives the LLM a private “scratch space” to pause and reason about a prompt before formulating its final response. This can improve the quality and thoughtfulness of the output, especially for complex or ambiguous questions.\nThis technique was inspired by a post on Anthropic’s engineering blog. The output of the think tool is hidden from the user by default in the editor plugins, though it is still present in the .lec file.\n\n\ntools:\n  - think_about: &gt;\n      What the user is really asking for, and what hidden assumptions they\n      might have.\n    name: scratchpad # Optional name\n\n\n\nWhat's the best city in the world?\n\n:::Assistant\n\n&lt;tool-call with=\"scratchpad\"&gt;\n&lt;arguments&gt;\n&lt;thought&gt;\n┆\"Best\" is subjective. The user could mean best for travel, for\n┆food, for work, etc. I need to ask for clarification.\n&lt;/thought&gt;\n&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆thought complete.\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\nThat depends on what you're looking for! Are you interested in the best city\nfor tourism, career opportunities, or something else?\n:::",
    "crumbs": [
      "Tools",
      "Other Tools"
    ]
  },
  {
    "objectID": "tools/06_other_tools.html#the-serve-tool",
    "href": "tools/06_other_tools.html#the-serve-tool",
    "title": "Other Tools: think, serve, and native",
    "section": "",
    "text": "The serve tool allows the LLM to spin up a simple, single-use web server to present content, such as an HTML file or a small web application it has generated.\nWhen the LLM uses this tool, Lectic starts a server on the specified port. It will then attempt to open the page in your default web browser. The server shuts down automatically after the first request is served. While the page is loading, Lectic waits for the first request—so the conversation resumes once your browser has loaded the page.\n\n\ntools:\n  - serve_on_port: 8080\n    name: web_server # Optional name\n\n\n\nGenerate a simple tic-tac-toe game in HTML and serve it to me.\n\n:::Assistant\n\n&lt;tool-call with=\"web_server\"&gt;\n&lt;arguments&gt;\n&lt;pageHtml&gt;\n┆&lt;!DOCTYPE html&gt;\n┆&lt;html&gt;\n┆&lt;head&gt;\n┆&lt;title&gt;Tic-Tac-Toe&lt;/title&gt;\n┆... (rest of the HTML/JS/CSS) ...\n┆&lt;/head&gt;\n┆&lt;body&gt;\n┆...\n┆&lt;/body&gt;\n┆&lt;/html&gt;\n&lt;/pageHtml&gt;\n&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆page is now available\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\n\nI have generated the game for you. It should be opening in your browser at\nhttp://localhost:8080.\n:::",
    "crumbs": [
      "Tools",
      "Other Tools"
    ]
  },
  {
    "objectID": "tools/06_other_tools.html#native-tools",
    "href": "tools/06_other_tools.html#native-tools",
    "title": "Other Tools: think, serve, and native",
    "section": "",
    "text": "Native tools allow you to access functionality that is built directly into the LLM provider’s backend, such as web search or a code interpreter environment for data analysis.\nSupport for native tools varies by provider.\n\n\nYou enable native tools by specifying their type.\ntools:\n  - native: search # Enable the provider's built-in web search.\n  - native: code   # Enable the provider's built-in code interpreter.\n\n\n\n\nGemini: Supports both search and code. Note that the Gemini API has a limitation where you can only use one native tool at a time, and it cannot be combined with other (non-native) tools.\nAnthropic: Supports search only.\nOpenAI: Supports both search and code via the openai provider (not the legacy openai/chat provider).",
    "crumbs": [
      "Tools",
      "Other Tools"
    ]
  },
  {
    "objectID": "tools/02_exec.html",
    "href": "tools/02_exec.html",
    "title": "Tools: Command Execution (exec)",
    "section": "",
    "text": "The exec tool is one of the most versatile tools in Lectic. It allows the LLM to execute commands and scripts, enabling it to interact directly with your system, run code, and interface with other command‑line applications.\n\n\nThe snippets below show only the tool definition. They assume you have an interlocutor with a valid prompt and model configuration. See Getting Started for a full header example.\nYou configure an exec tool by providing the command to be executed. You can also provide a custom name for the LLM to use, a usage guide, and optional parameters for security and execution control.\n\n\nThis configuration allows the LLM to run the python3 interpreter.\ntools:\n  - exec: python3\n    name: python\n    usage: &gt;\n      Use this to execute Python code. The code to be executed should be\n      written inside the tool call block.\n\n\n\nYou can also provide a multi‑line script in the YAML. The first line of the script must be a shebang (for example, #!/usr/bin/env bash) to choose the interpreter.\ntools:\n  - name: line_counter\n    usage: \"Counts the number of lines in a file. Takes one argument: path.\"\n    exec: |\n      #!/usr/bin/env bash\n      # A simple script to count the lines in a file\n      wc -l \"$1\"\n\n\n\n\nexec: (Required) The command or inline script to execute.\nname: An optional name for the tool.\nusage: A string with instructions for the LLM. It also accepts file: and exec: sources. See External Prompts for semantics.\nsandbox: A command string to wrap execution (e.g. /path/to/script.sh or wrapper.sh arg1). See safety below. Overrides any interlocutor-level sandbox.\ntimeoutSeconds: Seconds to wait before aborting a long‑running call.\nenv: Environment variables to set for the subprocess.\nschema: A map of parameter name → description. When present, the tool takes named string parameters (exposed as env vars). When absent, the tool instead takes a required arguments array of strings.\n\n\n\n\n\n\nNo shell is involved when executing single line commands. The command is executed directly. Shell features like globbing or command substitution will not work unless you invoke a shell yourself.\nSingle‑line exec values have environment variables expanded before execution using the tool’s env plus standard Lectic variables.\nSingle‑line commands are split into argv using simple shell‑like rules: single ‘…’ and double “…” quotes are supported; no globbing or substitution. If you need shell features, invoke a shell explicitly, e.g., bash -lc '...'.\nMulti‑line exec values must start with a shebang. Lectic writes the script to a temporary file and executes it with that interpreter.\n\n\n\nThe current working directory for exec is:\n\nIf you run with -f or -i: the directory containing the .lec file.\nOtherwise: the directory from which you invoked the lectic command.\n\nThis means relative paths in your commands and scripts resolve relative to that directory. Temporary scripts are written into the same working directory.\n\n\n\nLectic captures stdout and stderr separately and returns both to the model. It also includes the numeric exit code when it is non‑zero. You will see these serialized inside the tool call results as XML tags like , , and .\nIf a timeout occurs, Lectic kills the subprocess and throws an error that includes any partial stdout and stderr collected so far.\n\n\n\nYou might want to control what arguments your LLM can pass to a command or script, or offer a template for correct usage. If your configuration includes a schema, the LLM will be guided to provide specific parameters when calling the script or command. Each parameter is a string and Lectic exposes it to the subprocess via an environment variable with the same name.\nThis applies to both commands and scripts:\n\nFor scripts, parameters are available as $PARAM_NAME inside the script.\nFor commands, parameters are available in the subprocess environment and also expanded in the command.\n\nExample:\n# YAML configuration\ntools:\n  - name: greeter\n    exec: |\n      #!/usr/bin/env bash\n      echo \"Hello, ${NAME}! Today is ${DAY}.\"\n    schema:\n      NAME: The name to greet.\n      DAY: The day string to include.\nor equivalently\n# YAML configuration\ntools:\n  - name: greeter\n    exec: echo \"Hello, ${NAME}! Today is ${DAY}.\"\n    schema:\n      NAME: The name to greet.\n      DAY: The day string to include.\nIf the LLM provides { NAME: \"Ada\", DAY: \"Friday\" } Lectic will fill in the results:\n&lt;tool-call with=\"greeter\"&gt;\n&lt;arguments&gt;\n&lt;NAME&gt;\n┆Ada\n&lt;/NAME&gt;\n&lt;DAY&gt;\n┆Friday\n&lt;/DAY&gt;\n&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆Hello, Ada! Today is Friday.\n┆\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\n\n\n\nWhen Lectic runs your command or script, it sets a few helpful environment variables. In particular, LECTIC_INTERLOCUTOR is set to the name of the interlocutor who invoked the tool. This makes it easy to maintain per‑interlocutor state (for example, separate scratch directories or memory stores) in your scripts or sandbox wrappers.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nGranting an LLM the ability to execute commands can be dangerous. Treat every exec tool as a capability you are delegating. Combine human‑in‑the‑ loop confirmation and sandboxing to minimize risk. Do not expose sensitive files or networks unless you fully trust the tool and its usage.\n\n\nLectic provides two mechanisms to help you keep exec tools safe: hooks and sandboxing.\n\n\nYou can use the tool_use_pre hook to implement confirmation dialogs or logic. See Hooks for examples.\n\n\n\nWhen a sandbox is configured, a tool call will actually execute the sandbox command, which will receive the original command and the LLM provided parameters as arguments. The wrapper is responsible for creating a controlled environment to run the command.\nYou can include arguments in the sandbox string (e.g. bwrap.sh --net).\nSee the Custom Sandboxing cookbook recipe for a detailed guide on writing your own sandbox scripts.\nFor example, extra/sandbox/bwrap-sandbox.sh uses Bubblewrap to create a minimal, isolated environment with a temporary home directory.\nYou can also set a default sandbox on the interlocutor object. If set, it applies to all exec tools that don’t specify their own.",
    "crumbs": [
      "Tools",
      "Exec"
    ]
  },
  {
    "objectID": "tools/02_exec.html#configuration",
    "href": "tools/02_exec.html#configuration",
    "title": "Tools: Command Execution (exec)",
    "section": "",
    "text": "The snippets below show only the tool definition. They assume you have an interlocutor with a valid prompt and model configuration. See Getting Started for a full header example.\nYou configure an exec tool by providing the command to be executed. You can also provide a custom name for the LLM to use, a usage guide, and optional parameters for security and execution control.\n\n\nThis configuration allows the LLM to run the python3 interpreter.\ntools:\n  - exec: python3\n    name: python\n    usage: &gt;\n      Use this to execute Python code. The code to be executed should be\n      written inside the tool call block.\n\n\n\nYou can also provide a multi‑line script in the YAML. The first line of the script must be a shebang (for example, #!/usr/bin/env bash) to choose the interpreter.\ntools:\n  - name: line_counter\n    usage: \"Counts the number of lines in a file. Takes one argument: path.\"\n    exec: |\n      #!/usr/bin/env bash\n      # A simple script to count the lines in a file\n      wc -l \"$1\"\n\n\n\n\nexec: (Required) The command or inline script to execute.\nname: An optional name for the tool.\nusage: A string with instructions for the LLM. It also accepts file: and exec: sources. See External Prompts for semantics.\nsandbox: A command string to wrap execution (e.g. /path/to/script.sh or wrapper.sh arg1). See safety below. Overrides any interlocutor-level sandbox.\ntimeoutSeconds: Seconds to wait before aborting a long‑running call.\nenv: Environment variables to set for the subprocess.\nschema: A map of parameter name → description. When present, the tool takes named string parameters (exposed as env vars). When absent, the tool instead takes a required arguments array of strings.",
    "crumbs": [
      "Tools",
      "Exec"
    ]
  },
  {
    "objectID": "tools/02_exec.html#execution-details",
    "href": "tools/02_exec.html#execution-details",
    "title": "Tools: Command Execution (exec)",
    "section": "",
    "text": "No shell is involved when executing single line commands. The command is executed directly. Shell features like globbing or command substitution will not work unless you invoke a shell yourself.\nSingle‑line exec values have environment variables expanded before execution using the tool’s env plus standard Lectic variables.\nSingle‑line commands are split into argv using simple shell‑like rules: single ‘…’ and double “…” quotes are supported; no globbing or substitution. If you need shell features, invoke a shell explicitly, e.g., bash -lc '...'.\nMulti‑line exec values must start with a shebang. Lectic writes the script to a temporary file and executes it with that interpreter.\n\n\n\nThe current working directory for exec is:\n\nIf you run with -f or -i: the directory containing the .lec file.\nOtherwise: the directory from which you invoked the lectic command.\n\nThis means relative paths in your commands and scripts resolve relative to that directory. Temporary scripts are written into the same working directory.\n\n\n\nLectic captures stdout and stderr separately and returns both to the model. It also includes the numeric exit code when it is non‑zero. You will see these serialized inside the tool call results as XML tags like , , and .\nIf a timeout occurs, Lectic kills the subprocess and throws an error that includes any partial stdout and stderr collected so far.\n\n\n\nYou might want to control what arguments your LLM can pass to a command or script, or offer a template for correct usage. If your configuration includes a schema, the LLM will be guided to provide specific parameters when calling the script or command. Each parameter is a string and Lectic exposes it to the subprocess via an environment variable with the same name.\nThis applies to both commands and scripts:\n\nFor scripts, parameters are available as $PARAM_NAME inside the script.\nFor commands, parameters are available in the subprocess environment and also expanded in the command.\n\nExample:\n# YAML configuration\ntools:\n  - name: greeter\n    exec: |\n      #!/usr/bin/env bash\n      echo \"Hello, ${NAME}! Today is ${DAY}.\"\n    schema:\n      NAME: The name to greet.\n      DAY: The day string to include.\nor equivalently\n# YAML configuration\ntools:\n  - name: greeter\n    exec: echo \"Hello, ${NAME}! Today is ${DAY}.\"\n    schema:\n      NAME: The name to greet.\n      DAY: The day string to include.\nIf the LLM provides { NAME: \"Ada\", DAY: \"Friday\" } Lectic will fill in the results:\n&lt;tool-call with=\"greeter\"&gt;\n&lt;arguments&gt;\n&lt;NAME&gt;\n┆Ada\n&lt;/NAME&gt;\n&lt;DAY&gt;\n┆Friday\n&lt;/DAY&gt;\n&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆Hello, Ada! Today is Friday.\n┆\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;",
    "crumbs": [
      "Tools",
      "Exec"
    ]
  },
  {
    "objectID": "tools/02_exec.html#execution-environment",
    "href": "tools/02_exec.html#execution-environment",
    "title": "Tools: Command Execution (exec)",
    "section": "",
    "text": "When Lectic runs your command or script, it sets a few helpful environment variables. In particular, LECTIC_INTERLOCUTOR is set to the name of the interlocutor who invoked the tool. This makes it easy to maintain per‑interlocutor state (for example, separate scratch directories or memory stores) in your scripts or sandbox wrappers.",
    "crumbs": [
      "Tools",
      "Exec"
    ]
  },
  {
    "objectID": "tools/02_exec.html#safety-and-trust",
    "href": "tools/02_exec.html#safety-and-trust",
    "title": "Tools: Command Execution (exec)",
    "section": "",
    "text": "Warning\n\n\n\nGranting an LLM the ability to execute commands can be dangerous. Treat every exec tool as a capability you are delegating. Combine human‑in‑the‑ loop confirmation and sandboxing to minimize risk. Do not expose sensitive files or networks unless you fully trust the tool and its usage.\n\n\nLectic provides two mechanisms to help you keep exec tools safe: hooks and sandboxing.\n\n\nYou can use the tool_use_pre hook to implement confirmation dialogs or logic. See Hooks for examples.\n\n\n\nWhen a sandbox is configured, a tool call will actually execute the sandbox command, which will receive the original command and the LLM provided parameters as arguments. The wrapper is responsible for creating a controlled environment to run the command.\nYou can include arguments in the sandbox string (e.g. bwrap.sh --net).\nSee the Custom Sandboxing cookbook recipe for a detailed guide on writing your own sandbox scripts.\nFor example, extra/sandbox/bwrap-sandbox.sh uses Bubblewrap to create a minimal, isolated environment with a temporary home directory.\nYou can also set a default sandbox on the interlocutor object. If set, it applies to all exec tools that don’t specify their own.",
    "crumbs": [
      "Tools",
      "Exec"
    ]
  },
  {
    "objectID": "04_conversation_format.html",
    "href": "04_conversation_format.html",
    "title": "The Lectic Conversation Format",
    "section": "",
    "text": "Lectic conversations are stored in plain markdown files, typically with a .lec extension. They use a superset of CommonMark, adding two specific conventions: a YAML frontmatter block for configuration and “container directives” for assistant responses.\n\n\nEvery Lectic file begins with a YAML frontmatter block, enclosed by three dashes (---). This is where you configure the conversation, defining the interlocutor(s), their models, prompts, and any tools they might use.\nA minimal header looks like this:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic\n  model: claude-3-haiku-20240307\n  # Optional thinking controls (Anthropic/Gemini):\n  # thinking_budget: 1024     # integer token budget for reasoning\n---\nThe frontmatter can be closed with either three dashes (---) or three periods (...). For a complete guide to all available options, see the Configuration page.\n\n\n\nAnything in the file that is not part of the YAML frontmatter or an assistant response block is considered a user message. You write your prompts, questions, and instructions here as plain text or standard markdown.\nThis is a user message.\n\nSo is this. You can include any markdown you like, such as **bold text** or\n`inline code`.\n\n\n\nLectic uses “container directives” to represent messages from the LLM. These are fenced blocks that start with a run of colons, followed immediately by the name of the interlocutor.\nThe canonical form is exactly three colons on open and close, like this:\n:::Name\n\nSome content.\n\n:::\nMarkdown code fences inside assistant blocks can also use three backticks.\n\n\nLectic records some generated content directly into the transcript as inline attachments. This includes:\n\nOutput of :cmd[...] directives (kind=\"cmd\")\nOutput of inline hooks (kind=\"hook\")\n\nInline attachments appear inside the assistant’s response block as XML. They include a &lt;command&gt; field (the command that produced the content, or the hook’s do value) and a &lt;content&gt; field.\nExample (:cmd[...]):\n&lt;inline-attachment kind=\"cmd\"&gt;\n&lt;command&gt;git diff --staged&lt;/command&gt;\n&lt;content type=\"text/plain\"&gt;\n┆diff --git a/src/main.ts b/src/main.ts\n┆...\n&lt;/content&gt;\n&lt;/inline-attachment&gt;\nExample (inline hook output):\n&lt;inline-attachment kind=\"hook\" final=\"true\"&gt;\n&lt;command&gt;~/.config/lectic/my-hook.sh&lt;/command&gt;\n&lt;content type=\"text/plain\"&gt;\n┆System check complete.\n&lt;/content&gt;\n&lt;/inline-attachment&gt;\nInline attachments serve two purposes:\n\nCaching: The results are stored in the file, so re-running Lectic doesn’t re-execute old commands. Only :cmd directives in the most recent user message are executed.\nContext positioning: When sending the conversation to the provider, attachments are treated as if they were a user message This keeps provider caches stable and avoids token recomputation.\n\nYou’ll see inline attachments frequently when using :cmd. They’re part of the conversation record and should generally be left alone. Editor plugins typically fold them by default to reduce visual clutter.\n\n\n\nHere is a complete, simple conversation file showing all the parts together:\n---\ninterlocutor:\n  name: Oggle\n  prompt: You are a skeptical assistant.\n---\n\nI'd like to know more about container directives.\n\n:::Oggle\n\nAre you sure? It seems like a rather niche topic. They are part of a\nproposed extension to CommonMark that allows for custom block-level\nelements.\n\nInside one of these blocks, standard markdown is still supported:\n\n```python\n# This is a regular code block\nprint(\"Hello from inside a directive!\")\n```\n\nIs that all you wanted to know?\n\n:::\nWhen you run lectic, it reads the entire file, sends the content to the LLM, and then appends the next assistant response in a new directive block.",
    "crumbs": [
      "Core Concepts",
      "Conversation Format"
    ]
  },
  {
    "objectID": "04_conversation_format.html#yaml-frontmatter",
    "href": "04_conversation_format.html#yaml-frontmatter",
    "title": "The Lectic Conversation Format",
    "section": "",
    "text": "Every Lectic file begins with a YAML frontmatter block, enclosed by three dashes (---). This is where you configure the conversation, defining the interlocutor(s), their models, prompts, and any tools they might use.\nA minimal header looks like this:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic\n  model: claude-3-haiku-20240307\n  # Optional thinking controls (Anthropic/Gemini):\n  # thinking_budget: 1024     # integer token budget for reasoning\n---\nThe frontmatter can be closed with either three dashes (---) or three periods (...). For a complete guide to all available options, see the Configuration page.",
    "crumbs": [
      "Core Concepts",
      "Conversation Format"
    ]
  },
  {
    "objectID": "04_conversation_format.html#user-messages",
    "href": "04_conversation_format.html#user-messages",
    "title": "The Lectic Conversation Format",
    "section": "",
    "text": "Anything in the file that is not part of the YAML frontmatter or an assistant response block is considered a user message. You write your prompts, questions, and instructions here as plain text or standard markdown.\nThis is a user message.\n\nSo is this. You can include any markdown you like, such as **bold text** or\n`inline code`.",
    "crumbs": [
      "Core Concepts",
      "Conversation Format"
    ]
  },
  {
    "objectID": "04_conversation_format.html#assistant-responses",
    "href": "04_conversation_format.html#assistant-responses",
    "title": "The Lectic Conversation Format",
    "section": "",
    "text": "Lectic uses “container directives” to represent messages from the LLM. These are fenced blocks that start with a run of colons, followed immediately by the name of the interlocutor.\nThe canonical form is exactly three colons on open and close, like this:\n:::Name\n\nSome content.\n\n:::\nMarkdown code fences inside assistant blocks can also use three backticks.\n\n\nLectic records some generated content directly into the transcript as inline attachments. This includes:\n\nOutput of :cmd[...] directives (kind=\"cmd\")\nOutput of inline hooks (kind=\"hook\")\n\nInline attachments appear inside the assistant’s response block as XML. They include a &lt;command&gt; field (the command that produced the content, or the hook’s do value) and a &lt;content&gt; field.\nExample (:cmd[...]):\n&lt;inline-attachment kind=\"cmd\"&gt;\n&lt;command&gt;git diff --staged&lt;/command&gt;\n&lt;content type=\"text/plain\"&gt;\n┆diff --git a/src/main.ts b/src/main.ts\n┆...\n&lt;/content&gt;\n&lt;/inline-attachment&gt;\nExample (inline hook output):\n&lt;inline-attachment kind=\"hook\" final=\"true\"&gt;\n&lt;command&gt;~/.config/lectic/my-hook.sh&lt;/command&gt;\n&lt;content type=\"text/plain\"&gt;\n┆System check complete.\n&lt;/content&gt;\n&lt;/inline-attachment&gt;\nInline attachments serve two purposes:\n\nCaching: The results are stored in the file, so re-running Lectic doesn’t re-execute old commands. Only :cmd directives in the most recent user message are executed.\nContext positioning: When sending the conversation to the provider, attachments are treated as if they were a user message This keeps provider caches stable and avoids token recomputation.\n\nYou’ll see inline attachments frequently when using :cmd. They’re part of the conversation record and should generally be left alone. Editor plugins typically fold them by default to reduce visual clutter.\n\n\n\nHere is a complete, simple conversation file showing all the parts together:\n---\ninterlocutor:\n  name: Oggle\n  prompt: You are a skeptical assistant.\n---\n\nI'd like to know more about container directives.\n\n:::Oggle\n\nAre you sure? It seems like a rather niche topic. They are part of a\nproposed extension to CommonMark that allows for custom block-level\nelements.\n\nInside one of these blocks, standard markdown is still supported:\n\n```python\n# This is a regular code block\nprint(\"Hello from inside a directive!\")\n```\n\nIs that all you wanted to know?\n\n:::\nWhen you run lectic, it reads the entire file, sends the content to the LLM, and then appends the next assistant response in a new directive block.",
    "crumbs": [
      "Core Concepts",
      "Conversation Format"
    ]
  },
  {
    "objectID": "reference/03_lsp.html",
    "href": "reference/03_lsp.html",
    "title": "LSP Server",
    "section": "",
    "text": "Lectic includes a small Language Server Protocol (LSP) server that provides completion for directives, macros, and common YAML header fields, plus hovers. It is stdio only.\nOverview - Command: lectic lsp - Transport: stdio (no --node-ipc or --socket) - Features: textDocument/completion, textDocument/hover, diagnostics, document symbols (outline), workspace symbols, folding ranges, code actions, semantic tokens, go to definition - Triggers: :, [ in directive brackets, and - in tools arrays - Insertions - Directives use snippets and place the cursor inside brackets (or at the end for reset): :cmd[${0:command}], :ask[$0], :aside[$0], :reset[]$0. - Macro names are suggested on : and insert as directives: :name[]$0. - Inside brackets of :ask[...] and :aside[...], only interlocutor names are offered. - Matching: case‑insensitive; typed prefix after : or inside [ is respected. - Fences: no suggestions inside :: or ::: runs - Trigger filtering: only :ask[/:aside[ produce bracket completions\nWhere completions come from - Directives: built‑in suggestions for :cmd, :ask, :aside, and :reset. - Macro names: merged from the same places and precedence as the CLI (higher wins): 1) System config: ${LECTIC_CONFIG}/lectic.yaml 2) Workspace config: lectic.yaml in the document directory 3) The document’s YAML header - YAML header fields: - interlocutor / interlocutors blocks: top‑level interlocutor properties such as name, prompt, provider, model, temperature, max_tokens, max_tool_use, thinking_effort, thinking_budget, tools, and nocache. - model: provider‑appropriate model names. - tools array items: tool kinds after a bare -. - kit: ...: merged kit names from system/workspace/header. - agent: ...: merged interlocutor names. - native: ...: supported native types (search, code). - Interlocutors: collected from the merged header as above, combining interlocutor and interlocutors. - De‑duplication is case‑insensitive on name. Higher‑precedence entry wins. - The server shows a simple preview in the completion item.\nBehavior examples - Type - on a line inside tools: → suggestions for tool kinds (exec, sqlite, mcp_*, native, kit, …). - Type : → suggestions for directives and macro names. - Type :su (with a summarize macro defined) → summarize appears and inserts :summarize[]$0. - Type :ask[ or :aside[ → invoke completion to see interlocutor names; selecting a name replaces only the text inside the […]. - Type :: or ::: → no suggestions (reserved for directive fences). - Place the cursor on a directive (e.g., :name[], :ask[Name]) or a name field in the YAML header (e.g., name: Assistant) and invoke “Go to Definition” to jump to the relevant definition. If multiple definitions exist (e.g., a local override of a workspace or system interlocutor), the LSP returns all locations, prioritized by proximity (local &gt; workspace &gt; system).\nNeovim setup (vim.lsp.start) - Minimal startup for the current buffer:\nlocal client_id = vim.lsp.start({\n  name = \"lectic\",\n  cmd = { \"lectic\", \"lsp\" },\n  root_dir = vim.fs.root(0, { \".git\", \"lectic.yaml\" })\n             or vim.fn.getcwd(),\n  single_file_support = true,\n})\n\nAuto‑start for .lec (and optionally markdown) files:\n\nvim.api.nvim_create_autocmd(\"FileType\", {\n  pattern = { \"lectic\", \"markdown.lectic\", \"lectic.markdown\" },\n  callback = function(args)\n    vim.lsp.start({\n      name = \"lectic\",\n      cmd = { \"lectic\", \"lsp\" },\n      root_dir = vim.fs.root(args.buf, { \".git\", \"lectic.yaml\" })\n                 or vim.fn.getcwd(),\n      single_file_support = true,\n    })\n  end,\n})\n\nUsing completion\n\nWith nvim‑cmp (recommended), LSP completion pops on : and -.\nInside brackets, invoke completion manually (Ctrl‑x Ctrl‑o) if your setup is not configured to trigger automatically.\n\n\nVS Code setup - The server is an external stdio LSP. You can connect to it from a VS Code extension. The repository includes extra/lectic.vscode for a ready‑made extension.\nDiagnostics - The server publishes diagnostics on open/change. - Header field validation covers missing or mistyped interlocutor properties (name, prompt, model, provider, etc.). Unknown top‑level properties on an interlocutor are reported as warnings (for example, Unknown property \"mood\" on interlocutor A.). - Duplicate names in the document header are warned with precise ranges. Later entries win at runtime; the warning helps catch mistakes. - Duplicates originating only from included configs may be reported with a coarse header-range warning.\nFolding - The LSP provides folding ranges for tool‑call and inline‑attachment blocks. A block must be a serialized &lt;tool-call ...&gt; ... &lt;/tool-call&gt; or &lt;inline-attachment ...&gt; ... &lt;/inline-attachment&gt; that appears as a direct child of an interlocutor container directive (:::Name).\nHovers - Hover over a directive (e.g., :ask[...]) to see a short description. - Hover on a macro directive name (e.g., :summarize[]) to preview the macro expansion.\nNotes - Completion previews are static; the server does not expand macros or read files referenced by file:.",
    "crumbs": [
      "Reference",
      "LSP Server"
    ]
  },
  {
    "objectID": "02_getting_started.html",
    "href": "02_getting_started.html",
    "title": "Getting Started with Lectic",
    "section": "",
    "text": "This short guide helps you install Lectic and run your first conversation. Along the way, you will verify your install, set an API key, and see a simple tool in action.\n\n\nChoose the method that fits your system.\n\n\nIf you use Nix, install directly from the repository:\nnix profile install github:gleachkr/lectic\n\n\n\nDownload the AppImage from the GitHub Releases page. Make it executable and put it on your PATH.\nchmod +x lectic-*.AppImage\nmv lectic-*.AppImage ~/.local/bin/lectic\n\n\n\nDownload the macOS binary from the GitHub Releases page and put it on your PATH.\n\n\n\n\nlectic --version\nIf you see a version number, you are ready to go.\n\n\n\nLectic has an extensible tab completion system that supports standard flags and Custom Subcommands.\nTo enable it, source the completion script in your shell configuration (e.g., ~/.bashrc):\n# Adjust path to where you cloned/extracted Lectic\nsource /path/to/lectic/extra/tab_complete/lectic_completion.bash\nor place the script in ~/.local/share/bash-completion/completions/ or one of the other standard locations for completion scripts.\n\n\n\n\n\nLectic talks to LLM providers. Put at least one provider key in your environment so Lectic can pick a default.\nexport ANTHROPIC_API_KEY=\"your-api-key-here\"\nLectic chooses a default provider by checking for keys in this order: Anthropic, then Gemini, then OpenAI, then OpenRouter. If you need Bedrock, set provider: anthropic/bedrock explicitly in your file and make sure your AWS credentials are configured. Bedrock is not auto‑selected.\nFinally, OpenAI has two provider choices. Use openai for the newer Responses API and native tools. Use openai/chat for the legacy Chat Completions API when you need it.\n\n\n\nMake a new file, for example my_convo.lec. The .lec extension helps with editor integration.\nAdd a minimal YAML header and your first user message:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic\n  model: claude-3-haiku-20240307\n  # Optional thinking controls (Anthropic/Gemini):\n  # thinking_budget: 1024     # integer token budget for reasoning\n---\n\nHello, world! What is a fun fact about the Rust programming language?\n\n\n\nFrom your terminal, run Lectic on the file. The -i flag updates the file in place.\nlectic -i my_convo.lec\nLectic sends your message to the model and appends its response in a new assistant block. You can add your next message under that block and run the command again to continue the conversation.\n\n\n\nNow add a very small tool to see the tool flow. This one exposes date.\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic\n  model: claude-3-haiku-20240307\n  tools:\n    - exec: date\n      name: get_date\n---\n\nWhat is today's date?\nRun Lectic again. The assistant block will now include an XML tool call and the recorded results. You will see tags like &lt;tool-call&gt;, &lt;arguments&gt;, and &lt;results&gt; in the block.\n\n\n\n\n\n\nTip\n\n\n\nYou can load prompts from files or compute them with commands using file: and exec:. See External Prompts.\n\n\n\n\n\n\nHere are solutions to common issues when getting started.\n\n\nLectic needs at least one provider key in your environment. Make sure you’ve exported it in the same shell session:\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\nlectic -i my_convo.lec\nIf you set the key in a config file (like .bashrc), you may need to restart your terminal or run source ~/.bashrc.\n\n\n\nCheck that your YAML header is valid. Common mistakes:\n\nIndentation errors (YAML requires consistent spacing)\nMissing colons after keys\nForgetting the closing --- after the frontmatter\n\nThe LSP server catches many of these. See Editor Integration to set it up.\n\n\n\nModel names vary by provider. Use lectic models to see what’s available for your configured API keys. Some common model names:\n\nAnthropic: claude-sonnet-4-20250514, claude-3-haiku-20240307\nOpenAI: gpt-4o, gpt-4o-mini\nGemini: gemini-2.5-flash, gemini-2.5-pro\n\nThe LSP server can autocomplete model names, so tab-complete is your friend here.\n\n\n\nMake sure tools are defined under the tools key inside interlocutor, not at the top level:\n# Correct\ninterlocutor:\n  name: Assistant\n  prompt: You are helpful.\n  tools:\n    - exec: date\n      name: get_date\n\n# Wrong - tools at top level\ninterlocutor:\n  name: Assistant\n  prompt: You are helpful.\ntools:  # This won't work\n  - exec: date\n\n\n\n\nNow that you have Lectic working, you’ll want to:\n\nSet up your editor. The intended workflow is to run Lectic with a single keypress. See Editor Integration for Neovim, VS Code, and other editors.\nLearn the configuration system. You can set global defaults, project-specific settings, and per-conversation overrides. See Configuration.\nExplore the cookbook. The Cookbook has ready-to-use recipes for common workflows like coding assistants, commit message generation, and multi-perspective research.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "02_getting_started.html#installation",
    "href": "02_getting_started.html#installation",
    "title": "Getting Started with Lectic",
    "section": "",
    "text": "Choose the method that fits your system.\n\n\nIf you use Nix, install directly from the repository:\nnix profile install github:gleachkr/lectic\n\n\n\nDownload the AppImage from the GitHub Releases page. Make it executable and put it on your PATH.\nchmod +x lectic-*.AppImage\nmv lectic-*.AppImage ~/.local/bin/lectic\n\n\n\nDownload the macOS binary from the GitHub Releases page and put it on your PATH.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "02_getting_started.html#verify-the-install",
    "href": "02_getting_started.html#verify-the-install",
    "title": "Getting Started with Lectic",
    "section": "",
    "text": "lectic --version\nIf you see a version number, you are ready to go.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "02_getting_started.html#tab-completion-optional",
    "href": "02_getting_started.html#tab-completion-optional",
    "title": "Getting Started with Lectic",
    "section": "",
    "text": "Lectic has an extensible tab completion system that supports standard flags and Custom Subcommands.\nTo enable it, source the completion script in your shell configuration (e.g., ~/.bashrc):\n# Adjust path to where you cloned/extracted Lectic\nsource /path/to/lectic/extra/tab_complete/lectic_completion.bash\nor place the script in ~/.local/share/bash-completion/completions/ or one of the other standard locations for completion scripts.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "02_getting_started.html#your-first-conversation",
    "href": "02_getting_started.html#your-first-conversation",
    "title": "Getting Started with Lectic",
    "section": "",
    "text": "Lectic talks to LLM providers. Put at least one provider key in your environment so Lectic can pick a default.\nexport ANTHROPIC_API_KEY=\"your-api-key-here\"\nLectic chooses a default provider by checking for keys in this order: Anthropic, then Gemini, then OpenAI, then OpenRouter. If you need Bedrock, set provider: anthropic/bedrock explicitly in your file and make sure your AWS credentials are configured. Bedrock is not auto‑selected.\nFinally, OpenAI has two provider choices. Use openai for the newer Responses API and native tools. Use openai/chat for the legacy Chat Completions API when you need it.\n\n\n\nMake a new file, for example my_convo.lec. The .lec extension helps with editor integration.\nAdd a minimal YAML header and your first user message:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic\n  model: claude-3-haiku-20240307\n  # Optional thinking controls (Anthropic/Gemini):\n  # thinking_budget: 1024     # integer token budget for reasoning\n---\n\nHello, world! What is a fun fact about the Rust programming language?\n\n\n\nFrom your terminal, run Lectic on the file. The -i flag updates the file in place.\nlectic -i my_convo.lec\nLectic sends your message to the model and appends its response in a new assistant block. You can add your next message under that block and run the command again to continue the conversation.\n\n\n\nNow add a very small tool to see the tool flow. This one exposes date.\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic\n  model: claude-3-haiku-20240307\n  tools:\n    - exec: date\n      name: get_date\n---\n\nWhat is today's date?\nRun Lectic again. The assistant block will now include an XML tool call and the recorded results. You will see tags like &lt;tool-call&gt;, &lt;arguments&gt;, and &lt;results&gt; in the block.\n\n\n\n\n\n\nTip\n\n\n\nYou can load prompts from files or compute them with commands using file: and exec:. See External Prompts.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "02_getting_started.html#troubleshooting",
    "href": "02_getting_started.html#troubleshooting",
    "title": "Getting Started with Lectic",
    "section": "",
    "text": "Here are solutions to common issues when getting started.\n\n\nLectic needs at least one provider key in your environment. Make sure you’ve exported it in the same shell session:\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\nlectic -i my_convo.lec\nIf you set the key in a config file (like .bashrc), you may need to restart your terminal or run source ~/.bashrc.\n\n\n\nCheck that your YAML header is valid. Common mistakes:\n\nIndentation errors (YAML requires consistent spacing)\nMissing colons after keys\nForgetting the closing --- after the frontmatter\n\nThe LSP server catches many of these. See Editor Integration to set it up.\n\n\n\nModel names vary by provider. Use lectic models to see what’s available for your configured API keys. Some common model names:\n\nAnthropic: claude-sonnet-4-20250514, claude-3-haiku-20240307\nOpenAI: gpt-4o, gpt-4o-mini\nGemini: gemini-2.5-flash, gemini-2.5-pro\n\nThe LSP server can autocomplete model names, so tab-complete is your friend here.\n\n\n\nMake sure tools are defined under the tools key inside interlocutor, not at the top level:\n# Correct\ninterlocutor:\n  name: Assistant\n  prompt: You are helpful.\n  tools:\n    - exec: date\n      name: get_date\n\n# Wrong - tools at top level\ninterlocutor:\n  name: Assistant\n  prompt: You are helpful.\ntools:  # This won't work\n  - exec: date",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "02_getting_started.html#next-steps",
    "href": "02_getting_started.html#next-steps",
    "title": "Getting Started with Lectic",
    "section": "",
    "text": "Now that you have Lectic working, you’ll want to:\n\nSet up your editor. The intended workflow is to run Lectic with a single keypress. See Editor Integration for Neovim, VS Code, and other editors.\nLearn the configuration system. You can set global defaults, project-specific settings, and per-conversation overrides. See Configuration.\nExplore the cookbook. The Cookbook has ready-to-use recipes for common workflows like coding assistants, commit message generation, and multi-perspective research.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "01_introduction.html",
    "href": "01_introduction.html",
    "title": "Introduction to Lectic",
    "section": "",
    "text": "Lectic is a unixy LLM toolbox. It treats conversations as plain text files, which means you can version control them, grep them, pipe them, email them, and edit them in whatever editor you like.\n\n\n\n\nEvery conversation is a markdown file (.lec). Because your conversations are files, you can do anything with them that you can do with files:\n\nVersion control: Track changes with git, branch experiments, diff conversations.\nSearch: grep across your conversation history.\nProcess: Pipe conversations through other tools. Combine lectic with sed, pandoc, or anything else.\nBack up: Copy files. Sync with rsync. Store wherever you want.\n\n\n\n\nLectic includes an LSP server that provides completions, diagnostics, hover information, go-to-definition, and folding for .lec files. You can use lectic with Neovim, VS Code, or any editor that speaks LSP.\nFor editors without LSP support, the basic workflow still works: edit a file, run lectic -i file.lec, and the response is appended.\n\n\n\nTools, hooks, and macros are executables. Write them in whatever language you prefer. If it can read environment variables and write to stdout, it works with Lectic.\nThis means you’re not locked into a plugin ecosystem or a specific scripting language. Your existing scripts and tools integrate directly.\n\n\n\nLectic provides a small set of building blocks:\n\n:cmd: Run a command and include its output.\n:ask / :aside: Switch between interlocutors.\n:reset: Clear conversation context.\nMacros: Reusable text expansions.\nHooks: Run code on events (message sent, tool called, etc.).\nTools: Give the LLM capabilities (shell, database, MCP servers).\n\nYou can put these together to build all sorts of AI applications: coding assistants, reseach and analysis workflows, orchestrated multi-agent swarms, or simple one-shot command line text processing tools. Take a look at the Cookbook for some detailed recipes.\n\n\n\n\nA minimal conversation:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n---\n\nWhat's 2 + 2?\n\n:::Assistant\n\n4\n\n:::\nA more interesting one:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a code reviewer.\n  tools:\n    - exec: cat\n      name: read_file\n    - exec: rg --json\n      name: search\n---\nReview the error handling in src/main.ts. Are there any uncaught\nexceptions?\nThe assistant can read files and search the codebase to answer.\n\n\n\n\nGetting Started: Install Lectic and run your first conversation.\nEditor Integration: Set up your editor for the best experience.\nConfiguration: Learn about the configuration system.\nTools: Give your LLM capabilities.\nCookbook: Ready-to-use recipes for common workflows.\n\nAll of the documentation concatinated into a single markdown file can be found here",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01_introduction.html#core-principles",
    "href": "01_introduction.html#core-principles",
    "title": "Introduction to Lectic",
    "section": "",
    "text": "Every conversation is a markdown file (.lec). Because your conversations are files, you can do anything with them that you can do with files:\n\nVersion control: Track changes with git, branch experiments, diff conversations.\nSearch: grep across your conversation history.\nProcess: Pipe conversations through other tools. Combine lectic with sed, pandoc, or anything else.\nBack up: Copy files. Sync with rsync. Store wherever you want.\n\n\n\n\nLectic includes an LSP server that provides completions, diagnostics, hover information, go-to-definition, and folding for .lec files. You can use lectic with Neovim, VS Code, or any editor that speaks LSP.\nFor editors without LSP support, the basic workflow still works: edit a file, run lectic -i file.lec, and the response is appended.\n\n\n\nTools, hooks, and macros are executables. Write them in whatever language you prefer. If it can read environment variables and write to stdout, it works with Lectic.\nThis means you’re not locked into a plugin ecosystem or a specific scripting language. Your existing scripts and tools integrate directly.\n\n\n\nLectic provides a small set of building blocks:\n\n:cmd: Run a command and include its output.\n:ask / :aside: Switch between interlocutors.\n:reset: Clear conversation context.\nMacros: Reusable text expansions.\nHooks: Run code on events (message sent, tool called, etc.).\nTools: Give the LLM capabilities (shell, database, MCP servers).\n\nYou can put these together to build all sorts of AI applications: coding assistants, reseach and analysis workflows, orchestrated multi-agent swarms, or simple one-shot command line text processing tools. Take a look at the Cookbook for some detailed recipes.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01_introduction.html#quick-example",
    "href": "01_introduction.html#quick-example",
    "title": "Introduction to Lectic",
    "section": "",
    "text": "A minimal conversation:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n---\n\nWhat's 2 + 2?\n\n:::Assistant\n\n4\n\n:::\nA more interesting one:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a code reviewer.\n  tools:\n    - exec: cat\n      name: read_file\n    - exec: rg --json\n      name: search\n---\nReview the error handling in src/main.ts. Are there any uncaught\nexceptions?\nThe assistant can read files and search the codebase to answer.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01_introduction.html#next-steps",
    "href": "01_introduction.html#next-steps",
    "title": "Introduction to Lectic",
    "section": "",
    "text": "Getting Started: Install Lectic and run your first conversation.\nEditor Integration: Set up your editor for the best experience.\nConfiguration: Learn about the configuration system.\nTools: Give your LLM capabilities.\nCookbook: Ready-to-use recipes for common workflows.\n\nAll of the documentation concatinated into a single markdown file can be found here",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "context_management/01_external_content.html",
    "href": "context_management/01_external_content.html",
    "title": "Managing Context: External Content",
    "section": "",
    "text": "Lectic aims to make it easy to pull external information into the conversation, providing the LLM with the context it needs to answer questions, analyze data, or perform tasks.\nThis is done in two primary ways: by referencing files and URIs using standard Markdown links, and by executing shell commands with the :cmd directive.\n\n\nYou can include local or remote content using the standard Markdown link syntax, [Title](URI). Lectic will fetch the content from the URI and include it in the context for the LLM.\nPlease summarize this local document: [Notes](./notes.md)\n\nAnalyze the data in this S3 bucket: [Dataset](s3://my_bucket/dataset.csv)\n\nWhat does this README say?\n[Repo](github+repo://gleachkr/Lectic/contents/README.md)\n\n\n\nText: Plain text files are included directly.\nImages: PNG, JPEG, GIF, and WebP images are supported.\nPDFs: Content from PDF files can be extracted (requires a provider that supports PDF ingestion, such as Anthropic, Gemini, or OpenAI).\nAudio: Gemini and OpenAI support audio inputs. For OpenAI, use provider: openai/chat with an audio‑capable model; supported formats include MP3, MPEG, and WAV. Gemini supports a broader set of audio types.\nVideo: Gemini supports video understanding. See supported formats in Google’s docs: https://ai.google.dev/gemini-api/docs/video-understanding#supported-formats\n\n\n\n\nLectic supports several URI schemes for referencing content:\n\nLocal Files: Simple relative paths like ./src/main.rs or absolute file:///path/to/file.txt URIs.\nRemote Content: http:// and https:// for web pages and other online resources.\nAmazon S3: s3:// for referencing objects in S3 buckets. This requires AWS credentials to be configured in your environment.\nMCP Resources: You can reference resources provided by an MCP server using a custom scheme, like github+repo://..., where github is the name of the MCP server (provided in the tool specification), and the rest is the resource URL.\n\nA few convenience rules apply:\n\nFor local file references using file://, use absolute paths. A portable way to build these is with $PWD (e.g., file://$PWD/papers/some.pdf).\nEnvironment variables in URIs use the $VAR form; ${VAR} is not supported. Expansion happens before any globbing.\nEnvironment variable expansion also applies to bare local paths (non‑URL links), such as ./$DATA_ROOT/file.txt. Expansion happens before any globbing.\n\nYou can use glob patterns to include multiple files at once. This is useful for providing the entire source code of a project as context.\n[All source code](./src/**/*.ts)\n[All images in this directory](./images/*.jpg)\nLectic uses Bun’s Glob API for matching.\n\n\n\nUsing full file:// URIs for local content enables additional capabilities.\n\n\nLectic supports environment variable expansion in URIs. This helps in creating portable .lec files that don’t rely on hardcoded absolute paths.\n[My dataset](file://$DATA_ROOT/my_project/data.csv)\n[Log file](file://$PWD/logs/latest.log)\n\n\n\nWhen referencing a PDF, you can point to a specific page or a range of pages by adding a fragment to the URI. Page numbering starts at 1.\n\nSingle Page: [See page 5](file.pdf#page=5)\nPage Range: [See Chapter 2](book.pdf#pages=20-35)\n\nIf both page and pages are supplied, pages takes precedence. If a page or range is malformed or out of bounds, Lectic will surface an error that is visible to the LLM.\n\n\n\n\n\nUse :cmd[...] to execute a shell command and capture its stdout and stderr. Lectic runs the command with the Bun shell. The result is returned as an inline attachment that appears at the very top of the next assistant block.\nHow it works - When you run lectic, any :cmd[...] found in the last user message is executed, and the result is forwarded to the LLM. - The result is also inserted as an inline attachment chunk at the beginning of the generated assistant block. It looks like an XML block beginning with &lt;inline-attachment ...&gt;. It includes the command and its content (stdout, or an error wrapper that includes stdout and stderr). - During provider serialization, this attachment is treated as if it were a user message that immediately precedes the assistant’s round. This keeps provider caches stable and avoids recomputing earlier commands. - Older :cmd[...] directives are not re-executed. Their cached attachments remain part of the transcript and are reused across runs.\nUse cases - System information: What can you tell me about my system? :cmd[uname -a] - Project state: Write a commit message: :cmd[git diff --staged] - Data analysis: Compute the average: :cmd[cat data.csv | awk '...']\nNotes - Output is wrapped in XML. On success, stdout is included. On failure, an &lt;error&gt; wrapper includes both stdout and stderr. - :cmd runs with Bun’s $ shell in the current working directory. Standard Lectic environment variables like LECTIC_FILE are available. By contrast, single‑line exec tools run without a shell; invoke a shell explicitly if you need shell features. - line breaks inside of :cmd are ignored so that if a :cmd happens to line-wrap, the newline doesn’t affect the command to be executed.",
    "crumbs": [
      "Context Management",
      "External Content"
    ]
  },
  {
    "objectID": "context_management/01_external_content.html#content-references-via-markdown-links",
    "href": "context_management/01_external_content.html#content-references-via-markdown-links",
    "title": "Managing Context: External Content",
    "section": "",
    "text": "You can include local or remote content using the standard Markdown link syntax, [Title](URI). Lectic will fetch the content from the URI and include it in the context for the LLM.\nPlease summarize this local document: [Notes](./notes.md)\n\nAnalyze the data in this S3 bucket: [Dataset](s3://my_bucket/dataset.csv)\n\nWhat does this README say?\n[Repo](github+repo://gleachkr/Lectic/contents/README.md)\n\n\n\nText: Plain text files are included directly.\nImages: PNG, JPEG, GIF, and WebP images are supported.\nPDFs: Content from PDF files can be extracted (requires a provider that supports PDF ingestion, such as Anthropic, Gemini, or OpenAI).\nAudio: Gemini and OpenAI support audio inputs. For OpenAI, use provider: openai/chat with an audio‑capable model; supported formats include MP3, MPEG, and WAV. Gemini supports a broader set of audio types.\nVideo: Gemini supports video understanding. See supported formats in Google’s docs: https://ai.google.dev/gemini-api/docs/video-understanding#supported-formats\n\n\n\n\nLectic supports several URI schemes for referencing content:\n\nLocal Files: Simple relative paths like ./src/main.rs or absolute file:///path/to/file.txt URIs.\nRemote Content: http:// and https:// for web pages and other online resources.\nAmazon S3: s3:// for referencing objects in S3 buckets. This requires AWS credentials to be configured in your environment.\nMCP Resources: You can reference resources provided by an MCP server using a custom scheme, like github+repo://..., where github is the name of the MCP server (provided in the tool specification), and the rest is the resource URL.\n\nA few convenience rules apply:\n\nFor local file references using file://, use absolute paths. A portable way to build these is with $PWD (e.g., file://$PWD/papers/some.pdf).\nEnvironment variables in URIs use the $VAR form; ${VAR} is not supported. Expansion happens before any globbing.\nEnvironment variable expansion also applies to bare local paths (non‑URL links), such as ./$DATA_ROOT/file.txt. Expansion happens before any globbing.\n\nYou can use glob patterns to include multiple files at once. This is useful for providing the entire source code of a project as context.\n[All source code](./src/**/*.ts)\n[All images in this directory](./images/*.jpg)\nLectic uses Bun’s Glob API for matching.\n\n\n\nUsing full file:// URIs for local content enables additional capabilities.\n\n\nLectic supports environment variable expansion in URIs. This helps in creating portable .lec files that don’t rely on hardcoded absolute paths.\n[My dataset](file://$DATA_ROOT/my_project/data.csv)\n[Log file](file://$PWD/logs/latest.log)\n\n\n\nWhen referencing a PDF, you can point to a specific page or a range of pages by adding a fragment to the URI. Page numbering starts at 1.\n\nSingle Page: [See page 5](file.pdf#page=5)\nPage Range: [See Chapter 2](book.pdf#pages=20-35)\n\nIf both page and pages are supplied, pages takes precedence. If a page or range is malformed or out of bounds, Lectic will surface an error that is visible to the LLM.",
    "crumbs": [
      "Context Management",
      "External Content"
    ]
  },
  {
    "objectID": "context_management/01_external_content.html#command-output-via-cmd-directive",
    "href": "context_management/01_external_content.html#command-output-via-cmd-directive",
    "title": "Managing Context: External Content",
    "section": "",
    "text": "Use :cmd[...] to execute a shell command and capture its stdout and stderr. Lectic runs the command with the Bun shell. The result is returned as an inline attachment that appears at the very top of the next assistant block.\nHow it works - When you run lectic, any :cmd[...] found in the last user message is executed, and the result is forwarded to the LLM. - The result is also inserted as an inline attachment chunk at the beginning of the generated assistant block. It looks like an XML block beginning with &lt;inline-attachment ...&gt;. It includes the command and its content (stdout, or an error wrapper that includes stdout and stderr). - During provider serialization, this attachment is treated as if it were a user message that immediately precedes the assistant’s round. This keeps provider caches stable and avoids recomputing earlier commands. - Older :cmd[...] directives are not re-executed. Their cached attachments remain part of the transcript and are reused across runs.\nUse cases - System information: What can you tell me about my system? :cmd[uname -a] - Project state: Write a commit message: :cmd[git diff --staged] - Data analysis: Compute the average: :cmd[cat data.csv | awk '...']\nNotes - Output is wrapped in XML. On success, stdout is included. On failure, an &lt;error&gt; wrapper includes both stdout and stderr. - :cmd runs with Bun’s $ shell in the current working directory. Standard Lectic environment variables like LECTIC_FILE are available. By contrast, single‑line exec tools run without a shell; invoke a shell explicitly if you need shell features. - line breaks inside of :cmd are ignored so that if a :cmd happens to line-wrap, the newline doesn’t affect the command to be executed.",
    "crumbs": [
      "Context Management",
      "External Content"
    ]
  },
  {
    "objectID": "context_management/02_conversation_control.html",
    "href": "context_management/02_conversation_control.html",
    "title": "Managing Context: Conversation Control",
    "section": "",
    "text": "Beyond simply adding external data, Lectic provides directives that allow you to actively manage the flow of a conversation. You can switch between different LLM interlocutors and control the context window that is sent to the model.\n\n\nLectic allows you to define multiple interlocutors in the YAML frontmatter. This enables you to bring different “personalities” or models with different capabilities into a single conversation.\nTo do this, use the interlocutors key (instead of interlocutor) and provide a list of configurations.\n---\ninterlocutors:\n  - name: Boggle\n    provider: anthropic\n    model: claude-3-sonnet-20240229\n    prompt: You are an expert on personal finance.\n  - name: Oggle\n    provider: gemini\n    prompt: You are very skeptical of conventional financial advice.\n---\nOnce multiple interlocutors are defined, Lectic will continue with whichever one was last active. To direct your message to a specific interlocutor, you use the :ask and :aside directives.\n\n\nThe :ask[InterlocutorName] directive changes the active speaker. All subsequent conversation turns will be handled by this new interlocutor until another :ask directive is used.\n:ask[Boggle] What is the best way to save for retirement?\n\n:::Boggle\nThe most common advice is to invest in a diversified portfolio of low-cost\nindex funds.\n:::\n\n:ask[Oggle] What's your take on that?\n\n:::Oggle\n\"Common advice\" is often just dogma. That strategy leaves you completely\nexposed to market downturns.\n:::\n\n\n\nThe :aside[InterlocutorName] directive is for a one-off question or comment. It directs the current message to the specified interlocutor, but the conversation then reverts back to the previously active speaker for the next user message.\nThis is useful for quick interjections or for getting a second opinion without derailing the main flow of the conversation.\n\n\n\n\nAs a conversation grows longer, the context sent to the LLM on each turn also grows. This can increase costs and, in some cases, lead to the model getting “stuck” on earlier parts of the dialogue.\nThe :reset[] directive gives you manual control over the context window. When Lectic processes a message containing :reset[], it instructs the LLM to ignore all preceding conversation history. The context for that turn effectively begins with the message containing the directive.\n...a very long conversation happens here...\n\nOkay, let's start fresh. I'm going to :reset[] your context now. Please\nsummarize our previous discussion and then we can move on to the next topic.\n\n:::Assistant\nUnderstood. I have cleared my context.\n\nPreviously, we discussed the history of the Roman Empire, focusing on the\nreign of Augustus and the establishment of the Pax Romana. We concluded that\neconomic stability was a key factor in the era's success.\n\nI am ready for the next topic.\n:::\n\nExcellent. Now, how did this conversation begin?\n\n:::Assistant\nThis conversation began with you instructing me to reset my context and provide\na summary of our previous discussion about the Roman Empire.\n:::\nThis is a powerful tool for managing long-running conversations, allowing you to “compact” the context manually or with the help of the LLM.\n\n\n\nSometimes you need to change the configuration of the conversation on the fly. Lectic provides two directives for this purpose, allowing you to merge new YAML configuration into the header.\n\n\nThe :merge_yaml directive allows you to permanently update the conversation’s configuration. The provided YAML is merged with the existing header, overriding any existing keys.\n:merge_yaml[{ interlocutor: { model: \"claude-3-opus-20240229\" } }]\nThis change persists for all subsequent turns in the conversation.\n\n\n\nThe :temp_merge_yaml directive updates the configuration for the current turn only. This is useful for one-off changes, such as temporarily increasing the max_tokens limit or enabling a specific tool.\n:temp_merge_yaml[{ interlocutor: { max_tokens: 4000 } }]\nOnce the turn is complete, the configuration reverts to its previous state for subsequent messages.",
    "crumbs": [
      "Context Management",
      "Conversation Control"
    ]
  },
  {
    "objectID": "context_management/02_conversation_control.html#multiparty-conversations-with-ask-and-aside",
    "href": "context_management/02_conversation_control.html#multiparty-conversations-with-ask-and-aside",
    "title": "Managing Context: Conversation Control",
    "section": "",
    "text": "Lectic allows you to define multiple interlocutors in the YAML frontmatter. This enables you to bring different “personalities” or models with different capabilities into a single conversation.\nTo do this, use the interlocutors key (instead of interlocutor) and provide a list of configurations.\n---\ninterlocutors:\n  - name: Boggle\n    provider: anthropic\n    model: claude-3-sonnet-20240229\n    prompt: You are an expert on personal finance.\n  - name: Oggle\n    provider: gemini\n    prompt: You are very skeptical of conventional financial advice.\n---\nOnce multiple interlocutors are defined, Lectic will continue with whichever one was last active. To direct your message to a specific interlocutor, you use the :ask and :aside directives.\n\n\nThe :ask[InterlocutorName] directive changes the active speaker. All subsequent conversation turns will be handled by this new interlocutor until another :ask directive is used.\n:ask[Boggle] What is the best way to save for retirement?\n\n:::Boggle\nThe most common advice is to invest in a diversified portfolio of low-cost\nindex funds.\n:::\n\n:ask[Oggle] What's your take on that?\n\n:::Oggle\n\"Common advice\" is often just dogma. That strategy leaves you completely\nexposed to market downturns.\n:::\n\n\n\nThe :aside[InterlocutorName] directive is for a one-off question or comment. It directs the current message to the specified interlocutor, but the conversation then reverts back to the previously active speaker for the next user message.\nThis is useful for quick interjections or for getting a second opinion without derailing the main flow of the conversation.",
    "crumbs": [
      "Context Management",
      "Conversation Control"
    ]
  },
  {
    "objectID": "context_management/02_conversation_control.html#context-management-with-reset",
    "href": "context_management/02_conversation_control.html#context-management-with-reset",
    "title": "Managing Context: Conversation Control",
    "section": "",
    "text": "As a conversation grows longer, the context sent to the LLM on each turn also grows. This can increase costs and, in some cases, lead to the model getting “stuck” on earlier parts of the dialogue.\nThe :reset[] directive gives you manual control over the context window. When Lectic processes a message containing :reset[], it instructs the LLM to ignore all preceding conversation history. The context for that turn effectively begins with the message containing the directive.\n...a very long conversation happens here...\n\nOkay, let's start fresh. I'm going to :reset[] your context now. Please\nsummarize our previous discussion and then we can move on to the next topic.\n\n:::Assistant\nUnderstood. I have cleared my context.\n\nPreviously, we discussed the history of the Roman Empire, focusing on the\nreign of Augustus and the establishment of the Pax Romana. We concluded that\neconomic stability was a key factor in the era's success.\n\nI am ready for the next topic.\n:::\n\nExcellent. Now, how did this conversation begin?\n\n:::Assistant\nThis conversation began with you instructing me to reset my context and provide\na summary of our previous discussion about the Roman Empire.\n:::\nThis is a powerful tool for managing long-running conversations, allowing you to “compact” the context manually or with the help of the LLM.",
    "crumbs": [
      "Context Management",
      "Conversation Control"
    ]
  },
  {
    "objectID": "context_management/02_conversation_control.html#dynamic-configuration-with-merge_yaml-and-temp_merge_yaml",
    "href": "context_management/02_conversation_control.html#dynamic-configuration-with-merge_yaml-and-temp_merge_yaml",
    "title": "Managing Context: Conversation Control",
    "section": "",
    "text": "Sometimes you need to change the configuration of the conversation on the fly. Lectic provides two directives for this purpose, allowing you to merge new YAML configuration into the header.\n\n\nThe :merge_yaml directive allows you to permanently update the conversation’s configuration. The provided YAML is merged with the existing header, overriding any existing keys.\n:merge_yaml[{ interlocutor: { model: \"claude-3-opus-20240229\" } }]\nThis change persists for all subsequent turns in the conversation.\n\n\n\nThe :temp_merge_yaml directive updates the configuration for the current turn only. This is useful for one-off changes, such as temporarily increasing the max_tokens limit or enabling a specific tool.\n:temp_merge_yaml[{ interlocutor: { max_tokens: 4000 } }]\nOnce the turn is complete, the configuration reverts to its previous state for subsequent messages.",
    "crumbs": [
      "Context Management",
      "Conversation Control"
    ]
  },
  {
    "objectID": "context_management/03_external_prompts.html",
    "href": "context_management/03_external_prompts.html",
    "title": "External Prompts and Instructions",
    "section": "",
    "text": "Many Lectic fields can load their text from outside the document. You can point a field at a file on disk, or run a command (or script) and use its output. This lets you keep prompts, usage text, and notes in one place, or compute them on demand.\nWhat supports external sources:\n\ninterlocutor.prompt\nmacros[].expansion\ntools[].usage (for tools that accept a usage string)\ntools[].details (for tools that provide extra details)\n\nEach of these accepts either a plain string, or a string beginning with one of the prefixes below.\n\n\nLoads the contents of PATH and uses that as the field value. Environment variables in the path are expanded before reading.\nExamples\ninterlocutor:\n  name: Assistant\n  prompt: file:./prompts/assistant.md\nmacros:\n  - name: summarize\n    expansion: file:$HOME/.config/lectic/prompts/summarize.txt\n\n\n\nRuns the command and uses its stdout as the field value. There are two forms:\n\nSingle line: executed directly, not through a shell. Shell features like globbing and command substitution do not work. If you need them, invoke a shell explicitly (for example, bash -lc '...').\nMulti‑line: treated as a script. The first line must be a shebang (for example, #!/usr/bin/env bash). The script is written to a temporary file and executed with the interpreter from the shebang.\n\nEnvironment variables in a single‑line command are expanded before running. For multi‑line scripts, variables are available via the process environment at runtime.\n\n\n\nSingle line\ninterlocutor:\n  name: Assistant\n  prompt: exec:echo \"You are a helpful assistant.\"\nMulti‑line script\ninterlocutor:\n  name: Assistant\n  prompt: |\n    exec:#!/usr/bin/env bash\n    cat &lt;&lt;'PREFACE'\n    You are a helpful assistant.\n    You will incorporate recent memory below.\n    PREFACE\n    echo\n    echo \"Recent memory:\"\n    sqlite3 \"$LECTIC_DATA/memory.sqlite3\" \\\n      \"SELECT printf('- %s (%s)', text, ts) FROM memory \\\n       ORDER BY ts DESC LIMIT 5;\"\n\n\n\n\nfile: and exec: resolve relative paths and run commands in the current working directory of the lectic process (the directory from which you invoked the command). If you used -f or -i, note that the working directory does not automatically switch to the .lec file’s directory for these expansions. Use absolute paths or cd if you need a different base.\nStandard Lectic environment variables are provided, including LECTIC_CONFIG, LECTIC_DATA, LECTIC_CACHE, LECTIC_STATE, LECTIC_TEMP, and LECTIC_FILE (when using -f or -i). Your shell environment is also passed through.\nMacro expansions can inject additional variables into exec: via directive attributes. See the Macros guide for details.\n\n\n\n\n\nThe value is recomputed on each run. This makes it easy to incorporate recent state (for example, “memory” from a local database) into a prompt.\nIf a file cannot be read or a command fails, Lectic reports an error and aborts the run. Fix the source and try again.\n\nSee also\n\nExternal Content for attaching files to user messages.\nMacros for passing variables into exec: expansions within macros.",
    "crumbs": [
      "Context Management",
      "External Prompts"
    ]
  },
  {
    "objectID": "context_management/03_external_prompts.html#filepath",
    "href": "context_management/03_external_prompts.html#filepath",
    "title": "External Prompts and Instructions",
    "section": "",
    "text": "Loads the contents of PATH and uses that as the field value. Environment variables in the path are expanded before reading.\nExamples\ninterlocutor:\n  name: Assistant\n  prompt: file:./prompts/assistant.md\nmacros:\n  - name: summarize\n    expansion: file:$HOME/.config/lectic/prompts/summarize.txt",
    "crumbs": [
      "Context Management",
      "External Prompts"
    ]
  },
  {
    "objectID": "context_management/03_external_prompts.html#execcommand-or-execscript",
    "href": "context_management/03_external_prompts.html#execcommand-or-execscript",
    "title": "External Prompts and Instructions",
    "section": "",
    "text": "Runs the command and uses its stdout as the field value. There are two forms:\n\nSingle line: executed directly, not through a shell. Shell features like globbing and command substitution do not work. If you need them, invoke a shell explicitly (for example, bash -lc '...').\nMulti‑line: treated as a script. The first line must be a shebang (for example, #!/usr/bin/env bash). The script is written to a temporary file and executed with the interpreter from the shebang.\n\nEnvironment variables in a single‑line command are expanded before running. For multi‑line scripts, variables are available via the process environment at runtime.",
    "crumbs": [
      "Context Management",
      "External Prompts"
    ]
  },
  {
    "objectID": "context_management/03_external_prompts.html#examples",
    "href": "context_management/03_external_prompts.html#examples",
    "title": "External Prompts and Instructions",
    "section": "",
    "text": "Single line\ninterlocutor:\n  name: Assistant\n  prompt: exec:echo \"You are a helpful assistant.\"\nMulti‑line script\ninterlocutor:\n  name: Assistant\n  prompt: |\n    exec:#!/usr/bin/env bash\n    cat &lt;&lt;'PREFACE'\n    You are a helpful assistant.\n    You will incorporate recent memory below.\n    PREFACE\n    echo\n    echo \"Recent memory:\"\n    sqlite3 \"$LECTIC_DATA/memory.sqlite3\" \\\n      \"SELECT printf('- %s (%s)', text, ts) FROM memory \\\n       ORDER BY ts DESC LIMIT 5;\"",
    "crumbs": [
      "Context Management",
      "External Prompts"
    ]
  },
  {
    "objectID": "context_management/03_external_prompts.html#working-directory-and-environment",
    "href": "context_management/03_external_prompts.html#working-directory-and-environment",
    "title": "External Prompts and Instructions",
    "section": "",
    "text": "file: and exec: resolve relative paths and run commands in the current working directory of the lectic process (the directory from which you invoked the command). If you used -f or -i, note that the working directory does not automatically switch to the .lec file’s directory for these expansions. Use absolute paths or cd if you need a different base.\nStandard Lectic environment variables are provided, including LECTIC_CONFIG, LECTIC_DATA, LECTIC_CACHE, LECTIC_STATE, LECTIC_TEMP, and LECTIC_FILE (when using -f or -i). Your shell environment is also passed through.\nMacro expansions can inject additional variables into exec: via directive attributes. See the Macros guide for details.",
    "crumbs": [
      "Context Management",
      "External Prompts"
    ]
  },
  {
    "objectID": "context_management/03_external_prompts.html#behavior-and-errors",
    "href": "context_management/03_external_prompts.html#behavior-and-errors",
    "title": "External Prompts and Instructions",
    "section": "",
    "text": "The value is recomputed on each run. This makes it easy to incorporate recent state (for example, “memory” from a local database) into a prompt.\nIf a file cannot be read or a command fails, Lectic reports an error and aborts the run. Fix the source and try again.\n\nSee also\n\nExternal Content for attaching files to user messages.\nMacros for passing variables into exec: expansions within macros.",
    "crumbs": [
      "Context Management",
      "External Prompts"
    ]
  },
  {
    "objectID": "05_configuration.html",
    "href": "05_configuration.html",
    "title": "Lectic Configuration",
    "section": "",
    "text": "Lectic offers a flexible configuration system that lets you set global defaults, create per-project settings, and make conversation-specific overrides. This is managed through a hierarchy of YAML files.\n\n\nConfiguration settings are merged from multiple sources. Each source in the list below overrides the one before it, with the .lec file’s own header always having the final say.\n\nSystem Config Directory: Lectic first looks for a configuration file at lectic/lectic.yaml within your system’s standard config location (e.g., ~/.config/lectic/lectic.yaml on Linux). This is the ideal place for your global, user-level defaults.\nWorking Directory: Next, it looks for a lectic.yaml file in the current working directory. This is useful for project-level configuration that you might commit to a git repository.\nLectic File Header: The YAML frontmatter within your .lec file is the final and highest-precedence source of configuration.\n\n\n\n\nYou can change the default locations for Lectic’s data directories by setting the following environment variables:\n\n$LECTIC_CONFIG: Overrides the base configuration directory path.\n$LECTIC_DATA: Overrides the data directory path.\n$LECTIC_CACHE: Overrides the cache directory path.\n$LECTIC_STATE: Overrides the state directory path.\n\nThese variables, along with $LECTIC_TEMP (a temporary directory) and $LECTIC_FILE (the path to the active .lec file), are automatically passed into the environment of any subprocesses Lectic spawns, such as exec tools. This ensures your custom scripts have access to the same context as the main process.\n\n\n\nWhen combining settings from multiple configuration files, Lectic follows specific rules:\n\nObjects (Mappings): If the higher precedence source has a name attribute, and the name doesn’t match the name attribute of the lower precedence source, then the higher precedence source replaces the lower precedence source. Otherwise, the objects are combined and matching keys merged recursively.\nArrays (Lists): Merged based on the name attribute of their elements. If two objects in an array share the same name, they are merged. Otherwise, the elements are simply combined. This is especially useful for managing lists of tools and interlocutors. When duplicate named items appear within a single file, later entries override earlier ones. The LSP warns on duplicate names in the document header to help catch mistakes.\nOther Values: For simple values (strings, numbers) or if the types don’t match, the value from the highest-precedence source is used without any merging.\n\nAdditionally, the LSP validates header fields. It reports errors for missing or mistyped interlocutor properties and warns when you add unknown properties to an interlocutor mapping, which helps catch typos in keys like model or max_tokens.\n\n\nImagine you have a global config in ~/.config/lectic/lectic.yaml:\n# ~/.config/lectic/lectic.yaml\ninterlocutors:\n    - name: opus\n      provider: anthropic\n      model: claude-3-opus-20240229\nAnd a project-specific file, project.yaml:\n# ./project.yaml\ninterlocutor:\n    name: haiku\n    model: claude-3-haiku-20240307\n    tools:\n        - exec: bash\n        - agent: opus\nIf you place this project config at ./lectic.yaml in your working directory, Lectic will merge it with your system defaults and the document header using the precedence above. The haiku interlocutor will be configured with the claude-3-haiku model, and it will have access to a bash tool and an agent tool that can call opus. You can switch to opus within the conversation if needed, using an :ask[] directive.",
    "crumbs": [
      "Core Concepts",
      "Configuration"
    ]
  },
  {
    "objectID": "05_configuration.html#configuration-hierarchy",
    "href": "05_configuration.html#configuration-hierarchy",
    "title": "Lectic Configuration",
    "section": "",
    "text": "Configuration settings are merged from multiple sources. Each source in the list below overrides the one before it, with the .lec file’s own header always having the final say.\n\nSystem Config Directory: Lectic first looks for a configuration file at lectic/lectic.yaml within your system’s standard config location (e.g., ~/.config/lectic/lectic.yaml on Linux). This is the ideal place for your global, user-level defaults.\nWorking Directory: Next, it looks for a lectic.yaml file in the current working directory. This is useful for project-level configuration that you might commit to a git repository.\nLectic File Header: The YAML frontmatter within your .lec file is the final and highest-precedence source of configuration.",
    "crumbs": [
      "Core Concepts",
      "Configuration"
    ]
  },
  {
    "objectID": "05_configuration.html#overriding-default-directories",
    "href": "05_configuration.html#overriding-default-directories",
    "title": "Lectic Configuration",
    "section": "",
    "text": "You can change the default locations for Lectic’s data directories by setting the following environment variables:\n\n$LECTIC_CONFIG: Overrides the base configuration directory path.\n$LECTIC_DATA: Overrides the data directory path.\n$LECTIC_CACHE: Overrides the cache directory path.\n$LECTIC_STATE: Overrides the state directory path.\n\nThese variables, along with $LECTIC_TEMP (a temporary directory) and $LECTIC_FILE (the path to the active .lec file), are automatically passed into the environment of any subprocesses Lectic spawns, such as exec tools. This ensures your custom scripts have access to the same context as the main process.",
    "crumbs": [
      "Core Concepts",
      "Configuration"
    ]
  },
  {
    "objectID": "05_configuration.html#merging-logic",
    "href": "05_configuration.html#merging-logic",
    "title": "Lectic Configuration",
    "section": "",
    "text": "When combining settings from multiple configuration files, Lectic follows specific rules:\n\nObjects (Mappings): If the higher precedence source has a name attribute, and the name doesn’t match the name attribute of the lower precedence source, then the higher precedence source replaces the lower precedence source. Otherwise, the objects are combined and matching keys merged recursively.\nArrays (Lists): Merged based on the name attribute of their elements. If two objects in an array share the same name, they are merged. Otherwise, the elements are simply combined. This is especially useful for managing lists of tools and interlocutors. When duplicate named items appear within a single file, later entries override earlier ones. The LSP warns on duplicate names in the document header to help catch mistakes.\nOther Values: For simple values (strings, numbers) or if the types don’t match, the value from the highest-precedence source is used without any merging.\n\nAdditionally, the LSP validates header fields. It reports errors for missing or mistyped interlocutor properties and warns when you add unknown properties to an interlocutor mapping, which helps catch typos in keys like model or max_tokens.\n\n\nImagine you have a global config in ~/.config/lectic/lectic.yaml:\n# ~/.config/lectic/lectic.yaml\ninterlocutors:\n    - name: opus\n      provider: anthropic\n      model: claude-3-opus-20240229\nAnd a project-specific file, project.yaml:\n# ./project.yaml\ninterlocutor:\n    name: haiku\n    model: claude-3-haiku-20240307\n    tools:\n        - exec: bash\n        - agent: opus\nIf you place this project config at ./lectic.yaml in your working directory, Lectic will merge it with your system defaults and the document header using the precedence above. The haiku interlocutor will be configured with the claude-3-haiku model, and it will have access to a bash tool and an agent tool that can call opus. You can switch to opus within the conversation if needed, using an :ask[] directive.",
    "crumbs": [
      "Core Concepts",
      "Configuration"
    ]
  },
  {
    "objectID": "06_providers_and_models.html",
    "href": "06_providers_and_models.html",
    "title": "Providers and Models",
    "section": "",
    "text": "Lectic speaks to several providers. You pick a provider and a model in your YAML header, or let Lectic choose a default based on which API keys are in your environment.\n\n\nIf you do not set provider, Lectic checks for keys in this order and uses the first one it finds:\nAnthropic → Gemini → OpenAI → OpenRouter.\nSet one of these environment variables before you run Lectic:\n\nANTHROPIC_API_KEY\nGEMINI_API_KEY\nOPENAI_API_KEY\nOPENROUTER_API_KEY\n\nAWS credentials for Bedrock are not used for auto‑selection. If you want Anthropic via Bedrock, set provider: anthropic/bedrock explicitly and make sure your AWS environment is configured.\n\n\n\nYou can list available models for providers that have API keys configured by running:\nlectic models\nThe command prints each detected provider followed by its models. If no known provider keys are set, it prints a short message and exits.\n\n\n\nOpenAI has two modes in Lectic today.\n\nopenai selects the Responses API. Choose this when you want native tools like search and code.\nopenai/chat selects the legacy Chat Completions API.\n\n\n\n\nAnthropic, direct API (uses thinking_budget when set):\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic\n  model: claude-3-haiku-20240307\n  # Optional thinking controls (Anthropic/Gemini):\n  # thinking_budget: 1024     # integer token budget for reasoning\nAnthropic via Bedrock:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic/bedrock\n  model: anthropic.claude-3-haiku-20240307-v1:0\nOpenAI Responses API:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: openai\n  model: gpt-4o-mini\nOpenAI Chat Completions:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: openai/chat\n  model: gpt-4o-mini\nGemini:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: gemini\n  model: gemini-2.5-flash\nOpenRouter:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: openrouter\n  model: meta-llama/llama-3.1-70b-instruct\nOllama (local inference):\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: ollama\n  model: llama3.1\n\n\n\nProviders differ in what they accept as input. Most accept plain text and images. Many accept PDFs and short audio clips. Support changes quickly, so consult each provider’s documentation for current limits on formats, sizes, page counts, and rate limits.\nIn Lectic, you attach external content by linking files in the user message body. Lectic will package these and send them to the provider in a way that fits that provider’s API. See External Content for examples and tips.",
    "crumbs": [
      "Core Concepts",
      "Providers and Models"
    ]
  },
  {
    "objectID": "06_providers_and_models.html#picking-a-default-provider",
    "href": "06_providers_and_models.html#picking-a-default-provider",
    "title": "Providers and Models",
    "section": "",
    "text": "If you do not set provider, Lectic checks for keys in this order and uses the first one it finds:\nAnthropic → Gemini → OpenAI → OpenRouter.\nSet one of these environment variables before you run Lectic:\n\nANTHROPIC_API_KEY\nGEMINI_API_KEY\nOPENAI_API_KEY\nOPENROUTER_API_KEY\n\nAWS credentials for Bedrock are not used for auto‑selection. If you want Anthropic via Bedrock, set provider: anthropic/bedrock explicitly and make sure your AWS environment is configured.",
    "crumbs": [
      "Core Concepts",
      "Providers and Models"
    ]
  },
  {
    "objectID": "06_providers_and_models.html#discover-models",
    "href": "06_providers_and_models.html#discover-models",
    "title": "Providers and Models",
    "section": "",
    "text": "You can list available models for providers that have API keys configured by running:\nlectic models\nThe command prints each detected provider followed by its models. If no known provider keys are set, it prints a short message and exits.",
    "crumbs": [
      "Core Concepts",
      "Providers and Models"
    ]
  },
  {
    "objectID": "06_providers_and_models.html#openai-two-provider-strings",
    "href": "06_providers_and_models.html#openai-two-provider-strings",
    "title": "Providers and Models",
    "section": "",
    "text": "OpenAI has two modes in Lectic today.\n\nopenai selects the Responses API. Choose this when you want native tools like search and code.\nopenai/chat selects the legacy Chat Completions API.",
    "crumbs": [
      "Core Concepts",
      "Providers and Models"
    ]
  },
  {
    "objectID": "06_providers_and_models.html#examples",
    "href": "06_providers_and_models.html#examples",
    "title": "Providers and Models",
    "section": "",
    "text": "Anthropic, direct API (uses thinking_budget when set):\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic\n  model: claude-3-haiku-20240307\n  # Optional thinking controls (Anthropic/Gemini):\n  # thinking_budget: 1024     # integer token budget for reasoning\nAnthropic via Bedrock:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic/bedrock\n  model: anthropic.claude-3-haiku-20240307-v1:0\nOpenAI Responses API:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: openai\n  model: gpt-4o-mini\nOpenAI Chat Completions:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: openai/chat\n  model: gpt-4o-mini\nGemini:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: gemini\n  model: gemini-2.5-flash\nOpenRouter:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: openrouter\n  model: meta-llama/llama-3.1-70b-instruct\nOllama (local inference):\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: ollama\n  model: llama3.1",
    "crumbs": [
      "Core Concepts",
      "Providers and Models"
    ]
  },
  {
    "objectID": "06_providers_and_models.html#capabilities-and-media",
    "href": "06_providers_and_models.html#capabilities-and-media",
    "title": "Providers and Models",
    "section": "",
    "text": "Providers differ in what they accept as input. Most accept plain text and images. Many accept PDFs and short audio clips. Support changes quickly, so consult each provider’s documentation for current limits on formats, sizes, page counts, and rate limits.\nIn Lectic, you attach external content by linking files in the user message body. Lectic will package these and send them to the provider in a way that fits that provider’s API. See External Content for examples and tips.",
    "crumbs": [
      "Core Concepts",
      "Providers and Models"
    ]
  },
  {
    "objectID": "reference/01_cli.html",
    "href": "reference/01_cli.html",
    "title": "Reference: Command‑Line Interface",
    "section": "",
    "text": "The lectic command is the primary way to interact with Lectic. It can read from a file or from standard input, and it offers flags to control how the result is printed or saved.\n\n\nlectic [FLAGS] [OPTIONS] [SUBCOMMAND] [ARGS...]\n\n\n\n\nlectic lsp Start the LSP server. Transport: stdio.\nlectic parse Parse a lectic file into a JSON representation of the parsed file structure. Useful for programmatic analysis and modification.\nFlags:\n\n--yaml: Emit YAML instead of JSON.\n--reverse: Ingest JSON (or YAML) output and reconstruct the original lectic file.\n\nlectic models List available models for providers with detected API keys. Only providers with API keys in the environment are queried.\nlectic script Run an ES module file using Lectic’s internal Bun JS runtime. Works as a hashbang interpreter, useful for writing subcommands (see below), hooks, and exec tools. For example:\n#!/bin/env -S lectic script\nconsole.log(\"Hello from a lectic script!\")\n\n\n\n\nLectic supports git-style custom subcommands. If you invoke lectic &lt;command&gt;, Lectic will look for an executable named lectic-&lt;command&gt; in your configuration directory, data directory, or PATH.\nSee Custom Subcommands for a full guide on creating subcommands and adding tab completion for them.\n\n\n\nThe repository includes a bash completion script. See Getting Started for installation instructions.\nThe completion system is extensible. You can write plugins to provide completions for your custom subcommands. See the Custom Subcommands guide for details.\n\n\n\n\n-v, --version Prints the version string.\n-f, --file &lt;PATH&gt; Path to the conversation file (.lec) to process. If omitted, Lectic reads from standard input.\n-i, --inplace &lt;PATH&gt; Read from the given file and update it in place. Mutually exclusive with --file.\n-s, --short Only emit the newly generated assistant message, not the full updated conversation.\n-S, --Short Like --short, but emits only the raw message text (without the :::Speaker wrapper).\n-l, --log &lt;PATH&gt; Write detailed debug logs to the given file.\n-q, --quiet Suppress printing the assistant’s response to stdout.\n-h, --help Show help for all flags and options.\n\n\n\n\n\n–inplace cannot be combined with –file.\n–quiet cannot be combined with –short or –Short.\n\n\n\n\n\nGenerate the next message in a file and update it in place:\nlectic -i conversation.lec\nRead from stdin and write the full result to stdout:\ncat conversation.lec | lectic\nStream just the new assistant message:\nlectic -s -f conversation.lec\nAdd a message from the command line and update the file:\necho \"This is a new message.\" | lectic -i conversation.lec\nList available models for detected providers:\nlectic models\nStart the LSP server (stdio transport):\nlectic lsp\nParse a file to JSON:\nlectic parse -f conversation.lec\nRound-trip a file through parsing and reconstruction:\nlectic parse -f conversation.lec | lectic parse --reverse",
    "crumbs": [
      "Reference",
      "CLI"
    ]
  },
  {
    "objectID": "reference/01_cli.html#usage",
    "href": "reference/01_cli.html#usage",
    "title": "Reference: Command‑Line Interface",
    "section": "",
    "text": "lectic [FLAGS] [OPTIONS] [SUBCOMMAND] [ARGS...]",
    "crumbs": [
      "Reference",
      "CLI"
    ]
  },
  {
    "objectID": "reference/01_cli.html#subcommands",
    "href": "reference/01_cli.html#subcommands",
    "title": "Reference: Command‑Line Interface",
    "section": "",
    "text": "lectic lsp Start the LSP server. Transport: stdio.\nlectic parse Parse a lectic file into a JSON representation of the parsed file structure. Useful for programmatic analysis and modification.\nFlags:\n\n--yaml: Emit YAML instead of JSON.\n--reverse: Ingest JSON (or YAML) output and reconstruct the original lectic file.\n\nlectic models List available models for providers with detected API keys. Only providers with API keys in the environment are queried.\nlectic script Run an ES module file using Lectic’s internal Bun JS runtime. Works as a hashbang interpreter, useful for writing subcommands (see below), hooks, and exec tools. For example:\n#!/bin/env -S lectic script\nconsole.log(\"Hello from a lectic script!\")",
    "crumbs": [
      "Reference",
      "CLI"
    ]
  },
  {
    "objectID": "reference/01_cli.html#custom-subcommands",
    "href": "reference/01_cli.html#custom-subcommands",
    "title": "Reference: Command‑Line Interface",
    "section": "",
    "text": "Lectic supports git-style custom subcommands. If you invoke lectic &lt;command&gt;, Lectic will look for an executable named lectic-&lt;command&gt; in your configuration directory, data directory, or PATH.\nSee Custom Subcommands for a full guide on creating subcommands and adding tab completion for them.",
    "crumbs": [
      "Reference",
      "CLI"
    ]
  },
  {
    "objectID": "reference/01_cli.html#bash-completion",
    "href": "reference/01_cli.html#bash-completion",
    "title": "Reference: Command‑Line Interface",
    "section": "",
    "text": "The repository includes a bash completion script. See Getting Started for installation instructions.\nThe completion system is extensible. You can write plugins to provide completions for your custom subcommands. See the Custom Subcommands guide for details.",
    "crumbs": [
      "Reference",
      "CLI"
    ]
  },
  {
    "objectID": "reference/01_cli.html#flags-and-options",
    "href": "reference/01_cli.html#flags-and-options",
    "title": "Reference: Command‑Line Interface",
    "section": "",
    "text": "-v, --version Prints the version string.\n-f, --file &lt;PATH&gt; Path to the conversation file (.lec) to process. If omitted, Lectic reads from standard input.\n-i, --inplace &lt;PATH&gt; Read from the given file and update it in place. Mutually exclusive with --file.\n-s, --short Only emit the newly generated assistant message, not the full updated conversation.\n-S, --Short Like --short, but emits only the raw message text (without the :::Speaker wrapper).\n-l, --log &lt;PATH&gt; Write detailed debug logs to the given file.\n-q, --quiet Suppress printing the assistant’s response to stdout.\n-h, --help Show help for all flags and options.",
    "crumbs": [
      "Reference",
      "CLI"
    ]
  },
  {
    "objectID": "reference/01_cli.html#constraints",
    "href": "reference/01_cli.html#constraints",
    "title": "Reference: Command‑Line Interface",
    "section": "",
    "text": "–inplace cannot be combined with –file.\n–quiet cannot be combined with –short or –Short.",
    "crumbs": [
      "Reference",
      "CLI"
    ]
  },
  {
    "objectID": "reference/01_cli.html#common-examples",
    "href": "reference/01_cli.html#common-examples",
    "title": "Reference: Command‑Line Interface",
    "section": "",
    "text": "Generate the next message in a file and update it in place:\nlectic -i conversation.lec\nRead from stdin and write the full result to stdout:\ncat conversation.lec | lectic\nStream just the new assistant message:\nlectic -s -f conversation.lec\nAdd a message from the command line and update the file:\necho \"This is a new message.\" | lectic -i conversation.lec\nList available models for detected providers:\nlectic models\nStart the LSP server (stdio transport):\nlectic lsp\nParse a file to JSON:\nlectic parse -f conversation.lec\nRound-trip a file through parsing and reconstruction:\nlectic parse -f conversation.lec | lectic parse --reverse",
    "crumbs": [
      "Reference",
      "CLI"
    ]
  },
  {
    "objectID": "reference/02_configuration.html",
    "href": "reference/02_configuration.html",
    "title": "Reference: Configuration Keys",
    "section": "",
    "text": "This document provides a reference for all the keys available in Lectic’s YAML configuration, including the main .lec file frontmatter and any included configuration files.\n\n\n\ninterlocutor: A single object defining the primary LLM speaker.\ninterlocutors: A list of interlocutor objects for multiparty conversations.\nkits: A list of named tool kits you can reference from interlocutors.\nmacros: A list of macro definitions. See Macros.\nhooks: A list of hook definitions. See Hooks.\n\n\n\n\n\nAn interlocutor object defines a single LLM “personality” or configuration.\n\nname: (Required) The name of the speaker, used in the :::Name response blocks.\nprompt: (Required) The base system prompt that defines the LLM’s personality and instructions. The value can be a string, or it can be loaded from a file (file:./path.txt) or a command (exec:get-prompt). See External Prompts for details and examples.\nhooks: A list of hook definitions. See Hooks. These hooks fire only when this interlocutor is active.\nsandbox: A command string (e.g. /path/to/script.sh or wrapper.sh arg1) to wrap execution for all exec tools and local mcp_command tools used by this interlocutor, unless overridden by the tool’s own sandbox setting.\n\n\n\n\nprovider: The LLM provider to use. Supported values include anthropic, anthropic/bedrock, openai (Responses API), openai/chat (legacy Chat Completions), gemini, ollama, and openrouter.\nmodel: The specific model to use, e.g., claude-3-opus-20240229.\ntemperature: A number between 0 and 1 controlling the randomness of the output.\nmax_tokens: The maximum number of tokens to generate in a response.\nmax_tool_use: The maximum number of tool calls the LLM is allowed to make in a single turn.\nthinking_effort: Optional hint (used by the openai Responses provider, and by gemini-3-pro) about how much effort to spend reasoning. One of none, low, medium, or high.\nthinking_budget: Optional integer token budget for providers that support structured thinking phases (Anthropic, Anthropic/Bedrock, Gemini). Ignored by the openai and openai/chat providers.\n\n\n\nIf you don’t specify provider, Lectic picks a default based on your environment. It checks for known API keys in this order and uses the first one it finds:\n\nANTHROPIC_API_KEY\nGEMINI_API_KEY\nOPENAI_API_KEY\nOPENROUTER_API_KEY\n\nAWS credentials for Bedrock are not considered for auto‑selection. If you want Anthropic via Bedrock, set provider: anthropic/bedrock explicitly and ensure your AWS environment is configured.\nOpenAI has two provider options:\n\nopenai uses the Responses API. You’ll want this for native tools like search and code.\nopenai/chat uses the legacy Chat Completions API. You’ll need this for certain audio workflows that still require chat‑style models.\n\nFor a more detailed discussion of provider and model options, see Providers and Models.\n\n\n\n\n\ntools: A list of tool definitions that this interlocutor can use. The format of each object in the list depends on the tool type. See the Tools section for detailed configuration guides. All tools support a hooks array for tool_use_pre hooks scoped to that particular tool.\n\n\n\nThese keys are shared across multiple tool types:\n\nname: A custom name for the tool. If omitted, a default is derived from the tool type.\nusage: Instructions for the LLM on when and how to use the tool. Accepts a string, file:, or exec: source.\nhooks: A list of hooks scoped to this tool (typically tool_use_pre).\n\n\n\n\nRun commands and scripts.\n\nexec: (Required) The command or inline script to execute. Multi-line values must start with a shebang.\nschema: A map of parameter name → description. When present, the tool takes named string parameters (exposed as env vars). When absent, the tool takes a required arguments array of strings.\nsandbox: Command string to wrap execution. Arguments supported.\ntimeoutSeconds: Seconds to wait before aborting.\nenv: Environment variables to set for the subprocess.\n\n\n\n\nQuery SQLite databases.\n\nsqlite: (Required) Path to the SQLite database file.\nreadonly: Boolean. If true, opens the database in read-only mode.\nlimit: Maximum size of serialized response in bytes.\ndetails: Extra context for the model. Accepts string, file:, or exec:.\nextensions: A list of SQLite extension libraries to load.\n\n\n\n\nCall another interlocutor as a tool.\n\nagent: (Required) The name of the interlocutor to call.\nraw_output: Boolean. If true, includes raw tool call results in the output rather than sanitized text.\n\n\n\n\nConnect to Model Context Protocol servers.\n\nOne of: mcp_command, mcp_ws, mcp_sse, or mcp_shttp.\nargs: Arguments for mcp_command.\nenv: Environment variables for mcp_command.\nsandbox: Optional wrapper command to isolate mcp_command servers.\nroots: Optional list of root objects for file access (each with uri and optional name).\nexclude: Optional list of server tool names to hide.\n\n\n\n\n\nthink_about: (String) Creates a thinking/scratchpad tool with the given prompt.\nserve_on_port: (Integer) Creates a single-use web server on the given port.\nnative: One of search or code. Enables provider built-in tools.\nkit: Name of a tool kit to include.\n\nIf you add keys to an interlocutor object that are not listed in this section, Lectic will still parse the YAML, but the LSP marks those properties as unknown with a warning. This is usually a sign of a typo in a key name.\n\n\n\n\n\n\n\nname: (Required) The name of the macro, used when invoking it with :name[] or :name[args].\nexpansion: (Optional) The content to be expanded. Can be a string, or loaded via file: or exec:. Equivalent to post if provided. See External Prompts for details about file: and exec:.\npre: (Optional) Expansion content for the pre-order phase.\npost: (Optional) Expansion content for the post-order phase.\nenv: (Optional) A dictionary of environment variables to be set during the macro’s execution. These are merged with any arguments provided at the call site.\n\n\n\n\n\n\non: (Required) A single event name or a list of event names to trigger the hook. Supported events are user_message, assistant_message, error, and tool_use_pre.\ndo: (Required) The command or inline script to run when the event occurs. If multi‑line, it must start with a shebang (e.g., #!/bin/bash). Event context is provided as environment variables. See the Hooks guide for details.\ninline: (Optional) Boolean. If true, the output of the hook is captured and injected into the conversation. Defaults to false.\nname: (Optional) A name for the hook. Used for merging and overriding hooks from different configuration sources.\nenv: (Optional) A dictionary of environment variables to be set when the hook runs.",
    "crumbs": [
      "Reference",
      "Configuration Keys"
    ]
  },
  {
    "objectID": "reference/02_configuration.html#top-level-keys",
    "href": "reference/02_configuration.html#top-level-keys",
    "title": "Reference: Configuration Keys",
    "section": "",
    "text": "interlocutor: A single object defining the primary LLM speaker.\ninterlocutors: A list of interlocutor objects for multiparty conversations.\nkits: A list of named tool kits you can reference from interlocutors.\nmacros: A list of macro definitions. See Macros.\nhooks: A list of hook definitions. See Hooks.",
    "crumbs": [
      "Reference",
      "Configuration Keys"
    ]
  },
  {
    "objectID": "reference/02_configuration.html#the-interlocutor-object",
    "href": "reference/02_configuration.html#the-interlocutor-object",
    "title": "Reference: Configuration Keys",
    "section": "",
    "text": "An interlocutor object defines a single LLM “personality” or configuration.\n\nname: (Required) The name of the speaker, used in the :::Name response blocks.\nprompt: (Required) The base system prompt that defines the LLM’s personality and instructions. The value can be a string, or it can be loaded from a file (file:./path.txt) or a command (exec:get-prompt). See External Prompts for details and examples.\nhooks: A list of hook definitions. See Hooks. These hooks fire only when this interlocutor is active.\nsandbox: A command string (e.g. /path/to/script.sh or wrapper.sh arg1) to wrap execution for all exec tools and local mcp_command tools used by this interlocutor, unless overridden by the tool’s own sandbox setting.\n\n\n\n\nprovider: The LLM provider to use. Supported values include anthropic, anthropic/bedrock, openai (Responses API), openai/chat (legacy Chat Completions), gemini, ollama, and openrouter.\nmodel: The specific model to use, e.g., claude-3-opus-20240229.\ntemperature: A number between 0 and 1 controlling the randomness of the output.\nmax_tokens: The maximum number of tokens to generate in a response.\nmax_tool_use: The maximum number of tool calls the LLM is allowed to make in a single turn.\nthinking_effort: Optional hint (used by the openai Responses provider, and by gemini-3-pro) about how much effort to spend reasoning. One of none, low, medium, or high.\nthinking_budget: Optional integer token budget for providers that support structured thinking phases (Anthropic, Anthropic/Bedrock, Gemini). Ignored by the openai and openai/chat providers.\n\n\n\nIf you don’t specify provider, Lectic picks a default based on your environment. It checks for known API keys in this order and uses the first one it finds:\n\nANTHROPIC_API_KEY\nGEMINI_API_KEY\nOPENAI_API_KEY\nOPENROUTER_API_KEY\n\nAWS credentials for Bedrock are not considered for auto‑selection. If you want Anthropic via Bedrock, set provider: anthropic/bedrock explicitly and ensure your AWS environment is configured.\nOpenAI has two provider options:\n\nopenai uses the Responses API. You’ll want this for native tools like search and code.\nopenai/chat uses the legacy Chat Completions API. You’ll need this for certain audio workflows that still require chat‑style models.\n\nFor a more detailed discussion of provider and model options, see Providers and Models.\n\n\n\n\n\ntools: A list of tool definitions that this interlocutor can use. The format of each object in the list depends on the tool type. See the Tools section for detailed configuration guides. All tools support a hooks array for tool_use_pre hooks scoped to that particular tool.\n\n\n\nThese keys are shared across multiple tool types:\n\nname: A custom name for the tool. If omitted, a default is derived from the tool type.\nusage: Instructions for the LLM on when and how to use the tool. Accepts a string, file:, or exec: source.\nhooks: A list of hooks scoped to this tool (typically tool_use_pre).\n\n\n\n\nRun commands and scripts.\n\nexec: (Required) The command or inline script to execute. Multi-line values must start with a shebang.\nschema: A map of parameter name → description. When present, the tool takes named string parameters (exposed as env vars). When absent, the tool takes a required arguments array of strings.\nsandbox: Command string to wrap execution. Arguments supported.\ntimeoutSeconds: Seconds to wait before aborting.\nenv: Environment variables to set for the subprocess.\n\n\n\n\nQuery SQLite databases.\n\nsqlite: (Required) Path to the SQLite database file.\nreadonly: Boolean. If true, opens the database in read-only mode.\nlimit: Maximum size of serialized response in bytes.\ndetails: Extra context for the model. Accepts string, file:, or exec:.\nextensions: A list of SQLite extension libraries to load.\n\n\n\n\nCall another interlocutor as a tool.\n\nagent: (Required) The name of the interlocutor to call.\nraw_output: Boolean. If true, includes raw tool call results in the output rather than sanitized text.\n\n\n\n\nConnect to Model Context Protocol servers.\n\nOne of: mcp_command, mcp_ws, mcp_sse, or mcp_shttp.\nargs: Arguments for mcp_command.\nenv: Environment variables for mcp_command.\nsandbox: Optional wrapper command to isolate mcp_command servers.\nroots: Optional list of root objects for file access (each with uri and optional name).\nexclude: Optional list of server tool names to hide.\n\n\n\n\n\nthink_about: (String) Creates a thinking/scratchpad tool with the given prompt.\nserve_on_port: (Integer) Creates a single-use web server on the given port.\nnative: One of search or code. Enables provider built-in tools.\nkit: Name of a tool kit to include.\n\nIf you add keys to an interlocutor object that are not listed in this section, Lectic will still parse the YAML, but the LSP marks those properties as unknown with a warning. This is usually a sign of a typo in a key name.",
    "crumbs": [
      "Reference",
      "Configuration Keys"
    ]
  },
  {
    "objectID": "reference/02_configuration.html#the-macro-object",
    "href": "reference/02_configuration.html#the-macro-object",
    "title": "Reference: Configuration Keys",
    "section": "",
    "text": "name: (Required) The name of the macro, used when invoking it with :name[] or :name[args].\nexpansion: (Optional) The content to be expanded. Can be a string, or loaded via file: or exec:. Equivalent to post if provided. See External Prompts for details about file: and exec:.\npre: (Optional) Expansion content for the pre-order phase.\npost: (Optional) Expansion content for the post-order phase.\nenv: (Optional) A dictionary of environment variables to be set during the macro’s execution. These are merged with any arguments provided at the call site.",
    "crumbs": [
      "Reference",
      "Configuration Keys"
    ]
  },
  {
    "objectID": "reference/02_configuration.html#the-hook-object",
    "href": "reference/02_configuration.html#the-hook-object",
    "title": "Reference: Configuration Keys",
    "section": "",
    "text": "on: (Required) A single event name or a list of event names to trigger the hook. Supported events are user_message, assistant_message, error, and tool_use_pre.\ndo: (Required) The command or inline script to run when the event occurs. If multi‑line, it must start with a shebang (e.g., #!/bin/bash). Event context is provided as environment variables. See the Hooks guide for details.\ninline: (Optional) Boolean. If true, the output of the hook is captured and injected into the conversation. Defaults to false.\nname: (Optional) A name for the hook. Used for merging and overriding hooks from different configuration sources.\nenv: (Optional) A dictionary of environment variables to be set when the hook runs.",
    "crumbs": [
      "Reference",
      "Configuration Keys"
    ]
  },
  {
    "objectID": "03_editor_integration.html",
    "href": "03_editor_integration.html",
    "title": "Editor Integration",
    "section": "",
    "text": "Lectic is designed to work with your editor, not replace it. The core workflow is simple: edit a file, run lectic, get a response appended. But with proper editor integration, you can do this with a single keypress and get features like completions, diagnostics, and folding.\n\n\nLectic includes a Language Server Protocol (LSP) server that provides:\n\nCompletions for directives (:cmd, :ask, :reset, etc.), interlocutor names, macros names, kits names, YAML header fields, tool types, and some other configuration parameters.\nDiagnostics for missing or invalid configuration, duplicate names, and broken file references\nHover information for directives, tool calls, and file links\nGo to definition for macros, kits, and interlocutors\nFolding for tool-call and inline-attachment blocks\nDocument outline showing conversation structure\nCode Actions for setting up a minimal YAML header, performing minor lints, and expanding macros (the macros don’t need to be expanded manually for the LLM to see the expansion results, but they can be if you want to preview or inline the expansion output).\n\nStart it with:\nlectic lsp\nThe server uses stdio transport and works with any LSP-capable editor.\n\n\n\nThe repository includes a full-featured plugin at extra/lectic.nvim.\n\n\nlazy.nvim:\n{\n  'gleachkr/lectic',\n  name = 'lectic.nvim',\n  config = function(plugin)\n    vim.opt.rtp:append(plugin.dir .. \"/extra/lectic.nvim\")\n  end\n}\nvim-plug:\nPlug 'gleachkr/lectic', { 'rtp': 'extra/lectic.nvim' }\n\n\n\n\nFiletype detection for .lec and .lectic files\nAsync submission — send conversations without blocking\nStreaming responses — watch the response appear in real-time\nVisual feedback — spinner while processing\nResponse highlighting — distinguish LLM blocks from your text\nTool call folding — collapsed by default, showing tool name\nSelection explanation — select text, ask for elaboration\n\n\n\n\n\n\n\nKey\nMode\nAction\n\n\n\n\n&lt;localleader&gt;l\nNormal\nSubmit conversation\n\n\n&lt;localleader&gt;c\nNormal\nCancel generation\n\n\n&lt;localleader&gt;e\nVisual\nExplain selection\n\n\n\nCustomize with:\nvim.g.lectic_key_submit = '&lt;Leader&gt;l'\nvim.g.lectic_key_cancel_submit = '&lt;Leader&gt;c'\nvim.g.lectic_key_explain = '&lt;Leader&gt;e'\n\n\n\nThe plugin handles filetype detection. For LSP features, add:\nvim.api.nvim_create_autocmd(\"FileType\", {\n  pattern = { \"lectic\", \"markdown.lectic\", \"lectic.markdown\" },\n  callback = function(args)\n    vim.lsp.start({\n      name = \"lectic\",\n      cmd = { \"lectic\", \"lsp\" },\n      root_dir = vim.fs.root(args.buf, { \".git\", \"lectic.yaml\" })\n                 or vim.fn.getcwd(),\n      single_file_support = true,\n    })\n  end,\n})\nFor LSP-based folding:\nvim.opt.foldexpr = 'vim.lsp.foldexpr()'\n\n\n\n\nAn extension is available at extra/lectic.vscode. VSIX files are distributed with releases.\n\n\n\nGenerate Next Response — stream LLM output into the editor\nExplain Selection — rewrite selected text with more detail\nBlock highlighting — visual distinction for response blocks\nTool call folding — collapse verbose tool output\nLSP integration — completions, diagnostics, and hovers\n\n\n\n\n\n\n\nKey\nAction\n\n\n\n\nAlt+L (Cmd+L on macOS)\nGenerate next response\n\n\nAlt+C (Cmd+C on macOS)\nConsolidate\n\n\nAlt+E (Cmd+E on macOS)\nExplain selection\n\n\n\n\n\n\n\nlectic.executablePath: Path to lectic if not in PATH\nlectic.blockBackgroundColor: Background color for ::: blocks\n\n\n\n\n\nAny editor that can run an external command on the current buffer works with Lectic. The basic pattern:\ncat file.lec | lectic &gt; file.lec\nOr use -i for in-place updates:\nlectic -i file.lec\n\n\nA minimal setup using shell-command-on-region:\n(defun lectic-submit ()\n  \"Send the buffer to lectic and replace with output.\"\n  (interactive)\n  (shell-command-on-region\n   (point-min) (point-max)\n   \"lectic\"\n   nil t))\n\n(add-to-list 'auto-mode-alist '(\"\\\\.lec\\\\'\" . markdown-mode))\n(add-hook 'markdown-mode-hook\n          (lambda ()\n            (when (string-match-p \"\\\\.lec\\\\'\" (buffer-file-name))\n              (local-set-key (kbd \"C-c C-l\") 'lectic-submit))))\nFor LSP support, use eglot or lsp-mode:\n;; With eglot\n(add-to-list 'eglot-server-programs\n             '((markdown-mode :language-id \"lectic\")\n               . (\"lectic\" \"lsp\")))\n\n\n\nAdd to languages.toml:\n[[language]]\nname = \"lectic\"\nscope = \"source.lectic\"\nfile-types = [\"lec\", \"lectic\"]\nlanguage-servers = [\"lectic-lsp\"]\ngrammar = \"markdown\"\n\n[language-server.lectic-lsp]\ncommand = \"lectic\"\nargs = [\"lsp\"]\n\n\n\nInstall the LSP package, then add to LSP settings:\n{\n  \"clients\": {\n    \"lectic\": {\n      \"command\": [\"lectic\", \"lsp\"],\n      \"selector\": \"text.html.markdown\",\n      \"file_patterns\": [\"*.lec\"]\n    }\n  }\n}\n\n\n\n\n\nWorking directory matters. When you run lectic -i file.lec, tools and file references resolve relative to the file’s directory. Most editor plugins handle this automatically.\nUse the LSP for header editing. Completions for model names, tool types, and interlocutor properties save time and catch typos.\nFold tool calls. Long tool outputs can obscure the conversation. Both the Neovim and VS Code plugins fold these by default.\nStream for long responses. The -s flag outputs just the new response, which editor plugins use to stream incrementally.",
    "crumbs": [
      "Editor Integration"
    ]
  },
  {
    "objectID": "03_editor_integration.html#the-lsp-server",
    "href": "03_editor_integration.html#the-lsp-server",
    "title": "Editor Integration",
    "section": "",
    "text": "Lectic includes a Language Server Protocol (LSP) server that provides:\n\nCompletions for directives (:cmd, :ask, :reset, etc.), interlocutor names, macros names, kits names, YAML header fields, tool types, and some other configuration parameters.\nDiagnostics for missing or invalid configuration, duplicate names, and broken file references\nHover information for directives, tool calls, and file links\nGo to definition for macros, kits, and interlocutors\nFolding for tool-call and inline-attachment blocks\nDocument outline showing conversation structure\nCode Actions for setting up a minimal YAML header, performing minor lints, and expanding macros (the macros don’t need to be expanded manually for the LLM to see the expansion results, but they can be if you want to preview or inline the expansion output).\n\nStart it with:\nlectic lsp\nThe server uses stdio transport and works with any LSP-capable editor.",
    "crumbs": [
      "Editor Integration"
    ]
  },
  {
    "objectID": "03_editor_integration.html#neovim",
    "href": "03_editor_integration.html#neovim",
    "title": "Editor Integration",
    "section": "",
    "text": "The repository includes a full-featured plugin at extra/lectic.nvim.\n\n\nlazy.nvim:\n{\n  'gleachkr/lectic',\n  name = 'lectic.nvim',\n  config = function(plugin)\n    vim.opt.rtp:append(plugin.dir .. \"/extra/lectic.nvim\")\n  end\n}\nvim-plug:\nPlug 'gleachkr/lectic', { 'rtp': 'extra/lectic.nvim' }\n\n\n\n\nFiletype detection for .lec and .lectic files\nAsync submission — send conversations without blocking\nStreaming responses — watch the response appear in real-time\nVisual feedback — spinner while processing\nResponse highlighting — distinguish LLM blocks from your text\nTool call folding — collapsed by default, showing tool name\nSelection explanation — select text, ask for elaboration\n\n\n\n\n\n\n\nKey\nMode\nAction\n\n\n\n\n&lt;localleader&gt;l\nNormal\nSubmit conversation\n\n\n&lt;localleader&gt;c\nNormal\nCancel generation\n\n\n&lt;localleader&gt;e\nVisual\nExplain selection\n\n\n\nCustomize with:\nvim.g.lectic_key_submit = '&lt;Leader&gt;l'\nvim.g.lectic_key_cancel_submit = '&lt;Leader&gt;c'\nvim.g.lectic_key_explain = '&lt;Leader&gt;e'\n\n\n\nThe plugin handles filetype detection. For LSP features, add:\nvim.api.nvim_create_autocmd(\"FileType\", {\n  pattern = { \"lectic\", \"markdown.lectic\", \"lectic.markdown\" },\n  callback = function(args)\n    vim.lsp.start({\n      name = \"lectic\",\n      cmd = { \"lectic\", \"lsp\" },\n      root_dir = vim.fs.root(args.buf, { \".git\", \"lectic.yaml\" })\n                 or vim.fn.getcwd(),\n      single_file_support = true,\n    })\n  end,\n})\nFor LSP-based folding:\nvim.opt.foldexpr = 'vim.lsp.foldexpr()'",
    "crumbs": [
      "Editor Integration"
    ]
  },
  {
    "objectID": "03_editor_integration.html#vs-code",
    "href": "03_editor_integration.html#vs-code",
    "title": "Editor Integration",
    "section": "",
    "text": "An extension is available at extra/lectic.vscode. VSIX files are distributed with releases.\n\n\n\nGenerate Next Response — stream LLM output into the editor\nExplain Selection — rewrite selected text with more detail\nBlock highlighting — visual distinction for response blocks\nTool call folding — collapse verbose tool output\nLSP integration — completions, diagnostics, and hovers\n\n\n\n\n\n\n\nKey\nAction\n\n\n\n\nAlt+L (Cmd+L on macOS)\nGenerate next response\n\n\nAlt+C (Cmd+C on macOS)\nConsolidate\n\n\nAlt+E (Cmd+E on macOS)\nExplain selection\n\n\n\n\n\n\n\nlectic.executablePath: Path to lectic if not in PATH\nlectic.blockBackgroundColor: Background color for ::: blocks",
    "crumbs": [
      "Editor Integration"
    ]
  },
  {
    "objectID": "03_editor_integration.html#other-editors",
    "href": "03_editor_integration.html#other-editors",
    "title": "Editor Integration",
    "section": "",
    "text": "Any editor that can run an external command on the current buffer works with Lectic. The basic pattern:\ncat file.lec | lectic &gt; file.lec\nOr use -i for in-place updates:\nlectic -i file.lec\n\n\nA minimal setup using shell-command-on-region:\n(defun lectic-submit ()\n  \"Send the buffer to lectic and replace with output.\"\n  (interactive)\n  (shell-command-on-region\n   (point-min) (point-max)\n   \"lectic\"\n   nil t))\n\n(add-to-list 'auto-mode-alist '(\"\\\\.lec\\\\'\" . markdown-mode))\n(add-hook 'markdown-mode-hook\n          (lambda ()\n            (when (string-match-p \"\\\\.lec\\\\'\" (buffer-file-name))\n              (local-set-key (kbd \"C-c C-l\") 'lectic-submit))))\nFor LSP support, use eglot or lsp-mode:\n;; With eglot\n(add-to-list 'eglot-server-programs\n             '((markdown-mode :language-id \"lectic\")\n               . (\"lectic\" \"lsp\")))\n\n\n\nAdd to languages.toml:\n[[language]]\nname = \"lectic\"\nscope = \"source.lectic\"\nfile-types = [\"lec\", \"lectic\"]\nlanguage-servers = [\"lectic-lsp\"]\ngrammar = \"markdown\"\n\n[language-server.lectic-lsp]\ncommand = \"lectic\"\nargs = [\"lsp\"]\n\n\n\nInstall the LSP package, then add to LSP settings:\n{\n  \"clients\": {\n    \"lectic\": {\n      \"command\": [\"lectic\", \"lsp\"],\n      \"selector\": \"text.html.markdown\",\n      \"file_patterns\": [\"*.lec\"]\n    }\n  }\n}",
    "crumbs": [
      "Editor Integration"
    ]
  },
  {
    "objectID": "03_editor_integration.html#tips",
    "href": "03_editor_integration.html#tips",
    "title": "Editor Integration",
    "section": "",
    "text": "Working directory matters. When you run lectic -i file.lec, tools and file references resolve relative to the file’s directory. Most editor plugins handle this automatically.\nUse the LSP for header editing. Completions for model names, tool types, and interlocutor properties save time and catch typos.\nFold tool calls. Long tool outputs can obscure the conversation. Both the Neovim and VS Code plugins fold these by default.\nStream for long responses. The -s flag outputs just the new response, which editor plugins use to stream incrementally.",
    "crumbs": [
      "Editor Integration"
    ]
  },
  {
    "objectID": "tools/03_sqlite.html",
    "href": "tools/03_sqlite.html",
    "title": "Tools: SQLite Query",
    "section": "",
    "text": "The sqlite tool gives your LLM the ability to query SQLite databases directly. This is a powerful way to provide access to structured data, allowing the LLM to perform data analysis, answer questions from a knowledge base, or check the state of an application.\n\n\nThe snippets below show only the tool definition. They assume you have an interlocutor with a valid prompt and model configuration. See Getting Started for a full header example.\nTo configure the tool, you must provide the path to the SQLite database file. The database schema is automatically introspected and provided to the LLM, so it knows what tables and columns are available.\ntools:\n  - sqlite: ./products.db\n    name: db_query\n    limit: 10000\n    readonly: true\n    details: &gt;\n      Contains the full product catalog and inventory levels. Use this to\n      answer questions about what is in stock.\n    extensions:\n      - ./lib/vector0\n      - ./lib/math\nThe path can include environment variables (for example, $DATA_DIR/main.db), which Lectic expands.\n\n\n\nsqlite: (Required) Path to the SQLite database file.\nname: A custom tool name.\nreadonly: To set the database as read only.\nlimit: Maximum size of the serialized response in bytes. Large results raise an error instead of flooding the model.\ndetails: Extra high‑level context for the model. String, file:, or exec: are accepted. See External Prompts.\nextensions: A list of SQLite extension libraries to load before queries.\n\n\n\n\nConfiguration:\ntools:\n  - sqlite: ./chinook.db\n    name: chinook\nConversation:\nWho are the top 5 artists by number of tracks?\n\n:::Assistant\n\nI will query the database to find out.\n\n&lt;tool-call with=\"chinook\"&gt;\n&lt;arguments&gt;\n&lt;query&gt;\n┆SELECT\n┆ar.Name,\n┆COUNT(t.TrackId) AS TrackCount\n┆FROM Artists ar\n┆JOIN Albums al ON ar.ArtistId = al.ArtistId\n┆JOIN Tracks t ON al.AlbumId = t.AlbumId\n┆GROUP BY ar.Name\n┆ORDER BY TrackCount DESC\n┆LIMIT 5;\n&lt;/query&gt;\n&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆- Name: Iron Maiden\n┆TrackCount: 213\n┆- Name: Led Zeppelin\n┆TrackCount: 114\n┆- Name: Metallica\n┆TrackCount: 112\n┆- Name: U2\n┆TrackCount: 110\n┆- Name: Deep Purple\n┆TrackCount: 92\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\nBased on the data, the top artists by track count are Iron Maiden, Led\nZeppelin, Metallica, U2, and Deep Purple.\n\n:::\n\n\n\n\nWrites are allowed by default. Each tool call runs inside a transaction and is atomic. If any statement in the call fails, Lectic rolls back the entire call, so the database is unchanged.\n\n\n\nThe limit parameter caps the size of the serialized YAML that Lectic returns. If a result exceeds the cap, the tool raises an error. Tighten your query (for example, add LIMIT or select fewer columns) to stay under the cap.\n\n\n\nYou can load extensions by path before queries run. On macOS, note that the system SQLite build may restrict loading extensions. Consult the Bun SQLite extension documentation if you hit issues.",
    "crumbs": [
      "Tools",
      "SQLite"
    ]
  },
  {
    "objectID": "tools/03_sqlite.html#configuration",
    "href": "tools/03_sqlite.html#configuration",
    "title": "Tools: SQLite Query",
    "section": "",
    "text": "The snippets below show only the tool definition. They assume you have an interlocutor with a valid prompt and model configuration. See Getting Started for a full header example.\nTo configure the tool, you must provide the path to the SQLite database file. The database schema is automatically introspected and provided to the LLM, so it knows what tables and columns are available.\ntools:\n  - sqlite: ./products.db\n    name: db_query\n    limit: 10000\n    readonly: true\n    details: &gt;\n      Contains the full product catalog and inventory levels. Use this to\n      answer questions about what is in stock.\n    extensions:\n      - ./lib/vector0\n      - ./lib/math\nThe path can include environment variables (for example, $DATA_DIR/main.db), which Lectic expands.\n\n\n\nsqlite: (Required) Path to the SQLite database file.\nname: A custom tool name.\nreadonly: To set the database as read only.\nlimit: Maximum size of the serialized response in bytes. Large results raise an error instead of flooding the model.\ndetails: Extra high‑level context for the model. String, file:, or exec: are accepted. See External Prompts.\nextensions: A list of SQLite extension libraries to load before queries.\n\n\n\n\nConfiguration:\ntools:\n  - sqlite: ./chinook.db\n    name: chinook\nConversation:\nWho are the top 5 artists by number of tracks?\n\n:::Assistant\n\nI will query the database to find out.\n\n&lt;tool-call with=\"chinook\"&gt;\n&lt;arguments&gt;\n&lt;query&gt;\n┆SELECT\n┆ar.Name,\n┆COUNT(t.TrackId) AS TrackCount\n┆FROM Artists ar\n┆JOIN Albums al ON ar.ArtistId = al.ArtistId\n┆JOIN Tracks t ON al.AlbumId = t.AlbumId\n┆GROUP BY ar.Name\n┆ORDER BY TrackCount DESC\n┆LIMIT 5;\n&lt;/query&gt;\n&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆- Name: Iron Maiden\n┆TrackCount: 213\n┆- Name: Led Zeppelin\n┆TrackCount: 114\n┆- Name: Metallica\n┆TrackCount: 112\n┆- Name: U2\n┆TrackCount: 110\n┆- Name: Deep Purple\n┆TrackCount: 92\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\nBased on the data, the top artists by track count are Iron Maiden, Led\nZeppelin, Metallica, U2, and Deep Purple.\n\n:::",
    "crumbs": [
      "Tools",
      "SQLite"
    ]
  },
  {
    "objectID": "tools/03_sqlite.html#writes-and-transactions",
    "href": "tools/03_sqlite.html#writes-and-transactions",
    "title": "Tools: SQLite Query",
    "section": "",
    "text": "Writes are allowed by default. Each tool call runs inside a transaction and is atomic. If any statement in the call fails, Lectic rolls back the entire call, so the database is unchanged.",
    "crumbs": [
      "Tools",
      "SQLite"
    ]
  },
  {
    "objectID": "tools/03_sqlite.html#limits-and-large-results",
    "href": "tools/03_sqlite.html#limits-and-large-results",
    "title": "Tools: SQLite Query",
    "section": "",
    "text": "The limit parameter caps the size of the serialized YAML that Lectic returns. If a result exceeds the cap, the tool raises an error. Tighten your query (for example, add LIMIT or select fewer columns) to stay under the cap.",
    "crumbs": [
      "Tools",
      "SQLite"
    ]
  },
  {
    "objectID": "tools/03_sqlite.html#extensions",
    "href": "tools/03_sqlite.html#extensions",
    "title": "Tools: SQLite Query",
    "section": "",
    "text": "You can load extensions by path before queries run. On macOS, note that the system SQLite build may restrict loading extensions. Consult the Bun SQLite extension documentation if you hit issues.",
    "crumbs": [
      "Tools",
      "SQLite"
    ]
  },
  {
    "objectID": "tools/01_overview.html",
    "href": "tools/01_overview.html",
    "title": "Tools Overview",
    "section": "",
    "text": "Tools let your LLM do things. Instead of stopping at text, it can run a command, query a database, call another agent, or reach out to a service.\n\n\n\n\n\nTool\nPurpose\nMinimal Config\n\n\n\n\nexec\nRun commands and scripts\nexec: date\n\n\nsqlite\nQuery SQLite databases\nsqlite: ./data.db\n\n\nmcp\nConnect to MCP servers\nmcp_command: npx ...\n\n\nagent\nCall another interlocutor\nagent: OtherName\n\n\nthink\nPrivate reasoning scratchpad\nthink_about: the problem\n\n\nserve\nServe HTML to browser\nserve_on_port: 8080\n\n\nnative\nProvider built-ins (search, code)\nnative: search\n\n\n\n\n\n\nIn Lectic, you configure tools for each interlocutor in the YAML frontmatter. A tool call follows a four-step process:\n\nUser Prompt: You ask something that requires a tool.\nLLM Tool Call: The LLM outputs a block indicating which tool to use.\nLectic Executes: Lectic runs the tool and captures output.\nLLM Response: The tool output goes back to the LLM, which answers.\n\n\n\n\nLectic uses XML blocks for tool calls:\n&lt;tool-call with=\"tool_name\"&gt;\n&lt;arguments&gt;\n  &lt;!-- one element per parameter --&gt;\n&lt;/arguments&gt;\n&lt;results&gt;\n  &lt;!-- filled by Lectic after execution --&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\nYou’ll see these in assistant blocks. Lectic writes the block when the model requests a tool, then appends results after running it.\n\n\nConfiguration:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  tools:\n    - exec: date\n      name: get_date\n---\n\nWhat's the date today?\nResult:\n:::Assistant\n\n&lt;tool-call with=\"get_date\"&gt;\n&lt;arguments&gt;&lt;argv&gt;[ ]&lt;/argv&gt;&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆&lt;stdout&gt;Fri Mar 15 14:35:18 PDT 2024&lt;/stdout&gt;\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\nToday is March 15th, 2024.\n\n:::\n\n\n\n\nWhen an LLM uses multiple tools in one turn, Lectic runs them concurrently. This speeds up tasks that gather information from several sources.\n\n\n\nReuse tool sets across interlocutors by defining named kits:\nkits:\n  - name: typescript_tools\n    tools:\n      - exec: tsc --noEmit\n        name: typecheck\n      - exec: eslint\n        name: lint\n\ninterlocutor:\n  name: Assistant\n  prompt: You help with TypeScript.\n  tools:\n    - kit: typescript_tools\n    - exec: cat\n      name: read_file\n\n\n\nThe tool_use_pre hook fires after parameters are collected but before execution. If the hook exits non-zero, the call is blocked:\ninterlocutor:\n  tools:\n    - exec: rm\n      name: delete\n      hooks:\n        - on: tool_use_pre\n          do: ~/.config/lectic/confirm.sh\nSee Hooks for details.\n\n\n\nEach tool type has its own detailed guide:\n\nExec: Shell commands and scripts. The most versatile tool — anything you can run from the command line, your LLM can run too.\nSQLite: Direct database queries. Schema is auto-introspected and provided to the LLM.\nMCP: Model Context Protocol servers. Connect to a growing ecosystem of pre-built tools and services.\nAgent: Multi-LLM workflows. One interlocutor can delegate to another, enabling specialized agents.\nOther Tools: The think tool for reasoning, serve for rendering HTML, and native for provider built-ins like web search.\n\n\n\n\n\n\n\nNote\n\n\n\nNative tools (native: search, native: code) do not support hooks.",
    "crumbs": [
      "Tools",
      "Overview"
    ]
  },
  {
    "objectID": "tools/01_overview.html#quick-reference",
    "href": "tools/01_overview.html#quick-reference",
    "title": "Tools Overview",
    "section": "",
    "text": "Tool\nPurpose\nMinimal Config\n\n\n\n\nexec\nRun commands and scripts\nexec: date\n\n\nsqlite\nQuery SQLite databases\nsqlite: ./data.db\n\n\nmcp\nConnect to MCP servers\nmcp_command: npx ...\n\n\nagent\nCall another interlocutor\nagent: OtherName\n\n\nthink\nPrivate reasoning scratchpad\nthink_about: the problem\n\n\nserve\nServe HTML to browser\nserve_on_port: 8080\n\n\nnative\nProvider built-ins (search, code)\nnative: search",
    "crumbs": [
      "Tools",
      "Overview"
    ]
  },
  {
    "objectID": "tools/01_overview.html#how-tool-calls-work",
    "href": "tools/01_overview.html#how-tool-calls-work",
    "title": "Tools Overview",
    "section": "",
    "text": "In Lectic, you configure tools for each interlocutor in the YAML frontmatter. A tool call follows a four-step process:\n\nUser Prompt: You ask something that requires a tool.\nLLM Tool Call: The LLM outputs a block indicating which tool to use.\nLectic Executes: Lectic runs the tool and captures output.\nLLM Response: The tool output goes back to the LLM, which answers.",
    "crumbs": [
      "Tools",
      "Overview"
    ]
  },
  {
    "objectID": "tools/01_overview.html#tool-call-syntax",
    "href": "tools/01_overview.html#tool-call-syntax",
    "title": "Tools Overview",
    "section": "",
    "text": "Lectic uses XML blocks for tool calls:\n&lt;tool-call with=\"tool_name\"&gt;\n&lt;arguments&gt;\n  &lt;!-- one element per parameter --&gt;\n&lt;/arguments&gt;\n&lt;results&gt;\n  &lt;!-- filled by Lectic after execution --&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\nYou’ll see these in assistant blocks. Lectic writes the block when the model requests a tool, then appends results after running it.\n\n\nConfiguration:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  tools:\n    - exec: date\n      name: get_date\n---\n\nWhat's the date today?\nResult:\n:::Assistant\n\n&lt;tool-call with=\"get_date\"&gt;\n&lt;arguments&gt;&lt;argv&gt;[ ]&lt;/argv&gt;&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆&lt;stdout&gt;Fri Mar 15 14:35:18 PDT 2024&lt;/stdout&gt;\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\nToday is March 15th, 2024.\n\n:::",
    "crumbs": [
      "Tools",
      "Overview"
    ]
  },
  {
    "objectID": "tools/01_overview.html#parallel-execution",
    "href": "tools/01_overview.html#parallel-execution",
    "title": "Tools Overview",
    "section": "",
    "text": "When an LLM uses multiple tools in one turn, Lectic runs them concurrently. This speeds up tasks that gather information from several sources.",
    "crumbs": [
      "Tools",
      "Overview"
    ]
  },
  {
    "objectID": "tools/01_overview.html#tool-kits",
    "href": "tools/01_overview.html#tool-kits",
    "title": "Tools Overview",
    "section": "",
    "text": "Reuse tool sets across interlocutors by defining named kits:\nkits:\n  - name: typescript_tools\n    tools:\n      - exec: tsc --noEmit\n        name: typecheck\n      - exec: eslint\n        name: lint\n\ninterlocutor:\n  name: Assistant\n  prompt: You help with TypeScript.\n  tools:\n    - kit: typescript_tools\n    - exec: cat\n      name: read_file",
    "crumbs": [
      "Tools",
      "Overview"
    ]
  },
  {
    "objectID": "tools/01_overview.html#hooks",
    "href": "tools/01_overview.html#hooks",
    "title": "Tools Overview",
    "section": "",
    "text": "The tool_use_pre hook fires after parameters are collected but before execution. If the hook exits non-zero, the call is blocked:\ninterlocutor:\n  tools:\n    - exec: rm\n      name: delete\n      hooks:\n        - on: tool_use_pre\n          do: ~/.config/lectic/confirm.sh\nSee Hooks for details.",
    "crumbs": [
      "Tools",
      "Overview"
    ]
  },
  {
    "objectID": "tools/01_overview.html#tool-guides",
    "href": "tools/01_overview.html#tool-guides",
    "title": "Tools Overview",
    "section": "",
    "text": "Each tool type has its own detailed guide:\n\nExec: Shell commands and scripts. The most versatile tool — anything you can run from the command line, your LLM can run too.\nSQLite: Direct database queries. Schema is auto-introspected and provided to the LLM.\nMCP: Model Context Protocol servers. Connect to a growing ecosystem of pre-built tools and services.\nAgent: Multi-LLM workflows. One interlocutor can delegate to another, enabling specialized agents.\nOther Tools: The think tool for reasoning, serve for rendering HTML, and native for provider built-ins like web search.\n\n\n\n\n\n\n\nNote\n\n\n\nNative tools (native: search, native: code) do not support hooks.",
    "crumbs": [
      "Tools",
      "Overview"
    ]
  },
  {
    "objectID": "tools/04_mcp.html",
    "href": "tools/04_mcp.html",
    "title": "Tools: Model Context Protocol (mcp)",
    "section": "",
    "text": "Lectic can act as a client for servers that implement the Model Context Protocol (MCP). This allows you to connect your LLM to a vast and growing ecosystem of pre-built tools and services.\nYou can find lists of available servers here: - Official MCP Server List - Awesome MCP Servers\n\n\nNote: The snippets below show only the tool definition. They assume you have an interlocutor with a valid prompt and model configuration. See Getting Started for a full header example.\nYou can connect to an MCP server in three ways: by running a local server as a command, or by connecting to a remote server over WebSockets or SSE.\n\n\nThis is the most common way to run an MCP server. You provide the command to start the server, and Lectic manages its lifecycle.\ntools:\n  - name: brave\n    mcp_command: npx\n    args:\n      - \"-y\"\n      - \"@modelcontextprotocol/server-brave-search\"\n    env:\n      BRAVE_API_KEY: \"your_key_here\"\n    roots:\n      - /home/user/research-docs/\nLocal MCP servers are started on demand for the active interlocutor and managed by Lectic for the duration of the session.\n\n\n\nYou can also connect to running MCP servers.\n\nmcp_ws: The URL for a remote server using a WebSocket connection.\nmcp_sse: The URL for a remote server using Server-Sent Events.\nmcp_shttp: The URL for a remote server using Streamable HTTP.\n\nFor example:\ntools:\n  - name: documentation_search \n    mcp_shttp: https://mcp.context7.com/mcp\n\n\n\nIf you give an MCP tool a name (e.g., name: brave), you can access any resources it provides using a special content reference syntax. The scheme is the server’s name plus the resource type.\nFor example, to access a repo resource from a server named github: [README](github+repo://gleachkr/Lectic/contents/README.md)\nThe LLM is also given a tool to list the available resources from the server.\n\n\n\nYou can hide specific tools that a server exposes by listing their names under exclude.\ntools:\n  - name: github\n    mcp_ws: wss://example.org/mcp\n    exclude:\n      - dangerous_tool\n      - low_value_tool\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhile powerful, the MCP protocol carries significant security risks. Treat MCP integration as a high-trust capability. Never connect to untrusted servers; a malicious server could exfiltrate data or perform unwanted actions. Lectic’s safety mechanisms reduce mistakes from a well‑behaved LLM, not attacks from a hostile server.\n\n\n\n\nJust like with the exec tool, you can use the tool_use_pre hook to implement confirmation dialogs or logic. See Hooks for examples.\n\n\n\nFor local mcp_command tools, you can specify a sandbox command. This command will be used to launch the MCP server process in a controlled and isolated environment, limiting its access to your system. Arguments are supported (e.g. sandbox: wrapper.sh --strict).\nSee the documentation for the Exec Tool for more details on how sandboxing scripts work, and the Custom Sandboxing recipe for examples.\nYou can also set a default sandbox on the interlocutor object. If set, it applies to all local MCP tools that don’t specify their own.",
    "crumbs": [
      "Tools",
      "MCP"
    ]
  },
  {
    "objectID": "tools/04_mcp.html#configuration",
    "href": "tools/04_mcp.html#configuration",
    "title": "Tools: Model Context Protocol (mcp)",
    "section": "",
    "text": "Note: The snippets below show only the tool definition. They assume you have an interlocutor with a valid prompt and model configuration. See Getting Started for a full header example.\nYou can connect to an MCP server in three ways: by running a local server as a command, or by connecting to a remote server over WebSockets or SSE.\n\n\nThis is the most common way to run an MCP server. You provide the command to start the server, and Lectic manages its lifecycle.\ntools:\n  - name: brave\n    mcp_command: npx\n    args:\n      - \"-y\"\n      - \"@modelcontextprotocol/server-brave-search\"\n    env:\n      BRAVE_API_KEY: \"your_key_here\"\n    roots:\n      - /home/user/research-docs/\nLocal MCP servers are started on demand for the active interlocutor and managed by Lectic for the duration of the session.\n\n\n\nYou can also connect to running MCP servers.\n\nmcp_ws: The URL for a remote server using a WebSocket connection.\nmcp_sse: The URL for a remote server using Server-Sent Events.\nmcp_shttp: The URL for a remote server using Streamable HTTP.\n\nFor example:\ntools:\n  - name: documentation_search \n    mcp_shttp: https://mcp.context7.com/mcp\n\n\n\nIf you give an MCP tool a name (e.g., name: brave), you can access any resources it provides using a special content reference syntax. The scheme is the server’s name plus the resource type.\nFor example, to access a repo resource from a server named github: [README](github+repo://gleachkr/Lectic/contents/README.md)\nThe LLM is also given a tool to list the available resources from the server.\n\n\n\nYou can hide specific tools that a server exposes by listing their names under exclude.\ntools:\n  - name: github\n    mcp_ws: wss://example.org/mcp\n    exclude:\n      - dangerous_tool\n      - low_value_tool",
    "crumbs": [
      "Tools",
      "MCP"
    ]
  },
  {
    "objectID": "tools/04_mcp.html#safety-and-trust",
    "href": "tools/04_mcp.html#safety-and-trust",
    "title": "Tools: Model Context Protocol (mcp)",
    "section": "",
    "text": "Warning\n\n\n\nWhile powerful, the MCP protocol carries significant security risks. Treat MCP integration as a high-trust capability. Never connect to untrusted servers; a malicious server could exfiltrate data or perform unwanted actions. Lectic’s safety mechanisms reduce mistakes from a well‑behaved LLM, not attacks from a hostile server.\n\n\n\n\nJust like with the exec tool, you can use the tool_use_pre hook to implement confirmation dialogs or logic. See Hooks for examples.\n\n\n\nFor local mcp_command tools, you can specify a sandbox command. This command will be used to launch the MCP server process in a controlled and isolated environment, limiting its access to your system. Arguments are supported (e.g. sandbox: wrapper.sh --strict).\nSee the documentation for the Exec Tool for more details on how sandboxing scripts work, and the Custom Sandboxing recipe for examples.\nYou can also set a default sandbox on the interlocutor object. If set, it applies to all local MCP tools that don’t specify their own.",
    "crumbs": [
      "Tools",
      "MCP"
    ]
  },
  {
    "objectID": "automation/02_hooks.html",
    "href": "automation/02_hooks.html",
    "title": "Automation: Hooks",
    "section": "",
    "text": "Hooks are a powerful automation feature that let you run custom commands and scripts in response to events in Lectic’s lifecycle. Use them for logging, notifications, post‑processing, or integrating with other tools and workflows.\nHooks are defined in your YAML configuration under the hooks key, per-tool in the hooks key of a tool specification, or per-interlocutor in the hooks key of an interlocutor specification.\n\n\nA hook has five possible fields:\n\non: (Required) A single event name or a list of event names to listen for.\ndo: (Required) The command or inline script to run when the event fires.\ninline: (Optional) A boolean. If true, the standard output of the command is captured and injected into the conversation. Defaults to false. Only applicable to assistant_message and user_message.\nname: (Optional) A string name for the hook. If multiple hooks have the same name (e.g., one in your global config and one in a project config), the one defined later (or with higher precedence) overrides the earlier one. This allows you to replace default hooks with custom behavior.\nenv: (Optional) A map of environment variables to inject into the hook’s execution environment.\n\nhooks:\n  - name: logger\n    on: [assistant_message, user_message]\n    env:\n      LOG_FILE: /tmp/lectic.log\n    do: ./log-activity.sh\nIf do contains multiple lines, it is treated as a script and must begin with a shebang (e.g., #!/bin/bash). If it is a single line, it is treated as a command. Commands are executed directly (not through a shell), so shell features like command substitution will not work.\nHook commands run synchronously. By default, their stdout, stderr, and exit status are ignored by Lectic. However, if you set inline: true, the standard output is captured and added to the conversation.\n\nFor user_message events, the output is injected as context for the LLM before it generates a response. It also appears at the top of the assistant’s response block.\nFor assistant_message events, the output is appended to the end of the assistant’s response block. This will trigger another reply from the assistant, so be careful to only fire an inline hook when you want the assistant to generate more content.\n\nIn the .lec file, inline hook output is stored as an XML &lt;inline-attachment kind=\"hook\"&gt; block. The &lt;command&gt; element records the hook’s do field so you can see what produced the output.\n&lt;inline-attachment kind=\"hook\"&gt;\n&lt;command&gt;./my-hook.sh&lt;/command&gt;\n&lt;content type=\"text/plain\"&gt;\n┆System check complete.\n&lt;/content&gt;\n&lt;/inline-attachment&gt;\n\n\n\nLectic emits three hook events. When an event fires, the hook process receives its context as environment variables. No positional arguments are passed. However, the hook may receive content via standard input.\n\nuser_message\n\nEnvironment:\n\nUSER_MESSAGE: The text of the most recent user message.\nStandard Lectic variables like LECTIC_FILE, LECTIC_CONFIG, LECTIC_DATA, LECTIC_CACHE, LECTIC_STATE, and LECTIC_TEMP are also set when available.\n\nWhen: Just before the request is sent to the LLM provider.\n\nassistant_message\n\nStandard Input: The raw markdown text of the conversation body up to this point.\nEnvironment:\n\nASSISTANT_MESSAGE: The full text of the assistant’s response that was just produced.\nLECTIC_INTERLOCUTOR: The name of the interlocutor who spoke.\nLECTIC_MODEL: The model of the interlocutor who spoke.\nTOOL_USE_DONE: Set to 1 when the assistant has finished using tools and is ready to conclude. Not set if there are pending tool calls. This lets inline hooks decide whether to inject follow-up content only when all work is complete.\nTOKEN_USAGE_INPUT: Count of total input tokens used for this turn.\nTOKEN_USAGE_CACHED: Count of cached input tokens used for this turn.\nTOKEN_USAGE_OUTPUT: Count of output tokens used for this turn.\nTOKEN_USAGE_TOTAL: Total tokens used for this turn.\nLOOP_COUNT: How many times the tool calling loop has run (0-indexed).\nFINAL_PASS_COUNT: How many times the assistant has finished work but was kept alive by an inline hook.\nStandard Lectic variables as above.\n\nWhen: Immediately after the assistant’s message is streamed.\n\ntool_use_pre\n\nEnvironment:\n\nTOOL_NAME: The name of the tool being called.\nTOOL_ARGS: A JSON string containing the tool arguments.\nStandard Lectic variables as above.\n\nWhen: After tool parameters are collected but before execution.\nBehavior: If the hook exits with a non-zero status code, the tool call is blocked, and the LLM receives a “permission denied” error.\n\nerror\n\nEnvironment:\n\nERROR_MESSAGE: A descriptive error message.\nStandard Lectic variables as above.\n\nWhen: Whenever an uncaught error is encountered.\n\n\n\n\n\nHooks can pass metadata back to Lectic by including headers at the very beginning of their output. Headers follow the format LECTIC:KEY:VALUE or simply LECTIC:KEY (where the value defaults to “true”) and must appear before any other content. The headers are stripped from the visible output and stored as attributes on the inline attachment block.\n#!/usr/bin/env bash\necho \"LECTIC:final\"\necho \"\"\necho \"System check complete. One issue found.\"\nThis would be recorded roughly like this:\n&lt;inline-attachment kind=\"hook\" final=\"true\"&gt;\n&lt;command&gt;./my-hook.sh&lt;/command&gt;\n&lt;content type=\"text/plain\"&gt;\n┆System check complete. One issue found.\n&lt;/content&gt;\n&lt;/inline-attachment&gt;\nTwo headers affect control flow:\n\nfinal: When an inline hook generates output, Lectic normally continues the tool calling loop so that the assistant can see and respond to the new information. If the final header is present, Lectic prevents this extra pass, allowing the conversation turn to end immediately (unless the assistant explicitly called a tool).\nreset: When present, this header clears the conversation context up to the current message. The accumulated history sent to the provider is discarded, and the context effectively restarts from the message containing the hook output. This is useful for implementing custom context compaction or archival strategies when token limits are reached.\n\n\n\n\nThis example uses tool_use_pre to require confirmation before any tool execution. It uses zenity to show a dialog box with the tool name and arguments.\nhooks:\n  - on: tool_use_pre\n    do: |\n      #!/usr/bin/env bash\n      # Display a confirmation dialog\n      zenity --question \\\n             --title=\"Allow Tool Use?\" \\\n             --text=\"Tool: $TOOL_NAME\\nArgs: $TOOL_ARGS\"\n      # Zenity exits with 0 for Yes/OK and 1 for No/Cancel\n      exit $?\nThis example persists every user and assistant message to an SQLite database located in your Lectic data directory. You can later query this for personal memory, project history, or analytics.\nConfiguration:\nhooks:\n  - on: [user_message, assistant_message]\n    do: |\n      #!/usr/bin/env bash\n      set -euo pipefail\n      DB_ROOT=\"${LECTIC_DATA:-$HOME/.local/share/lectic}\"\n      DB_PATH=\"${DB_ROOT}/memory.sqlite3\"\n      mkdir -p \"${DB_ROOT}\"\n\n      # Determine role and text from available variables\n      if [[ -n \"${ASSISTANT_MESSAGE:-}\" ]]; then\n        ROLE=\"assistant\"\n        TEXT=\"$ASSISTANT_MESSAGE\"\n      else\n        ROLE=\"user\"\n        TEXT=\"${USER_MESSAGE:-}\"\n      fi\n\n      # Basic sanitizer for single quotes for SQL literal\n      esc_sq() { printf %s \"$1\" | sed \"s/'/''/g\"; }\n\n      TS=$(date -Is)\n      FILE_PATH=\"${LECTIC_FILE:-}\"\n      NAME=\"${LECTIC_INTERLOCUTOR:-}\"\n\n      sqlite3 \"$DB_PATH\" &lt;&lt;SQL\n      CREATE TABLE IF NOT EXISTS memory (\n        id INTEGER PRIMARY KEY,\n        ts TEXT NOT NULL,\n        role TEXT NOT NULL,\n        interlocutor TEXT,\n        file TEXT,\n        text TEXT NOT NULL\n      );\n      INSERT INTO memory(ts, role, interlocutor, file, text)\n      VALUES ('${TS}', '${ROLE}', '$(esc_sq \"$NAME\")',\n              '$(esc_sq \"$FILE_PATH\")', '$(esc_sq \"$TEXT\")');\n      SQL\nNotes:\n\nRequires the sqlite3 command-line tool to be installed and on your PATH.\nThe hook inspects which variable is set to decide whether the event was a user or assistant message.\nLECTIC_FILE is populated when using -f/-i and may be empty when streaming from stdin.\nAdjust the table schema to suit your use case.\n\n\n\n\nThis example automatically runs date before every user message and injects the output into the context. This allows the LLM to always know the date and time without you needing to run :cmd[date]\nhooks:\n  - on: user_message\n    inline: true\n    do: \n      #!/usr/bin/env bash\n      echo \"&lt;date-and-time&gt;\"\n      date\n      echo \"&lt;/date-and-time&gt;\"\n\n\n\nThis example sends a desktop notification when the assistant finishes a tool-use workflow. The hook checks TOOL_USE_DONE so you only get notified once the work is actually done, not after each intermediate step.\nhooks:\n  - on: assistant_message\n    do: |\n      #!/usr/bin/env bash\n      if [[ \"${TOOL_USE_DONE:-}\" == \"1\" ]]; then\n        notify-send \"Lectic\" \"Assistant finished working\"\n      fi\nThis is especially useful for long-running agentic tasks where you want to step away and be alerted when the assistant is done.\n\n\n\nWhen using the lectic.nvim plugin, the NVIM environment variable is set to Neovim’s RPC server address. This allows hooks to communicate directly with your editor—sending notifications, opening windows, or triggering any Neovim Lua API.\nThis example sends a notification to Neovim when the assistant finishes working:\nhooks:\n  - on: assistant_message\n    do: |\n      #!/usr/bin/env bash\n      if [[ \"${TOOL_USE_DONE:-}\" == \"1\" && -n \"${NVIM:-}\" ]]; then\n        nvim --server \"$NVIM\" --remote-expr \\\n          \"luaeval('vim.notify(\\\"Lectic: Assistant finished working\\\", vim.log.levels.INFO)')\"\n      fi\nThe pattern nvim --server \"$NVIM\" --remote-expr \"luaeval('...')\" lets you execute arbitrary Lua in the running Neovim instance. Some ideas:\n\nPlay a sound: vim.fn.system('paplay /usr/share/sounds/...')\nFlash the screen: vim.cmd('sleep 100m | redraw!')\nUpdate a status line variable\nTrigger a custom autocommand: vim.api.nvim_exec_autocmds('User', {pattern = 'LecticDone'})\n\n\n\n\nThis example checks the total token usage and, if it exceeds a limit, resets the conversation context. It also uses the final header to stop the assistant from responding to the reset message immediately.\nhooks:\n  - on: assistant_message\n    inline: true\n    do: |\n      #!/usr/bin/env bash\n      LIMIT=100000\n      TOTAL=\"${TOKEN_USAGE_TOTAL:-0}\"\n      \n      if [ \"$TOTAL\" -gt \"$LIMIT\" ]; then\n        echo \"LECTIC:reset\"\n        echo \"LECTIC:final\"\n        echo \"\"\n        echo \"**Context cleared (usage: $TOTAL tokens).**\"\n      fi",
    "crumbs": [
      "Automation",
      "Hooks"
    ]
  },
  {
    "objectID": "automation/02_hooks.html#hook-configuration",
    "href": "automation/02_hooks.html#hook-configuration",
    "title": "Automation: Hooks",
    "section": "",
    "text": "A hook has five possible fields:\n\non: (Required) A single event name or a list of event names to listen for.\ndo: (Required) The command or inline script to run when the event fires.\ninline: (Optional) A boolean. If true, the standard output of the command is captured and injected into the conversation. Defaults to false. Only applicable to assistant_message and user_message.\nname: (Optional) A string name for the hook. If multiple hooks have the same name (e.g., one in your global config and one in a project config), the one defined later (or with higher precedence) overrides the earlier one. This allows you to replace default hooks with custom behavior.\nenv: (Optional) A map of environment variables to inject into the hook’s execution environment.\n\nhooks:\n  - name: logger\n    on: [assistant_message, user_message]\n    env:\n      LOG_FILE: /tmp/lectic.log\n    do: ./log-activity.sh\nIf do contains multiple lines, it is treated as a script and must begin with a shebang (e.g., #!/bin/bash). If it is a single line, it is treated as a command. Commands are executed directly (not through a shell), so shell features like command substitution will not work.\nHook commands run synchronously. By default, their stdout, stderr, and exit status are ignored by Lectic. However, if you set inline: true, the standard output is captured and added to the conversation.\n\nFor user_message events, the output is injected as context for the LLM before it generates a response. It also appears at the top of the assistant’s response block.\nFor assistant_message events, the output is appended to the end of the assistant’s response block. This will trigger another reply from the assistant, so be careful to only fire an inline hook when you want the assistant to generate more content.\n\nIn the .lec file, inline hook output is stored as an XML &lt;inline-attachment kind=\"hook\"&gt; block. The &lt;command&gt; element records the hook’s do field so you can see what produced the output.\n&lt;inline-attachment kind=\"hook\"&gt;\n&lt;command&gt;./my-hook.sh&lt;/command&gt;\n&lt;content type=\"text/plain\"&gt;\n┆System check complete.\n&lt;/content&gt;\n&lt;/inline-attachment&gt;",
    "crumbs": [
      "Automation",
      "Hooks"
    ]
  },
  {
    "objectID": "automation/02_hooks.html#available-events-and-environment",
    "href": "automation/02_hooks.html#available-events-and-environment",
    "title": "Automation: Hooks",
    "section": "",
    "text": "Lectic emits three hook events. When an event fires, the hook process receives its context as environment variables. No positional arguments are passed. However, the hook may receive content via standard input.\n\nuser_message\n\nEnvironment:\n\nUSER_MESSAGE: The text of the most recent user message.\nStandard Lectic variables like LECTIC_FILE, LECTIC_CONFIG, LECTIC_DATA, LECTIC_CACHE, LECTIC_STATE, and LECTIC_TEMP are also set when available.\n\nWhen: Just before the request is sent to the LLM provider.\n\nassistant_message\n\nStandard Input: The raw markdown text of the conversation body up to this point.\nEnvironment:\n\nASSISTANT_MESSAGE: The full text of the assistant’s response that was just produced.\nLECTIC_INTERLOCUTOR: The name of the interlocutor who spoke.\nLECTIC_MODEL: The model of the interlocutor who spoke.\nTOOL_USE_DONE: Set to 1 when the assistant has finished using tools and is ready to conclude. Not set if there are pending tool calls. This lets inline hooks decide whether to inject follow-up content only when all work is complete.\nTOKEN_USAGE_INPUT: Count of total input tokens used for this turn.\nTOKEN_USAGE_CACHED: Count of cached input tokens used for this turn.\nTOKEN_USAGE_OUTPUT: Count of output tokens used for this turn.\nTOKEN_USAGE_TOTAL: Total tokens used for this turn.\nLOOP_COUNT: How many times the tool calling loop has run (0-indexed).\nFINAL_PASS_COUNT: How many times the assistant has finished work but was kept alive by an inline hook.\nStandard Lectic variables as above.\n\nWhen: Immediately after the assistant’s message is streamed.\n\ntool_use_pre\n\nEnvironment:\n\nTOOL_NAME: The name of the tool being called.\nTOOL_ARGS: A JSON string containing the tool arguments.\nStandard Lectic variables as above.\n\nWhen: After tool parameters are collected but before execution.\nBehavior: If the hook exits with a non-zero status code, the tool call is blocked, and the LLM receives a “permission denied” error.\n\nerror\n\nEnvironment:\n\nERROR_MESSAGE: A descriptive error message.\nStandard Lectic variables as above.\n\nWhen: Whenever an uncaught error is encountered.",
    "crumbs": [
      "Automation",
      "Hooks"
    ]
  },
  {
    "objectID": "automation/02_hooks.html#hook-headers-and-attributes",
    "href": "automation/02_hooks.html#hook-headers-and-attributes",
    "title": "Automation: Hooks",
    "section": "",
    "text": "Hooks can pass metadata back to Lectic by including headers at the very beginning of their output. Headers follow the format LECTIC:KEY:VALUE or simply LECTIC:KEY (where the value defaults to “true”) and must appear before any other content. The headers are stripped from the visible output and stored as attributes on the inline attachment block.\n#!/usr/bin/env bash\necho \"LECTIC:final\"\necho \"\"\necho \"System check complete. One issue found.\"\nThis would be recorded roughly like this:\n&lt;inline-attachment kind=\"hook\" final=\"true\"&gt;\n&lt;command&gt;./my-hook.sh&lt;/command&gt;\n&lt;content type=\"text/plain\"&gt;\n┆System check complete. One issue found.\n&lt;/content&gt;\n&lt;/inline-attachment&gt;\nTwo headers affect control flow:\n\nfinal: When an inline hook generates output, Lectic normally continues the tool calling loop so that the assistant can see and respond to the new information. If the final header is present, Lectic prevents this extra pass, allowing the conversation turn to end immediately (unless the assistant explicitly called a tool).\nreset: When present, this header clears the conversation context up to the current message. The accumulated history sent to the provider is discarded, and the context effectively restarts from the message containing the hook output. This is useful for implementing custom context compaction or archival strategies when token limits are reached.",
    "crumbs": [
      "Automation",
      "Hooks"
    ]
  },
  {
    "objectID": "automation/02_hooks.html#example-human-in-the-loop-tool-confirmation",
    "href": "automation/02_hooks.html#example-human-in-the-loop-tool-confirmation",
    "title": "Automation: Hooks",
    "section": "",
    "text": "This example uses tool_use_pre to require confirmation before any tool execution. It uses zenity to show a dialog box with the tool name and arguments.\nhooks:\n  - on: tool_use_pre\n    do: |\n      #!/usr/bin/env bash\n      # Display a confirmation dialog\n      zenity --question \\\n             --title=\"Allow Tool Use?\" \\\n             --text=\"Tool: $TOOL_NAME\\nArgs: $TOOL_ARGS\"\n      # Zenity exits with 0 for Yes/OK and 1 for No/Cancel\n      exit $?\nThis example persists every user and assistant message to an SQLite database located in your Lectic data directory. You can later query this for personal memory, project history, or analytics.\nConfiguration:\nhooks:\n  - on: [user_message, assistant_message]\n    do: |\n      #!/usr/bin/env bash\n      set -euo pipefail\n      DB_ROOT=\"${LECTIC_DATA:-$HOME/.local/share/lectic}\"\n      DB_PATH=\"${DB_ROOT}/memory.sqlite3\"\n      mkdir -p \"${DB_ROOT}\"\n\n      # Determine role and text from available variables\n      if [[ -n \"${ASSISTANT_MESSAGE:-}\" ]]; then\n        ROLE=\"assistant\"\n        TEXT=\"$ASSISTANT_MESSAGE\"\n      else\n        ROLE=\"user\"\n        TEXT=\"${USER_MESSAGE:-}\"\n      fi\n\n      # Basic sanitizer for single quotes for SQL literal\n      esc_sq() { printf %s \"$1\" | sed \"s/'/''/g\"; }\n\n      TS=$(date -Is)\n      FILE_PATH=\"${LECTIC_FILE:-}\"\n      NAME=\"${LECTIC_INTERLOCUTOR:-}\"\n\n      sqlite3 \"$DB_PATH\" &lt;&lt;SQL\n      CREATE TABLE IF NOT EXISTS memory (\n        id INTEGER PRIMARY KEY,\n        ts TEXT NOT NULL,\n        role TEXT NOT NULL,\n        interlocutor TEXT,\n        file TEXT,\n        text TEXT NOT NULL\n      );\n      INSERT INTO memory(ts, role, interlocutor, file, text)\n      VALUES ('${TS}', '${ROLE}', '$(esc_sq \"$NAME\")',\n              '$(esc_sq \"$FILE_PATH\")', '$(esc_sq \"$TEXT\")');\n      SQL\nNotes:\n\nRequires the sqlite3 command-line tool to be installed and on your PATH.\nThe hook inspects which variable is set to decide whether the event was a user or assistant message.\nLECTIC_FILE is populated when using -f/-i and may be empty when streaming from stdin.\nAdjust the table schema to suit your use case.",
    "crumbs": [
      "Automation",
      "Hooks"
    ]
  },
  {
    "objectID": "automation/02_hooks.html#example-automatically-injecting-context",
    "href": "automation/02_hooks.html#example-automatically-injecting-context",
    "title": "Automation: Hooks",
    "section": "",
    "text": "This example automatically runs date before every user message and injects the output into the context. This allows the LLM to always know the date and time without you needing to run :cmd[date]\nhooks:\n  - on: user_message\n    inline: true\n    do: \n      #!/usr/bin/env bash\n      echo \"&lt;date-and-time&gt;\"\n      date\n      echo \"&lt;/date-and-time&gt;\"",
    "crumbs": [
      "Automation",
      "Hooks"
    ]
  },
  {
    "objectID": "automation/02_hooks.html#example-notification-when-work-completes",
    "href": "automation/02_hooks.html#example-notification-when-work-completes",
    "title": "Automation: Hooks",
    "section": "",
    "text": "This example sends a desktop notification when the assistant finishes a tool-use workflow. The hook checks TOOL_USE_DONE so you only get notified once the work is actually done, not after each intermediate step.\nhooks:\n  - on: assistant_message\n    do: |\n      #!/usr/bin/env bash\n      if [[ \"${TOOL_USE_DONE:-}\" == \"1\" ]]; then\n        notify-send \"Lectic\" \"Assistant finished working\"\n      fi\nThis is especially useful for long-running agentic tasks where you want to step away and be alerted when the assistant is done.",
    "crumbs": [
      "Automation",
      "Hooks"
    ]
  },
  {
    "objectID": "automation/02_hooks.html#example-neovim-notification-from-hooks",
    "href": "automation/02_hooks.html#example-neovim-notification-from-hooks",
    "title": "Automation: Hooks",
    "section": "",
    "text": "When using the lectic.nvim plugin, the NVIM environment variable is set to Neovim’s RPC server address. This allows hooks to communicate directly with your editor—sending notifications, opening windows, or triggering any Neovim Lua API.\nThis example sends a notification to Neovim when the assistant finishes working:\nhooks:\n  - on: assistant_message\n    do: |\n      #!/usr/bin/env bash\n      if [[ \"${TOOL_USE_DONE:-}\" == \"1\" && -n \"${NVIM:-}\" ]]; then\n        nvim --server \"$NVIM\" --remote-expr \\\n          \"luaeval('vim.notify(\\\"Lectic: Assistant finished working\\\", vim.log.levels.INFO)')\"\n      fi\nThe pattern nvim --server \"$NVIM\" --remote-expr \"luaeval('...')\" lets you execute arbitrary Lua in the running Neovim instance. Some ideas:\n\nPlay a sound: vim.fn.system('paplay /usr/share/sounds/...')\nFlash the screen: vim.cmd('sleep 100m | redraw!')\nUpdate a status line variable\nTrigger a custom autocommand: vim.api.nvim_exec_autocmds('User', {pattern = 'LecticDone'})",
    "crumbs": [
      "Automation",
      "Hooks"
    ]
  },
  {
    "objectID": "automation/02_hooks.html#example-reset-context-on-token-limit",
    "href": "automation/02_hooks.html#example-reset-context-on-token-limit",
    "title": "Automation: Hooks",
    "section": "",
    "text": "This example checks the total token usage and, if it exceeds a limit, resets the conversation context. It also uses the final header to stop the assistant from responding to the reset message immediately.\nhooks:\n  - on: assistant_message\n    inline: true\n    do: |\n      #!/usr/bin/env bash\n      LIMIT=100000\n      TOTAL=\"${TOKEN_USAGE_TOTAL:-0}\"\n      \n      if [ \"$TOTAL\" -gt \"$LIMIT\" ]; then\n        echo \"LECTIC:reset\"\n        echo \"LECTIC:final\"\n        echo \"\"\n        echo \"**Context cleared (usage: $TOTAL tokens).**\"\n      fi",
    "crumbs": [
      "Automation",
      "Hooks"
    ]
  },
  {
    "objectID": "cookbook/index.html",
    "href": "cookbook/index.html",
    "title": "Cookbook",
    "section": "",
    "text": "This section contains practical recipes showing how to combine Lectic’s primitives to build useful workflows. Each recipe is self-contained and can be adapted to your needs.\n\n\n\nCoding Assistant: An agentic setup with shell tools, TypeScript checking, and human-in-the-loop confirmation.\nGit Commit Messages: A custom subcommand that generates commit messages from staged changes.\nResearch with Multiple Perspectives: Using multiple interlocutors to get different viewpoints on a problem.\nConversation Memory: Persisting conversations to SQLite and retrieving relevant context.\nContext Compaction: Automatically summarizing and resetting context when token limits approach.\nCustom Sandboxing: Isolate tool execution using wrapper scripts like Bubblewrap.\nControl Flow with Macros: Implement advanced logic like loops, conditionals, and maps using recursive macros.",
    "crumbs": [
      "Cookbook",
      "Overview"
    ]
  },
  {
    "objectID": "cookbook/index.html#recipes",
    "href": "cookbook/index.html#recipes",
    "title": "Cookbook",
    "section": "",
    "text": "Coding Assistant: An agentic setup with shell tools, TypeScript checking, and human-in-the-loop confirmation.\nGit Commit Messages: A custom subcommand that generates commit messages from staged changes.\nResearch with Multiple Perspectives: Using multiple interlocutors to get different viewpoints on a problem.\nConversation Memory: Persisting conversations to SQLite and retrieving relevant context.\nContext Compaction: Automatically summarizing and resetting context when token limits approach.\nCustom Sandboxing: Isolate tool execution using wrapper scripts like Bubblewrap.\nControl Flow with Macros: Implement advanced logic like loops, conditionals, and maps using recursive macros.",
    "crumbs": [
      "Cookbook",
      "Overview"
    ]
  },
  {
    "objectID": "cookbook/07_control_flow_macros.html",
    "href": "cookbook/07_control_flow_macros.html",
    "title": "Control Flow with Macros",
    "section": "",
    "text": "Because Lectic’s macros support recursion and can execute scripts during the expansion phase, it is possible to build powerful control flow structures like conditionals, loops, and maps.\nThis guide demonstrates how to implement these constructs. While complex logic is often better handled by writing a custom tool or script, these examples show the flexibility of the macro system.\n\n\nThe key to control flow is the pre phase of macro expansion. (See Automation: Macros).\nBecause the result of a pre expansion is itself recursively expanded, a macro can return a new instance of itself with different arguments, effectively creating a loop.\nAdditionally, because pre expansions can run shell scripts (exec:), they can make decisions based on arguments or environment variables.\n\n\n\nA simple conditional macro evaluates a condition and outputs either its content (the “then” block) or an alternative (the “else” block).\nDefinition:\nmacros:\n  - name: if\n    post: |\n      exec:#!/bin/bash\n      if [ \"$ARG\" = \"true\" ]; then\n        echo \"$THEN\"\n      else\n        echo \"$ELSE\"\n      fi\nUsage:\n:if[true]{THEN=\"This is displayed if true\" ELSE=\"This is displayed if false\"}\n:if[false]{THEN=\"This is hidden if not true\" ELSE=\"This is shown instead\"}\n:if[:some_check[]]{THEN=\"This is hidden if not true\" ELSE=\"This is shown instead\"}\n\n\n\nThe previous example required passing the content as attributes (THEN=\"...\"), which is clumsy for large blocks of text. More importantly, if we want to conditionally run a command, we need to prevent it from executing at all unless the condition is met.\nIf we use the post phase, the children are expanded before the parent macro. To achieve “short-circuiting” (where the children are only expanded if the condition is true), we can use the pre phase of macro expansion.\nDefinition:\nmacros:\n  - name: when\n    # In the 'pre' phase, ARG contains the raw, unexpanded body text.\n    pre: |\n      exec:#!/bin/bash\n      if [ \"$CONDITION\" = \"true\" ]; then\n        # Return the body to be expanded\n        echo \"$ARG\"\n      else\n        # Return a comment (effectively deleting the block)\n        echo \"&lt;!-- skipped --&gt;\"\n      fi\nUsage:\n:when[\n  This content is only processed if the condition is met.\n  :cmd[echo \"Expensive operation running...\"]\n]{CONDITION=\"false\"}\nIn this example the expensive :cmd is never expanded or executed.\n\n\n\nBy having a macro call itself, we can create loops. We need a termination condition to stop the recursion (preventing an infinite loop).\nDefinition:\nmacros:\n  - name: countdown\n    pre: |\n      exec:#!/bin/bash\n      N=${ARG:-10}\n      if [ \"$N\" -gt 0 ]; then\n        echo \"$N...\"\n        # Recursive call with N-1\n        echo \":countdown[$((N-1))]\"\n      else\n        echo \"Liftoff!\"\n      fi\nUsage:\n:countdown[3]\nOutput:\n3...\n2...\n1...\nLiftoff!\n\n\n\nWe can iterate over a list of items and apply another macro to each one. This is useful for batch processing files, names, or data.\nThis implementation assumes a space-separated list of items.\nDefinition:\nmacros:\n  - name: map\n    pre: |\n      exec:#!/bin/bash\n      # Split ARG into array (space separated)\n      items=($ARG)\n      \n      # Termination: if no items, stop\n      if [ ${#items[@]} -eq 0 ]; then\n          echo \"&lt;!-- --&gt;\"\n          exit 0\n      fi\n      \n      # Head: The first item\n      first=${items[0]}\n      \n      # Tail: The rest of the items\n      rest=${items[@]:1}\n      \n      # 1. Apply the target macro to the first item\n      echo \":$MACRO[$first]\"\n      \n      # 2. Recurse on the rest (if any)\n      if [ -n \"$rest\" ]; then\n         echo \":map[$rest]{MACRO=$MACRO}\"\n      fi\nUsage:\nSuppose you have a macro greet defined:\nmacros:\n  - name: greet\n    expansion: \"Hello, $ARG! \"\nYou can map it over a list of names:\n:map[Alice Bob Charlie]{MACRO=\"greet\"}\nOutput:\nHello, Alice! Hello, Bob! Hello, Charlie! \n\n\n\nLet’s combine these concepts into a “Launch Sequence” generator. We want to check a list of systems, and if they are all go, initiate a countdown.\nConfiguration:\nmacros:\n  - name: launch_sequence\n    expansion: |\n      # Check systems\n      :map[Propulsion Guidance Life-Support]{MACRO=\"check_system\"}\n      \n      # Start countdown\n      :countdown[5]\n\n  - name: check_system\n    expansion: \"Checking $ARG... OK.\\n\"\n\n  - name: map\n    pre: |\n      exec:#!/bin/bash\n      items=($ARG)\n      if [ ${#items[@]} -eq 0 ]; then echo \"&lt;!-- --&gt;\"; exit 0; fi\n      first=${items[0]}\n      rest=${items[@]:1}\n      echo \":$MACRO[$first]\"\n      if [ -n \"$rest\" ]; then echo \":map[$rest]{MACRO=$MACRO}\"; fi\n\n  - name: countdown\n    pre: |\n      exec:#!/bin/bash\n      N=${ARG:-10}\n      if [ \"$N\" -gt 0 ]; then\n        echo \"$N...\"\n        echo \":countdown[$((N-1))]\"\n      else\n        echo \"Liftoff!\"\n      fi\nUsage:\n:launch_sequence[]\nOutput:\nChecking Propulsion... OK.\nChecking Guidance... OK.\nChecking Life-Support... OK.\n5...\n4...\n3...\n2...\n1...\nLiftoff!\n\n\n\n\n\n\nNoteRecursion Limit\n\n\n\nLectic has a recursion depth limit (default 100) to prevent infinite loops from crashing the process. If your loop needs to run more than 100 times, you should probably use an external script (exec:) instead of a recursive macro.",
    "crumbs": [
      "Cookbook",
      "Control Flow With Macros"
    ]
  },
  {
    "objectID": "cookbook/07_control_flow_macros.html#the-mechanism-recursion-pre",
    "href": "cookbook/07_control_flow_macros.html#the-mechanism-recursion-pre",
    "title": "Control Flow with Macros",
    "section": "",
    "text": "The key to control flow is the pre phase of macro expansion. (See Automation: Macros).\nBecause the result of a pre expansion is itself recursively expanded, a macro can return a new instance of itself with different arguments, effectively creating a loop.\nAdditionally, because pre expansions can run shell scripts (exec:), they can make decisions based on arguments or environment variables.",
    "crumbs": [
      "Cookbook",
      "Control Flow With Macros"
    ]
  },
  {
    "objectID": "cookbook/07_control_flow_macros.html#recipe-1-conditional-if",
    "href": "cookbook/07_control_flow_macros.html#recipe-1-conditional-if",
    "title": "Control Flow with Macros",
    "section": "",
    "text": "A simple conditional macro evaluates a condition and outputs either its content (the “then” block) or an alternative (the “else” block).\nDefinition:\nmacros:\n  - name: if\n    post: |\n      exec:#!/bin/bash\n      if [ \"$ARG\" = \"true\" ]; then\n        echo \"$THEN\"\n      else\n        echo \"$ELSE\"\n      fi\nUsage:\n:if[true]{THEN=\"This is displayed if true\" ELSE=\"This is displayed if false\"}\n:if[false]{THEN=\"This is hidden if not true\" ELSE=\"This is shown instead\"}\n:if[:some_check[]]{THEN=\"This is hidden if not true\" ELSE=\"This is shown instead\"}",
    "crumbs": [
      "Cookbook",
      "Control Flow With Macros"
    ]
  },
  {
    "objectID": "cookbook/07_control_flow_macros.html#recipe-2-short-circuiting-conditional-when",
    "href": "cookbook/07_control_flow_macros.html#recipe-2-short-circuiting-conditional-when",
    "title": "Control Flow with Macros",
    "section": "",
    "text": "The previous example required passing the content as attributes (THEN=\"...\"), which is clumsy for large blocks of text. More importantly, if we want to conditionally run a command, we need to prevent it from executing at all unless the condition is met.\nIf we use the post phase, the children are expanded before the parent macro. To achieve “short-circuiting” (where the children are only expanded if the condition is true), we can use the pre phase of macro expansion.\nDefinition:\nmacros:\n  - name: when\n    # In the 'pre' phase, ARG contains the raw, unexpanded body text.\n    pre: |\n      exec:#!/bin/bash\n      if [ \"$CONDITION\" = \"true\" ]; then\n        # Return the body to be expanded\n        echo \"$ARG\"\n      else\n        # Return a comment (effectively deleting the block)\n        echo \"&lt;!-- skipped --&gt;\"\n      fi\nUsage:\n:when[\n  This content is only processed if the condition is met.\n  :cmd[echo \"Expensive operation running...\"]\n]{CONDITION=\"false\"}\nIn this example the expensive :cmd is never expanded or executed.",
    "crumbs": [
      "Cookbook",
      "Control Flow With Macros"
    ]
  },
  {
    "objectID": "cookbook/07_control_flow_macros.html#recipe-3-recursion-loops-countdown",
    "href": "cookbook/07_control_flow_macros.html#recipe-3-recursion-loops-countdown",
    "title": "Control Flow with Macros",
    "section": "",
    "text": "By having a macro call itself, we can create loops. We need a termination condition to stop the recursion (preventing an infinite loop).\nDefinition:\nmacros:\n  - name: countdown\n    pre: |\n      exec:#!/bin/bash\n      N=${ARG:-10}\n      if [ \"$N\" -gt 0 ]; then\n        echo \"$N...\"\n        # Recursive call with N-1\n        echo \":countdown[$((N-1))]\"\n      else\n        echo \"Liftoff!\"\n      fi\nUsage:\n:countdown[3]\nOutput:\n3...\n2...\n1...\nLiftoff!",
    "crumbs": [
      "Cookbook",
      "Control Flow With Macros"
    ]
  },
  {
    "objectID": "cookbook/07_control_flow_macros.html#recipe-4-iteration-map",
    "href": "cookbook/07_control_flow_macros.html#recipe-4-iteration-map",
    "title": "Control Flow with Macros",
    "section": "",
    "text": "We can iterate over a list of items and apply another macro to each one. This is useful for batch processing files, names, or data.\nThis implementation assumes a space-separated list of items.\nDefinition:\nmacros:\n  - name: map\n    pre: |\n      exec:#!/bin/bash\n      # Split ARG into array (space separated)\n      items=($ARG)\n      \n      # Termination: if no items, stop\n      if [ ${#items[@]} -eq 0 ]; then\n          echo \"&lt;!-- --&gt;\"\n          exit 0\n      fi\n      \n      # Head: The first item\n      first=${items[0]}\n      \n      # Tail: The rest of the items\n      rest=${items[@]:1}\n      \n      # 1. Apply the target macro to the first item\n      echo \":$MACRO[$first]\"\n      \n      # 2. Recurse on the rest (if any)\n      if [ -n \"$rest\" ]; then\n         echo \":map[$rest]{MACRO=$MACRO}\"\n      fi\nUsage:\nSuppose you have a macro greet defined:\nmacros:\n  - name: greet\n    expansion: \"Hello, $ARG! \"\nYou can map it over a list of names:\n:map[Alice Bob Charlie]{MACRO=\"greet\"}\nOutput:\nHello, Alice! Hello, Bob! Hello, Charlie!",
    "crumbs": [
      "Cookbook",
      "Control Flow With Macros"
    ]
  },
  {
    "objectID": "cookbook/07_control_flow_macros.html#fun-example-the-launch-sequence",
    "href": "cookbook/07_control_flow_macros.html#fun-example-the-launch-sequence",
    "title": "Control Flow with Macros",
    "section": "",
    "text": "Let’s combine these concepts into a “Launch Sequence” generator. We want to check a list of systems, and if they are all go, initiate a countdown.\nConfiguration:\nmacros:\n  - name: launch_sequence\n    expansion: |\n      # Check systems\n      :map[Propulsion Guidance Life-Support]{MACRO=\"check_system\"}\n      \n      # Start countdown\n      :countdown[5]\n\n  - name: check_system\n    expansion: \"Checking $ARG... OK.\\n\"\n\n  - name: map\n    pre: |\n      exec:#!/bin/bash\n      items=($ARG)\n      if [ ${#items[@]} -eq 0 ]; then echo \"&lt;!-- --&gt;\"; exit 0; fi\n      first=${items[0]}\n      rest=${items[@]:1}\n      echo \":$MACRO[$first]\"\n      if [ -n \"$rest\" ]; then echo \":map[$rest]{MACRO=$MACRO}\"; fi\n\n  - name: countdown\n    pre: |\n      exec:#!/bin/bash\n      N=${ARG:-10}\n      if [ \"$N\" -gt 0 ]; then\n        echo \"$N...\"\n        echo \":countdown[$((N-1))]\"\n      else\n        echo \"Liftoff!\"\n      fi\nUsage:\n:launch_sequence[]\nOutput:\nChecking Propulsion... OK.\nChecking Guidance... OK.\nChecking Life-Support... OK.\n5...\n4...\n3...\n2...\n1...\nLiftoff!\n\n\n\n\n\n\nNoteRecursion Limit\n\n\n\nLectic has a recursion depth limit (default 100) to prevent infinite loops from crashing the process. If your loop needs to run more than 100 times, you should probably use an external script (exec:) instead of a recursive macro.",
    "crumbs": [
      "Cookbook",
      "Control Flow With Macros"
    ]
  },
  {
    "objectID": "cookbook/01_coding_assistant.html",
    "href": "cookbook/01_coding_assistant.html",
    "title": "Recipe: Coding Assistant",
    "section": "",
    "text": "This recipe shows how to set up an agentic coding assistant with shell tools, type checking, and a confirmation dialog before tool execution.\n\n\nWe’ll give the assistant access to:\n\nFile reading and writing\nRunning TypeScript compiler and linter\nExecuting shell commands (with confirmation)\n\n\n\nCreate a lectic.yaml in your project root:\ninterlocutor:\n  name: Assistant\n  prompt: |\n    You are a senior software engineer helping with this codebase.\n    \n    When making changes:\n    1. Read relevant files first to understand context\n    2. Make minimal, focused changes\n    3. Run tsc and eslint after edits to catch errors\n    4. Explain your reasoning\n  provider: anthropic\n  model: claude-sonnet-4-20250514\n  tools:\n    - exec: cat\n      name: read_file\n      usage: Read a file. Pass the file path as an argument.\n    - name: write_file\n      usage: Write content to a file. \n      exec: |\n        #!/bin/bash\n        cat &gt; \"$FILE_PATH\"\n      schema:\n        FILE_PATH: The path to write to.\n        CONTENT: The content to write (passed via stdin).\n    - exec: tsc --noEmit\n      name: typecheck\n      usage: Run the TypeScript compiler to check for type errors.\n    - exec: eslint\n      name: lint\n      usage: Run ESLint on files. Pass file paths as arguments.\n    - exec: bash -c\n      name: shell\n      usage: Run a shell command. Use for git, grep, find, etc.\n\nhooks:\n  - on: tool_use_pre\n    do: ~/.config/lectic/confirm.sh\n\n\n\nFor graphical environments, create ~/.config/lectic/confirm.sh:\n#!/bin/bash\n# Requires: zenity (GTK) or kdialog (KDE)\n\n# Skip confirmation for read-only tools\ncase \"$TOOL_NAME\" in\n  read_file|typecheck|lint)\n    exit 0\n    ;;\nesac\n\n# Show confirmation dialog\nzenity --question \\\n  --title=\"Allow tool use?\" \\\n  --text=\"Tool: $TOOL_NAME\\n\\nArguments:\\n$TOOL_ARGS\" \\\n  --width=400\n\nexit $?\nMake it executable: chmod +x ~/.config/lectic/confirm.sh\n\n\n\n\n\n\nNote\n\n\n\nThe confirmation hook runs as a subprocess without access to a terminal, so interactive terminal prompts (like read -p) won’t work. Use a GUI dialog tool like zenity, kdialog, or osascript on macOS.\n\n\n\n\n\n\nCreate a conversation file in your project:\n---\n# Uses lectic.yaml from project root\n---\n\nI need to add input validation to the `processUser` function in \nsrc/users.ts. It should reject empty names and invalid email formats.\nRun it:\nlectic -i task.lec\nThe assistant will:\n\nRead src/users.ts to understand the current implementation\nPropose changes (you’ll see a confirmation dialog for writes)\nRun tsc and eslint to verify the changes\nReport results\n\n\n\n\n\n\nRemove write and shell tools for a safer setup that can only read and analyze:\ntools:\n  - exec: cat\n    name: read_file\n  - exec: rg --json\n    name: search\n    usage: Search with ripgrep. Pass pattern and optional path.\n  - exec: tsc --noEmit\n    name: typecheck\n\n\n\nFor stronger isolation, use the bubblewrap sandbox included in the repository at extra/sandbox/bwrap-sandbox.sh:\ntools:\n  - exec: bash -c\n    name: shell\n    sandbox: ./extra/sandbox/bwrap-sandbox.sh\nThe sandbox script uses Bubblewrap to run commands in an isolated environment. It:\n\nCreates a temporary home directory that’s discarded after execution\nMounts the current working directory read-write\nProvides read-only access to essential system paths (/usr, /bin, etc.)\nBlocks network access by default\n\nYou can copy the script to your config directory and modify it to suit your needs — for example, to allow network access or mount additional paths.\n\n\n\nAdd a hook to notify you when the assistant finishes working:\nhooks:\n  - on: tool_use_pre\n    do: ~/.config/lectic/confirm.sh\n  - on: assistant_message\n    do: |\n      #!/bin/bash\n      if [[ \"$TOOL_USE_DONE\" == \"1\" ]]; then\n        notify-send \"Lectic\" \"Task complete\"\n      fi",
    "crumbs": [
      "Cookbook",
      "Coding Assistant"
    ]
  },
  {
    "objectID": "cookbook/01_coding_assistant.html#the-setup",
    "href": "cookbook/01_coding_assistant.html#the-setup",
    "title": "Recipe: Coding Assistant",
    "section": "",
    "text": "We’ll give the assistant access to:\n\nFile reading and writing\nRunning TypeScript compiler and linter\nExecuting shell commands (with confirmation)\n\n\n\nCreate a lectic.yaml in your project root:\ninterlocutor:\n  name: Assistant\n  prompt: |\n    You are a senior software engineer helping with this codebase.\n    \n    When making changes:\n    1. Read relevant files first to understand context\n    2. Make minimal, focused changes\n    3. Run tsc and eslint after edits to catch errors\n    4. Explain your reasoning\n  provider: anthropic\n  model: claude-sonnet-4-20250514\n  tools:\n    - exec: cat\n      name: read_file\n      usage: Read a file. Pass the file path as an argument.\n    - name: write_file\n      usage: Write content to a file. \n      exec: |\n        #!/bin/bash\n        cat &gt; \"$FILE_PATH\"\n      schema:\n        FILE_PATH: The path to write to.\n        CONTENT: The content to write (passed via stdin).\n    - exec: tsc --noEmit\n      name: typecheck\n      usage: Run the TypeScript compiler to check for type errors.\n    - exec: eslint\n      name: lint\n      usage: Run ESLint on files. Pass file paths as arguments.\n    - exec: bash -c\n      name: shell\n      usage: Run a shell command. Use for git, grep, find, etc.\n\nhooks:\n  - on: tool_use_pre\n    do: ~/.config/lectic/confirm.sh\n\n\n\nFor graphical environments, create ~/.config/lectic/confirm.sh:\n#!/bin/bash\n# Requires: zenity (GTK) or kdialog (KDE)\n\n# Skip confirmation for read-only tools\ncase \"$TOOL_NAME\" in\n  read_file|typecheck|lint)\n    exit 0\n    ;;\nesac\n\n# Show confirmation dialog\nzenity --question \\\n  --title=\"Allow tool use?\" \\\n  --text=\"Tool: $TOOL_NAME\\n\\nArguments:\\n$TOOL_ARGS\" \\\n  --width=400\n\nexit $?\nMake it executable: chmod +x ~/.config/lectic/confirm.sh\n\n\n\n\n\n\nNote\n\n\n\nThe confirmation hook runs as a subprocess without access to a terminal, so interactive terminal prompts (like read -p) won’t work. Use a GUI dialog tool like zenity, kdialog, or osascript on macOS.",
    "crumbs": [
      "Cookbook",
      "Coding Assistant"
    ]
  },
  {
    "objectID": "cookbook/01_coding_assistant.html#usage",
    "href": "cookbook/01_coding_assistant.html#usage",
    "title": "Recipe: Coding Assistant",
    "section": "",
    "text": "Create a conversation file in your project:\n---\n# Uses lectic.yaml from project root\n---\n\nI need to add input validation to the `processUser` function in \nsrc/users.ts. It should reject empty names and invalid email formats.\nRun it:\nlectic -i task.lec\nThe assistant will:\n\nRead src/users.ts to understand the current implementation\nPropose changes (you’ll see a confirmation dialog for writes)\nRun tsc and eslint to verify the changes\nReport results",
    "crumbs": [
      "Cookbook",
      "Coding Assistant"
    ]
  },
  {
    "objectID": "cookbook/01_coding_assistant.html#variations",
    "href": "cookbook/01_coding_assistant.html#variations",
    "title": "Recipe: Coding Assistant",
    "section": "",
    "text": "Remove write and shell tools for a safer setup that can only read and analyze:\ntools:\n  - exec: cat\n    name: read_file\n  - exec: rg --json\n    name: search\n    usage: Search with ripgrep. Pass pattern and optional path.\n  - exec: tsc --noEmit\n    name: typecheck\n\n\n\nFor stronger isolation, use the bubblewrap sandbox included in the repository at extra/sandbox/bwrap-sandbox.sh:\ntools:\n  - exec: bash -c\n    name: shell\n    sandbox: ./extra/sandbox/bwrap-sandbox.sh\nThe sandbox script uses Bubblewrap to run commands in an isolated environment. It:\n\nCreates a temporary home directory that’s discarded after execution\nMounts the current working directory read-write\nProvides read-only access to essential system paths (/usr, /bin, etc.)\nBlocks network access by default\n\nYou can copy the script to your config directory and modify it to suit your needs — for example, to allow network access or mount additional paths.\n\n\n\nAdd a hook to notify you when the assistant finishes working:\nhooks:\n  - on: tool_use_pre\n    do: ~/.config/lectic/confirm.sh\n  - on: assistant_message\n    do: |\n      #!/bin/bash\n      if [[ \"$TOOL_USE_DONE\" == \"1\" ]]; then\n        notify-send \"Lectic\" \"Task complete\"\n      fi",
    "crumbs": [
      "Cookbook",
      "Coding Assistant"
    ]
  },
  {
    "objectID": "cookbook/02_commit_messages.html",
    "href": "cookbook/02_commit_messages.html",
    "title": "Recipe: Git Commit Messages",
    "section": "",
    "text": "This recipe creates a custom lectic commit subcommand that generates commit messages from your staged changes.\n\n\nLectic looks for executables named lectic-&lt;command&gt; in your config directory, data directory, or PATH. Create ~/.config/lectic/lectic-commit:\n#!/bin/bash\nset -euo pipefail\n\n# Check for staged changes\nif git diff --cached --quiet; then\n  echo \"No staged changes\" &gt;&2\n  exit 1\nfi\n\nlectic -f ~/.config/lectic/commit-prompt.lec -S\nMake it executable:\nchmod +x ~/.config/lectic/lectic-commit\n\n\n\nCreate ~/.config/lectic/commit-prompt.lec:\n---\ninterlocutor:\n  name: Assistant\n  prompt: |\n    You write git commit messages following the Conventional Commits\n    specification. Output ONLY the commit message, nothing else.\n    \n    Format:\n    &lt;type&gt;[optional scope]: &lt;description&gt;\n    \n    [optional body]\n    \n    Types: feat, fix, docs, style, refactor, perf, test, chore\n    \n    Rules:\n    - Subject line max 50 characters\n    - Use imperative mood (\"add\" not \"added\")\n    - Body wraps at 72 characters\n    - Explain what and why, not how\n  provider: anthropic\n  model: claude-3-haiku-20240307\n  max_tokens: 500\n---\n\nWrite a commit message for this diff:\n\n:cmd[git diff --cached]\nThe :cmd[git diff --cached] directive runs git diff --cached and includes the output in the context sent to the LLM.\n\n\n\nStage your changes and run:\ngit add -p\nlectic commit\nOutput:\nfeat(auth): add password strength validation\n\nImplement zxcvbn-based password strength checking during registration.\nReject passwords scoring below 3 and display feedback to users.\n\n\nlectic commit | git commit -F -\n\n\n\nlectic commit | git commit -eF -\n\n\n\n\n\n\nModify the prompt to show recent history:\nRecent commits for context:\n:cmd[git log --oneline -5]\n\nWrite a commit message for this diff:\n:cmd[git diff --cached]\n\n\n\nCreate multiple prompt files for different projects:\n\ncommit-prompt-conventional.lec — Conventional Commits\ncommit-prompt-gitmoji.lec — Gitmoji style\ncommit-prompt-simple.lec — Plain descriptions\n\nThen modify the subcommand to accept an argument:\n#!/bin/bash\nset -euo pipefail\n\nSTYLE=\"${1:-conventional}\"\nPROMPT=\"$HOME/.config/lectic/commit-prompt-$STYLE.lec\"\n\nif [[ ! -f \"$PROMPT\" ]]; then\n  echo \"Unknown style: $STYLE\" &gt;&2\n  exit 1\nfi\n\nif git diff --cached --quiet; then\n  echo \"No staged changes\" &gt;&2\n  exit 1\nfi\n\nlectic -f \"$PROMPT\" -S\nUsage: lectic commit gitmoji\n\n\n\nGenerate messages for each file separately:\n#!/bin/bash\nfor file in $(git diff --cached --name-only); do\n  echo \"=== $file ===\"\n  # Create a temporary prompt with just this file's diff\n  cat &gt; /tmp/commit-single.lec &lt;&lt; EOF\n---\ninterlocutor:\n  name: Assistant\n  prompt: Write a conventional commit message for this change. Output only the message.\n  model: claude-3-haiku-20240307\n  max_tokens: 200\n---\n\n:cmd[git diff --cached -- \"$file\"]\nEOF\n  lectic -f /tmp/commit-single.lec -S\n  echo\ndone",
    "crumbs": [
      "Cookbook",
      "Commit Messages"
    ]
  },
  {
    "objectID": "cookbook/02_commit_messages.html#the-subcommand",
    "href": "cookbook/02_commit_messages.html#the-subcommand",
    "title": "Recipe: Git Commit Messages",
    "section": "",
    "text": "Lectic looks for executables named lectic-&lt;command&gt; in your config directory, data directory, or PATH. Create ~/.config/lectic/lectic-commit:\n#!/bin/bash\nset -euo pipefail\n\n# Check for staged changes\nif git diff --cached --quiet; then\n  echo \"No staged changes\" &gt;&2\n  exit 1\nfi\n\nlectic -f ~/.config/lectic/commit-prompt.lec -S\nMake it executable:\nchmod +x ~/.config/lectic/lectic-commit",
    "crumbs": [
      "Cookbook",
      "Commit Messages"
    ]
  },
  {
    "objectID": "cookbook/02_commit_messages.html#the-prompt-template",
    "href": "cookbook/02_commit_messages.html#the-prompt-template",
    "title": "Recipe: Git Commit Messages",
    "section": "",
    "text": "Create ~/.config/lectic/commit-prompt.lec:\n---\ninterlocutor:\n  name: Assistant\n  prompt: |\n    You write git commit messages following the Conventional Commits\n    specification. Output ONLY the commit message, nothing else.\n    \n    Format:\n    &lt;type&gt;[optional scope]: &lt;description&gt;\n    \n    [optional body]\n    \n    Types: feat, fix, docs, style, refactor, perf, test, chore\n    \n    Rules:\n    - Subject line max 50 characters\n    - Use imperative mood (\"add\" not \"added\")\n    - Body wraps at 72 characters\n    - Explain what and why, not how\n  provider: anthropic\n  model: claude-3-haiku-20240307\n  max_tokens: 500\n---\n\nWrite a commit message for this diff:\n\n:cmd[git diff --cached]\nThe :cmd[git diff --cached] directive runs git diff --cached and includes the output in the context sent to the LLM.",
    "crumbs": [
      "Cookbook",
      "Commit Messages"
    ]
  },
  {
    "objectID": "cookbook/02_commit_messages.html#usage",
    "href": "cookbook/02_commit_messages.html#usage",
    "title": "Recipe: Git Commit Messages",
    "section": "",
    "text": "Stage your changes and run:\ngit add -p\nlectic commit\nOutput:\nfeat(auth): add password strength validation\n\nImplement zxcvbn-based password strength checking during registration.\nReject passwords scoring below 3 and display feedback to users.\n\n\nlectic commit | git commit -F -\n\n\n\nlectic commit | git commit -eF -",
    "crumbs": [
      "Cookbook",
      "Commit Messages"
    ]
  },
  {
    "objectID": "cookbook/02_commit_messages.html#variations",
    "href": "cookbook/02_commit_messages.html#variations",
    "title": "Recipe: Git Commit Messages",
    "section": "",
    "text": "Modify the prompt to show recent history:\nRecent commits for context:\n:cmd[git log --oneline -5]\n\nWrite a commit message for this diff:\n:cmd[git diff --cached]\n\n\n\nCreate multiple prompt files for different projects:\n\ncommit-prompt-conventional.lec — Conventional Commits\ncommit-prompt-gitmoji.lec — Gitmoji style\ncommit-prompt-simple.lec — Plain descriptions\n\nThen modify the subcommand to accept an argument:\n#!/bin/bash\nset -euo pipefail\n\nSTYLE=\"${1:-conventional}\"\nPROMPT=\"$HOME/.config/lectic/commit-prompt-$STYLE.lec\"\n\nif [[ ! -f \"$PROMPT\" ]]; then\n  echo \"Unknown style: $STYLE\" &gt;&2\n  exit 1\nfi\n\nif git diff --cached --quiet; then\n  echo \"No staged changes\" &gt;&2\n  exit 1\nfi\n\nlectic -f \"$PROMPT\" -S\nUsage: lectic commit gitmoji\n\n\n\nGenerate messages for each file separately:\n#!/bin/bash\nfor file in $(git diff --cached --name-only); do\n  echo \"=== $file ===\"\n  # Create a temporary prompt with just this file's diff\n  cat &gt; /tmp/commit-single.lec &lt;&lt; EOF\n---\ninterlocutor:\n  name: Assistant\n  prompt: Write a conventional commit message for this change. Output only the message.\n  model: claude-3-haiku-20240307\n  max_tokens: 200\n---\n\n:cmd[git diff --cached -- \"$file\"]\nEOF\n  lectic -f /tmp/commit-single.lec -S\n  echo\ndone",
    "crumbs": [
      "Cookbook",
      "Commit Messages"
    ]
  }
]