[
  {
    "objectID": "cookbook/06_custom_sandboxing.html",
    "href": "cookbook/06_custom_sandboxing.html",
    "title": "Recipe: Custom Sandboxing",
    "section": "",
    "text": "Giving an LLM access to exec tools is powerful, but it carries risk. While Lectic provides a sandbox configuration option, it doesn’t enforce a specific technology. Instead, it delegates execution to a script you control.\nThis recipe walks you through the sandbox script mechanism and helps you write wrappers to isolate tool execution.\n\n\nWhen you configure a sandbox in your lectic.yaml, Lectic wraps the execution of exec tools and local mcp_command tools. Instead of executing the tool directly, it executes your sandbox script and passes the tool’s command and arguments as arguments to that script.\nAn exec tool without sandbox: Lectic runs: ls -la\nWith sandbox: ./wrapper.sh: Lectic runs: ./wrapper.sh ls -la\nThe command to launch an MCP server is wrapped by the sandbox script in a similar way.\nYour sandbox script is responsible for:\n\nSetting up the environment.\nExecuting the command (passed in $@).\nCleaning up.\nReturning the exit code.\n\n\n\n\nBefore trying to isolate the filesystem, let’s make a wrapper that simply logs every command the assistant tries to run. This is useful for auditing.\nCreate ~/.config/lectic/audit.sh:\n#!/bin/bash\n# Append timestamp and command to a log file\necho \"[$(date)] Executing: $*\" &gt;&gt; \"$HOME/.lectic_audit_log\"\n\n# Run the actual command\nexec \"$@\"\nMake it executable:\nchmod +x ~/.config/lectic/audit.sh\nConfigure it in lectic.yaml:\ninterlocutor:\n  name: Assistant\n  # Apply to all exec/local MCP tools for this interlocutor\n  sandbox: ~/.config/lectic/audit.sh\n  tools:\n    - exec: ls\n\n\n\nFor actual safety, we can use Bubblewrap (bwrap). This tool creates a new namespace for the process, allowing you to control exactly which parts of your filesystem the assistant can see or write to.\nHere is a simplified version of the lectic-bwrap script found in the Lectic repository. It creates a read-only view of the system but gives the assistant a temporary, empty home directory.\nCreate ~/.config/lectic/safe-run.sh:\n#!/bin/bash\nset -euo pipefail\n\n# Create a temporary directory for the assistant's \"home\"\nFAKE_HOME=$(mktemp -d)\n\n# Ensure we clean up the temp dir when the script exits\ntrap 'rm -rf \"$FAKE_HOME\"' EXIT\n\n# Run bwrap with specific permissions\nbwrap \\\n  --ro-bind / / \\                 # Mount the root as read-only\n  --dev /dev \\                    # legitimate devices\n  --proc /proc \\                  # legitimate processes\n  --bind \"$PWD\" \"$PWD\" \\          # Allow read-write access to current project\n  --bind \"$FAKE_HOME\" \"$HOME\" \\   # Fake the home directory\n  --unshare-net \\                 # Disable network access (optional)\n  --die-with-parent \\             # Kill process if lectic dies\n  \"$@\"\n\n\n\nRead-only Root: The assistant cannot modify system files (/usr, /bin, etc.).\nFake Home: If the assistant runs rm -rf ~, it only deletes the temporary directory, not your actual home folder.\nProject Access: The script explicitly binds $PWD, so the assistant can still read and write files in the directory where you ran Lectic.\nNo Network: The --unshare-net flag prevents the assistant from making outbound connections (remove this if you want it to use curl or something similar).\n\n\n\n\n\nIsolating the filesystem by copying the project to a temporary directory is a great way to protect your work. However, simple scripts that create a new directory for every command will break stateful workflows (e.g., git init followed by git commit won’t work if they run in different directories).\nTo fix this, we need a stateful sandbox that persists across tool calls. We can use environment variables provided by Lectic to identify the session.\nCreate ~/.config/lectic/shadow-run.sh:\n#!/bin/bash\nset -euo pipefail\n\n# 1. Generate a stable path for this project + interlocutor\n# Using a hash of the current directory ensures we get a unique sandbox per project\nPROJ_HASH=$(echo -n \"$PWD\" | md5sum | awk '{print $1}')\nSANDBOX_ROOT=\"${TMPDIR:-/tmp}/lectic-sandbox-${PROJ_HASH}\"\n# LECTIC_INTERLOCUTOR is provided by Lectic\nSANDBOX_DIR=\"$SANDBOX_ROOT/${LECTIC_INTERLOCUTOR:-default}\"\n\n# 2. Initialize the sandbox if it's new\nif [[ ! -d \"$SANDBOX_DIR\" ]]; then\n  echo \"Initializing sandbox at $SANDBOX_DIR...\" &gt;&2\n  mkdir -p \"$SANDBOX_DIR\"\n  # Copy the project to the sandbox\n  cp -r . \"$SANDBOX_DIR\"\nfi\n\ncd \"$SANDBOX_DIR\"\n\n# 3. Run the command\n\"$@\"\nThis script creates a “shadow” copy of your project that persists as long as you don’t delete the temporary directory. The assistant can make changes, run builds, and edit files without affecting your real project. If you like the results, you can manually copy them back.\n\n\n\nYou can apply sandboxes project-wide (top-level), per-interlocutor, or per- tool.\nPrecedence is:\n\nTool sandbox\nInterlocutor sandbox\nTop-level sandbox\n\nProject-wide default (recommended for per-project configs): This wraps all exec tools and local mcp_command tools in the project, across all interlocutors.\nsandbox: ~/.config/lectic/safe-run.sh\n\ninterlocutor:\n  name: Assistant\n  tools:\n    - exec: bash\n    - exec: python3\nPer-interlocutor default: Useful when different interlocutors need different isolation.\ninterlocutor:\n  name: Assistant\n  sandbox: ~/.config/lectic/safe-run.sh\n  tools:\n    - exec: bash\n    - exec: python3\nPer-tool: Useful if you have a specific “dangerous” tool that needs isolation while others (like ls) can run natively.\ntools:\n  - exec: rm\n    name: delete_files\n    sandbox: ~/.config/lectic/safe-run.sh\n  - exec: ls\n    name: list_files\n    # No sandbox\n\n\n\n\nEnvironment variables: Lectic passes variables like LECTIC_INTERLOCUTOR into sandboxed commands. Use these for per-interlocutor state (for example, separate scratch directories). See Exec Tool and Configuration Reference.\nQuoting and arguments: A sandbox is a command string. If you need complex quoting or structured options, prefer writing a wrapper script.\nPerformance matters: Tools can be called in tight loops. Heavy sandboxes like docker run can add significant latency. Prefer docker exec into a long-running container if you go the Docker route.\nTest your sandbox: Verify it blocks what you think it blocks. Try to access files outside the allowed roots and confirm it fails.",
    "crumbs": [
      "Cookbook",
      "Custom Sandboxes"
    ]
  },
  {
    "objectID": "cookbook/06_custom_sandboxing.html#the-sandbox-protocol",
    "href": "cookbook/06_custom_sandboxing.html#the-sandbox-protocol",
    "title": "Recipe: Custom Sandboxing",
    "section": "",
    "text": "When you configure a sandbox in your lectic.yaml, Lectic wraps the execution of exec tools and local mcp_command tools. Instead of executing the tool directly, it executes your sandbox script and passes the tool’s command and arguments as arguments to that script.\nAn exec tool without sandbox: Lectic runs: ls -la\nWith sandbox: ./wrapper.sh: Lectic runs: ./wrapper.sh ls -la\nThe command to launch an MCP server is wrapped by the sandbox script in a similar way.\nYour sandbox script is responsible for:\n\nSetting up the environment.\nExecuting the command (passed in $@).\nCleaning up.\nReturning the exit code.",
    "crumbs": [
      "Cookbook",
      "Custom Sandboxes"
    ]
  },
  {
    "objectID": "cookbook/06_custom_sandboxing.html#level-1-observability-wrapper",
    "href": "cookbook/06_custom_sandboxing.html#level-1-observability-wrapper",
    "title": "Recipe: Custom Sandboxing",
    "section": "",
    "text": "Before trying to isolate the filesystem, let’s make a wrapper that simply logs every command the assistant tries to run. This is useful for auditing.\nCreate ~/.config/lectic/audit.sh:\n#!/bin/bash\n# Append timestamp and command to a log file\necho \"[$(date)] Executing: $*\" &gt;&gt; \"$HOME/.lectic_audit_log\"\n\n# Run the actual command\nexec \"$@\"\nMake it executable:\nchmod +x ~/.config/lectic/audit.sh\nConfigure it in lectic.yaml:\ninterlocutor:\n  name: Assistant\n  # Apply to all exec/local MCP tools for this interlocutor\n  sandbox: ~/.config/lectic/audit.sh\n  tools:\n    - exec: ls",
    "crumbs": [
      "Cookbook",
      "Custom Sandboxes"
    ]
  },
  {
    "objectID": "cookbook/06_custom_sandboxing.html#level-2-filesystem-isolation-bubblewrap",
    "href": "cookbook/06_custom_sandboxing.html#level-2-filesystem-isolation-bubblewrap",
    "title": "Recipe: Custom Sandboxing",
    "section": "",
    "text": "For actual safety, we can use Bubblewrap (bwrap). This tool creates a new namespace for the process, allowing you to control exactly which parts of your filesystem the assistant can see or write to.\nHere is a simplified version of the lectic-bwrap script found in the Lectic repository. It creates a read-only view of the system but gives the assistant a temporary, empty home directory.\nCreate ~/.config/lectic/safe-run.sh:\n#!/bin/bash\nset -euo pipefail\n\n# Create a temporary directory for the assistant's \"home\"\nFAKE_HOME=$(mktemp -d)\n\n# Ensure we clean up the temp dir when the script exits\ntrap 'rm -rf \"$FAKE_HOME\"' EXIT\n\n# Run bwrap with specific permissions\nbwrap \\\n  --ro-bind / / \\                 # Mount the root as read-only\n  --dev /dev \\                    # legitimate devices\n  --proc /proc \\                  # legitimate processes\n  --bind \"$PWD\" \"$PWD\" \\          # Allow read-write access to current project\n  --bind \"$FAKE_HOME\" \"$HOME\" \\   # Fake the home directory\n  --unshare-net \\                 # Disable network access (optional)\n  --die-with-parent \\             # Kill process if lectic dies\n  \"$@\"\n\n\n\nRead-only Root: The assistant cannot modify system files (/usr, /bin, etc.).\nFake Home: If the assistant runs rm -rf ~, it only deletes the temporary directory, not your actual home folder.\nProject Access: The script explicitly binds $PWD, so the assistant can still read and write files in the directory where you ran Lectic.\nNo Network: The --unshare-net flag prevents the assistant from making outbound connections (remove this if you want it to use curl or something similar).",
    "crumbs": [
      "Cookbook",
      "Custom Sandboxes"
    ]
  },
  {
    "objectID": "cookbook/06_custom_sandboxing.html#level-3-stateful-isolation-the-shadow-workspace",
    "href": "cookbook/06_custom_sandboxing.html#level-3-stateful-isolation-the-shadow-workspace",
    "title": "Recipe: Custom Sandboxing",
    "section": "",
    "text": "Isolating the filesystem by copying the project to a temporary directory is a great way to protect your work. However, simple scripts that create a new directory for every command will break stateful workflows (e.g., git init followed by git commit won’t work if they run in different directories).\nTo fix this, we need a stateful sandbox that persists across tool calls. We can use environment variables provided by Lectic to identify the session.\nCreate ~/.config/lectic/shadow-run.sh:\n#!/bin/bash\nset -euo pipefail\n\n# 1. Generate a stable path for this project + interlocutor\n# Using a hash of the current directory ensures we get a unique sandbox per project\nPROJ_HASH=$(echo -n \"$PWD\" | md5sum | awk '{print $1}')\nSANDBOX_ROOT=\"${TMPDIR:-/tmp}/lectic-sandbox-${PROJ_HASH}\"\n# LECTIC_INTERLOCUTOR is provided by Lectic\nSANDBOX_DIR=\"$SANDBOX_ROOT/${LECTIC_INTERLOCUTOR:-default}\"\n\n# 2. Initialize the sandbox if it's new\nif [[ ! -d \"$SANDBOX_DIR\" ]]; then\n  echo \"Initializing sandbox at $SANDBOX_DIR...\" &gt;&2\n  mkdir -p \"$SANDBOX_DIR\"\n  # Copy the project to the sandbox\n  cp -r . \"$SANDBOX_DIR\"\nfi\n\ncd \"$SANDBOX_DIR\"\n\n# 3. Run the command\n\"$@\"\nThis script creates a “shadow” copy of your project that persists as long as you don’t delete the temporary directory. The assistant can make changes, run builds, and edit files without affecting your real project. If you like the results, you can manually copy them back.",
    "crumbs": [
      "Cookbook",
      "Custom Sandboxes"
    ]
  },
  {
    "objectID": "cookbook/06_custom_sandboxing.html#configuration-usage",
    "href": "cookbook/06_custom_sandboxing.html#configuration-usage",
    "title": "Recipe: Custom Sandboxing",
    "section": "",
    "text": "You can apply sandboxes project-wide (top-level), per-interlocutor, or per- tool.\nPrecedence is:\n\nTool sandbox\nInterlocutor sandbox\nTop-level sandbox\n\nProject-wide default (recommended for per-project configs): This wraps all exec tools and local mcp_command tools in the project, across all interlocutors.\nsandbox: ~/.config/lectic/safe-run.sh\n\ninterlocutor:\n  name: Assistant\n  tools:\n    - exec: bash\n    - exec: python3\nPer-interlocutor default: Useful when different interlocutors need different isolation.\ninterlocutor:\n  name: Assistant\n  sandbox: ~/.config/lectic/safe-run.sh\n  tools:\n    - exec: bash\n    - exec: python3\nPer-tool: Useful if you have a specific “dangerous” tool that needs isolation while others (like ls) can run natively.\ntools:\n  - exec: rm\n    name: delete_files\n    sandbox: ~/.config/lectic/safe-run.sh\n  - exec: ls\n    name: list_files\n    # No sandbox",
    "crumbs": [
      "Cookbook",
      "Custom Sandboxes"
    ]
  },
  {
    "objectID": "cookbook/06_custom_sandboxing.html#tips",
    "href": "cookbook/06_custom_sandboxing.html#tips",
    "title": "Recipe: Custom Sandboxing",
    "section": "",
    "text": "Environment variables: Lectic passes variables like LECTIC_INTERLOCUTOR into sandboxed commands. Use these for per-interlocutor state (for example, separate scratch directories). See Exec Tool and Configuration Reference.\nQuoting and arguments: A sandbox is a command string. If you need complex quoting or structured options, prefer writing a wrapper script.\nPerformance matters: Tools can be called in tight loops. Heavy sandboxes like docker run can add significant latency. Prefer docker exec into a long-running container if you go the Docker route.\nTest your sandbox: Verify it blocks what you think it blocks. Try to access files outside the allowed roots and confirm it fails.",
    "crumbs": [
      "Cookbook",
      "Custom Sandboxes"
    ]
  },
  {
    "objectID": "cookbook/04_memory.html",
    "href": "cookbook/04_memory.html",
    "title": "Recipe: Conversation Memory",
    "section": "",
    "text": "This recipe shows two approaches to giving your assistant memory across conversations: explicit recall via a tool, and automatic context via hooks. These serve different purposes and have different tradeoffs.\n\n\nIn this approach, everything is recorded automatically, but the assistant must explicitly search for relevant memories. This keeps the prompt lean and preserves cache efficiency.\n\n\nAdd this hook to save every message:\nhooks:\n  - on: [user_message, assistant_message]\n    do: |\n      #!/bin/bash\n      set -euo pipefail\n      \n      DB=\"${LECTIC_DATA}/memory.sqlite3\"\n      \n      # Initialize database if needed\n      sqlite3 \"$DB\" &lt;&lt;'SQL'\n      CREATE TABLE IF NOT EXISTS messages (\n        id INTEGER PRIMARY KEY,\n        timestamp TEXT NOT NULL,\n        role TEXT NOT NULL,\n        interlocutor TEXT,\n        file TEXT,\n        content TEXT NOT NULL\n      );\n      CREATE INDEX IF NOT EXISTS idx_timestamp ON messages(timestamp);\n      SQL\n      \n      # Determine role and content\n      if [[ -n \"${ASSISTANT_MESSAGE:-}\" ]]; then\n        ROLE=\"assistant\"\n        CONTENT=\"$ASSISTANT_MESSAGE\"\n        NAME=\"${LECTIC_INTERLOCUTOR:-}\"\n      else\n        ROLE=\"user\"\n        CONTENT=\"${USER_MESSAGE:-}\"\n        NAME=\"\"\n      fi\n      \n      # Escape single quotes for SQL\n      CONTENT_ESC=\"${CONTENT//\\'/\\'\\'}\"\n      NAME_ESC=\"${NAME//\\'/\\'\\'}\"\n      FILE_ESC=\"${LECTIC_FILE//\\'/\\'\\'}\"\n      \n      sqlite3 \"$DB\" &lt;&lt;SQL\n      INSERT INTO messages (timestamp, role, interlocutor, file, content)\n      VALUES (datetime('now'), '$ROLE', '$NAME_ESC', '$FILE_ESC', '$CONTENT_ESC');\n      SQL\n\n\n\nGive the assistant a tool to search memory:\ntools:\n  - name: search_memory\n    usage: |\n      Search past conversation history. Use this when the user\n      references something from a previous conversation or when\n      context from past discussions would be helpful.\n    exec: |\n      #!/bin/bash\n      sqlite3 \"${LECTIC_DATA}/memory.sqlite3\" &lt;&lt;SQL\n      SELECT printf('[%s] %s: %s', timestamp, role, content)\n      FROM messages\n      WHERE content LIKE '%${QUERY}%'\n      ORDER BY timestamp DESC\n      LIMIT 10;\n      SQL\n    schema:\n      QUERY: The search term to look for in past messages.\n\n\n\n\nLong-running assistants where most turns don’t need memory\nWhen you want the assistant to decide what’s relevant\nWhen cache efficiency matters (the prompt stays constant)\n\n\n\n\n\nIn this approach, relevant memories are automatically injected into every prompt. Nothing is recorded automatically — instead, the assistant has a tool to explicitly remember things.\n\n\ntools:\n  - name: remember\n    usage: |\n      Store important information for future reference. Use this when\n      the user shares preferences, makes decisions, or provides context\n      that should persist across conversations.\n    exec: |\n      #!/bin/bash\n      set -euo pipefail\n      \n      DB=\"${LECTIC_DATA}/memory.sqlite3\"\n      \n      sqlite3 \"$DB\" &lt;&lt;'SQL'\n      CREATE TABLE IF NOT EXISTS memories (\n        id INTEGER PRIMARY KEY,\n        timestamp TEXT NOT NULL,\n        content TEXT NOT NULL\n      );\n      SQL\n      \n      CONTENT_ESC=\"${CONTENT//\\'/\\'\\'}\"\n      \n      sqlite3 \"$DB\" &lt;&lt;SQL\n      INSERT INTO memories (timestamp, content)\n      VALUES (datetime('now'), '$CONTENT_ESC');\n      SQL\n      \n      echo \"Remembered.\"\n    schema:\n      CONTENT: The information to remember.\n\n\n\nUse an exec: prompt to inject stored memories:\ninterlocutor:\n  name: Assistant\n  prompt: |\n    exec:#!/bin/bash\n    cat &lt;&lt;'PROMPT'\n    You are a helpful assistant with access to stored memories.\n    \n    Things you've been asked to remember:\n    PROMPT\n    \n    DB=\"${LECTIC_DATA}/memory.sqlite3\"\n    if [[ -f \"$DB\" ]]; then\n      sqlite3 \"$DB\" &lt;&lt;'SQL'\n      SELECT printf('- %s', content)\n      FROM memories\n      ORDER BY timestamp DESC\n      LIMIT 20;\n      SQL\n    else\n      echo \"(No memories yet)\"\n    fi\n    \n    echo \"\"\n    echo \"Use the remember tool to store new information.\"\n\n\n\n\nWhen you want explicit control over what’s remembered\nFor preferences and decisions rather than conversation history\nWhen memories are small and frequently relevant\n\n\n\n\n\n\nDon’t mix these approaches carelessly. Recording everything and injecting it into the prompt will bloat your context and hurt cache performance.\nBe selective about what you store. Tool call results and verbose outputs can bloat the database quickly.\nConsider privacy. Memory persists across sessions. Don’t store sensitive information you wouldn’t want retrieved later.\nMonitor database size. Add a periodic cleanup job or a forget tool for manual pruning.\n\n\n\ntools:\n  - name: forget\n    usage: Remove a specific memory by its content.\n    exec: |\n      #!/bin/bash\n      sqlite3 \"${LECTIC_DATA}/memory.sqlite3\" &lt;&lt;SQL\n      DELETE FROM memories WHERE content LIKE '%${PATTERN}%';\n      SELECT 'Deleted ' || changes() || ' memories';\n      SQL\n    schema:\n      PATTERN: Pattern to match memories to delete.",
    "crumbs": [
      "Cookbook",
      "Conversation Memory"
    ]
  },
  {
    "objectID": "cookbook/04_memory.html#approach-1-memory-as-a-tool",
    "href": "cookbook/04_memory.html#approach-1-memory-as-a-tool",
    "title": "Recipe: Conversation Memory",
    "section": "",
    "text": "In this approach, everything is recorded automatically, but the assistant must explicitly search for relevant memories. This keeps the prompt lean and preserves cache efficiency.\n\n\nAdd this hook to save every message:\nhooks:\n  - on: [user_message, assistant_message]\n    do: |\n      #!/bin/bash\n      set -euo pipefail\n      \n      DB=\"${LECTIC_DATA}/memory.sqlite3\"\n      \n      # Initialize database if needed\n      sqlite3 \"$DB\" &lt;&lt;'SQL'\n      CREATE TABLE IF NOT EXISTS messages (\n        id INTEGER PRIMARY KEY,\n        timestamp TEXT NOT NULL,\n        role TEXT NOT NULL,\n        interlocutor TEXT,\n        file TEXT,\n        content TEXT NOT NULL\n      );\n      CREATE INDEX IF NOT EXISTS idx_timestamp ON messages(timestamp);\n      SQL\n      \n      # Determine role and content\n      if [[ -n \"${ASSISTANT_MESSAGE:-}\" ]]; then\n        ROLE=\"assistant\"\n        CONTENT=\"$ASSISTANT_MESSAGE\"\n        NAME=\"${LECTIC_INTERLOCUTOR:-}\"\n      else\n        ROLE=\"user\"\n        CONTENT=\"${USER_MESSAGE:-}\"\n        NAME=\"\"\n      fi\n      \n      # Escape single quotes for SQL\n      CONTENT_ESC=\"${CONTENT//\\'/\\'\\'}\"\n      NAME_ESC=\"${NAME//\\'/\\'\\'}\"\n      FILE_ESC=\"${LECTIC_FILE//\\'/\\'\\'}\"\n      \n      sqlite3 \"$DB\" &lt;&lt;SQL\n      INSERT INTO messages (timestamp, role, interlocutor, file, content)\n      VALUES (datetime('now'), '$ROLE', '$NAME_ESC', '$FILE_ESC', '$CONTENT_ESC');\n      SQL\n\n\n\nGive the assistant a tool to search memory:\ntools:\n  - name: search_memory\n    usage: |\n      Search past conversation history. Use this when the user\n      references something from a previous conversation or when\n      context from past discussions would be helpful.\n    exec: |\n      #!/bin/bash\n      sqlite3 \"${LECTIC_DATA}/memory.sqlite3\" &lt;&lt;SQL\n      SELECT printf('[%s] %s: %s', timestamp, role, content)\n      FROM messages\n      WHERE content LIKE '%${QUERY}%'\n      ORDER BY timestamp DESC\n      LIMIT 10;\n      SQL\n    schema:\n      QUERY: The search term to look for in past messages.\n\n\n\n\nLong-running assistants where most turns don’t need memory\nWhen you want the assistant to decide what’s relevant\nWhen cache efficiency matters (the prompt stays constant)",
    "crumbs": [
      "Cookbook",
      "Conversation Memory"
    ]
  },
  {
    "objectID": "cookbook/04_memory.html#approach-2-automatic-context-injection",
    "href": "cookbook/04_memory.html#approach-2-automatic-context-injection",
    "title": "Recipe: Conversation Memory",
    "section": "",
    "text": "In this approach, relevant memories are automatically injected into every prompt. Nothing is recorded automatically — instead, the assistant has a tool to explicitly remember things.\n\n\ntools:\n  - name: remember\n    usage: |\n      Store important information for future reference. Use this when\n      the user shares preferences, makes decisions, or provides context\n      that should persist across conversations.\n    exec: |\n      #!/bin/bash\n      set -euo pipefail\n      \n      DB=\"${LECTIC_DATA}/memory.sqlite3\"\n      \n      sqlite3 \"$DB\" &lt;&lt;'SQL'\n      CREATE TABLE IF NOT EXISTS memories (\n        id INTEGER PRIMARY KEY,\n        timestamp TEXT NOT NULL,\n        content TEXT NOT NULL\n      );\n      SQL\n      \n      CONTENT_ESC=\"${CONTENT//\\'/\\'\\'}\"\n      \n      sqlite3 \"$DB\" &lt;&lt;SQL\n      INSERT INTO memories (timestamp, content)\n      VALUES (datetime('now'), '$CONTENT_ESC');\n      SQL\n      \n      echo \"Remembered.\"\n    schema:\n      CONTENT: The information to remember.\n\n\n\nUse an exec: prompt to inject stored memories:\ninterlocutor:\n  name: Assistant\n  prompt: |\n    exec:#!/bin/bash\n    cat &lt;&lt;'PROMPT'\n    You are a helpful assistant with access to stored memories.\n    \n    Things you've been asked to remember:\n    PROMPT\n    \n    DB=\"${LECTIC_DATA}/memory.sqlite3\"\n    if [[ -f \"$DB\" ]]; then\n      sqlite3 \"$DB\" &lt;&lt;'SQL'\n      SELECT printf('- %s', content)\n      FROM memories\n      ORDER BY timestamp DESC\n      LIMIT 20;\n      SQL\n    else\n      echo \"(No memories yet)\"\n    fi\n    \n    echo \"\"\n    echo \"Use the remember tool to store new information.\"\n\n\n\n\nWhen you want explicit control over what’s remembered\nFor preferences and decisions rather than conversation history\nWhen memories are small and frequently relevant",
    "crumbs": [
      "Cookbook",
      "Conversation Memory"
    ]
  },
  {
    "objectID": "cookbook/04_memory.html#tips",
    "href": "cookbook/04_memory.html#tips",
    "title": "Recipe: Conversation Memory",
    "section": "",
    "text": "Don’t mix these approaches carelessly. Recording everything and injecting it into the prompt will bloat your context and hurt cache performance.\nBe selective about what you store. Tool call results and verbose outputs can bloat the database quickly.\nConsider privacy. Memory persists across sessions. Don’t store sensitive information you wouldn’t want retrieved later.\nMonitor database size. Add a periodic cleanup job or a forget tool for manual pruning.\n\n\n\ntools:\n  - name: forget\n    usage: Remove a specific memory by its content.\n    exec: |\n      #!/bin/bash\n      sqlite3 \"${LECTIC_DATA}/memory.sqlite3\" &lt;&lt;SQL\n      DELETE FROM memories WHERE content LIKE '%${PATTERN}%';\n      SELECT 'Deleted ' || changes() || ' memories';\n      SQL\n    schema:\n      PATTERN: Pattern to match memories to delete.",
    "crumbs": [
      "Cookbook",
      "Conversation Memory"
    ]
  },
  {
    "objectID": "cookbook/03_research_perspectives.html",
    "href": "cookbook/03_research_perspectives.html",
    "title": "Recipe: Research with Multiple Perspectives",
    "section": "",
    "text": "This recipe shows how to use multiple interlocutors to explore a topic from different angles. One interlocutor does research, another provides critique, and you can quickly get second opinions without derailing the main conversation.\n\n\n---\ninterlocutors:\n  - name: Researcher\n    prompt: |\n      You are a thorough researcher. When exploring a topic:\n      - Consider multiple sources and viewpoints\n      - Note uncertainties and limitations\n      - Suggest follow-up questions\n    provider: anthropic\n    model: claude-sonnet-4-20250514\n    tools:\n      - native: search\n      - think_about: &gt;\n          What are the key questions here? What might I be missing?\n          What assumptions am I making?\n\n  - name: Critic  \n    prompt: |\n      You are a skeptical critic. Your job is to:\n      - Challenge assumptions and weak arguments\n      - Point out missing evidence or alternative explanations\n      - Steelman opposing viewpoints\n      Be constructive but rigorous.\n    provider: anthropic\n    model: claude-sonnet-4-20250514\n\n  - name: Synthesizer\n    prompt: |\n      You synthesize discussions into clear summaries. Focus on:\n      - Key points of agreement and disagreement\n      - Open questions that remain\n      - Actionable conclusions\n    provider: anthropic\n    model: claude-3-haiku-20240307\n---\n\n\n\n\n\n:ask[Researcher] I'm trying to understand the tradeoffs between\nmicroservices and monolithic architectures for a team of 5 developers\nbuilding a B2B SaaS product.\nThe Researcher will explore the topic, potentially using web search and their thinking tool.\n\n\n\nUse :aside to get feedback without switching the main conversation:\n:aside[Critic] What's wrong with this analysis?\nThe Critic responds, then the next message goes back to the Researcher automatically.\n\n\n\n:ask[Critic] Let's dig into the claim about \"complexity tax.\" What's\nthe actual evidence here?\nNow you’re in a conversation with the Critic until you switch again.\n\n\n\n:ask[Synthesizer] Summarize this discussion. What did we learn? What\nshould we do next?\n\n\n\n\n\n\ninterlocutors:\n  - name: Legal\n    prompt: You are a legal expert. Focus on regulatory compliance...\n  - name: Technical\n    prompt: You are a senior engineer. Focus on implementation...\n  - name: Business\n    prompt: You are a business strategist. Focus on market fit...\n\n\n\nHave one interlocutor call another as a tool:\ninterlocutors:\n  - name: Lead\n    prompt: You coordinate research. Delegate to specialists.\n    tools:\n      - agent: Researcher\n        name: research\n        usage: Get detailed research on a specific topic.\n      - agent: Critic\n        name: critique\n        usage: Get critical analysis of a claim or argument.\n  \n  - name: Researcher\n    prompt: ...\n    tools:\n      - native: search\n  \n  - name: Critic\n    prompt: ...\nNow the Lead can autonomously decide when to delegate:\n:ask[Lead] Evaluate whether we should migrate from PostgreSQL to\nCockroachDB for our multi-region deployment.\n\n\n\nUse cheaper/faster models for quick checks:\ninterlocutors:\n  - name: Deep\n    model: claude-sonnet-4-20250514  # For complex analysis\n    \n  - name: Quick\n    model: claude-3-haiku-20240307  # For quick sanity checks\n\n\n\n\n\nUse :aside liberally. It’s cheap to get a second opinion without losing your place in the main conversation.\nGive each interlocutor a distinct voice. The prompts should produce noticeably different responses, otherwise there’s no point in having multiple speakers.\nUse :reset[] when switching topics. If you’re starting a new line of inquiry, reset context to avoid confusion from earlier discussion.",
    "crumbs": [
      "Cookbook",
      "Research Perspectives"
    ]
  },
  {
    "objectID": "cookbook/03_research_perspectives.html#the-setup",
    "href": "cookbook/03_research_perspectives.html#the-setup",
    "title": "Recipe: Research with Multiple Perspectives",
    "section": "",
    "text": "---\ninterlocutors:\n  - name: Researcher\n    prompt: |\n      You are a thorough researcher. When exploring a topic:\n      - Consider multiple sources and viewpoints\n      - Note uncertainties and limitations\n      - Suggest follow-up questions\n    provider: anthropic\n    model: claude-sonnet-4-20250514\n    tools:\n      - native: search\n      - think_about: &gt;\n          What are the key questions here? What might I be missing?\n          What assumptions am I making?\n\n  - name: Critic  \n    prompt: |\n      You are a skeptical critic. Your job is to:\n      - Challenge assumptions and weak arguments\n      - Point out missing evidence or alternative explanations\n      - Steelman opposing viewpoints\n      Be constructive but rigorous.\n    provider: anthropic\n    model: claude-sonnet-4-20250514\n\n  - name: Synthesizer\n    prompt: |\n      You synthesize discussions into clear summaries. Focus on:\n      - Key points of agreement and disagreement\n      - Open questions that remain\n      - Actionable conclusions\n    provider: anthropic\n    model: claude-3-haiku-20240307\n---",
    "crumbs": [
      "Cookbook",
      "Research Perspectives"
    ]
  },
  {
    "objectID": "cookbook/03_research_perspectives.html#usage",
    "href": "cookbook/03_research_perspectives.html#usage",
    "title": "Recipe: Research with Multiple Perspectives",
    "section": "",
    "text": ":ask[Researcher] I'm trying to understand the tradeoffs between\nmicroservices and monolithic architectures for a team of 5 developers\nbuilding a B2B SaaS product.\nThe Researcher will explore the topic, potentially using web search and their thinking tool.\n\n\n\nUse :aside to get feedback without switching the main conversation:\n:aside[Critic] What's wrong with this analysis?\nThe Critic responds, then the next message goes back to the Researcher automatically.\n\n\n\n:ask[Critic] Let's dig into the claim about \"complexity tax.\" What's\nthe actual evidence here?\nNow you’re in a conversation with the Critic until you switch again.\n\n\n\n:ask[Synthesizer] Summarize this discussion. What did we learn? What\nshould we do next?",
    "crumbs": [
      "Cookbook",
      "Research Perspectives"
    ]
  },
  {
    "objectID": "cookbook/03_research_perspectives.html#variations",
    "href": "cookbook/03_research_perspectives.html#variations",
    "title": "Recipe: Research with Multiple Perspectives",
    "section": "",
    "text": "interlocutors:\n  - name: Legal\n    prompt: You are a legal expert. Focus on regulatory compliance...\n  - name: Technical\n    prompt: You are a senior engineer. Focus on implementation...\n  - name: Business\n    prompt: You are a business strategist. Focus on market fit...\n\n\n\nHave one interlocutor call another as a tool:\ninterlocutors:\n  - name: Lead\n    prompt: You coordinate research. Delegate to specialists.\n    tools:\n      - agent: Researcher\n        name: research\n        usage: Get detailed research on a specific topic.\n      - agent: Critic\n        name: critique\n        usage: Get critical analysis of a claim or argument.\n  \n  - name: Researcher\n    prompt: ...\n    tools:\n      - native: search\n  \n  - name: Critic\n    prompt: ...\nNow the Lead can autonomously decide when to delegate:\n:ask[Lead] Evaluate whether we should migrate from PostgreSQL to\nCockroachDB for our multi-region deployment.\n\n\n\nUse cheaper/faster models for quick checks:\ninterlocutors:\n  - name: Deep\n    model: claude-sonnet-4-20250514  # For complex analysis\n    \n  - name: Quick\n    model: claude-3-haiku-20240307  # For quick sanity checks",
    "crumbs": [
      "Cookbook",
      "Research Perspectives"
    ]
  },
  {
    "objectID": "cookbook/03_research_perspectives.html#tips",
    "href": "cookbook/03_research_perspectives.html#tips",
    "title": "Recipe: Research with Multiple Perspectives",
    "section": "",
    "text": "Use :aside liberally. It’s cheap to get a second opinion without losing your place in the main conversation.\nGive each interlocutor a distinct voice. The prompts should produce noticeably different responses, otherwise there’s no point in having multiple speakers.\nUse :reset[] when switching topics. If you’re starting a new line of inquiry, reset context to avoid confusion from earlier discussion.",
    "crumbs": [
      "Cookbook",
      "Research Perspectives"
    ]
  },
  {
    "objectID": "cookbook/05_context_compaction.html",
    "href": "cookbook/05_context_compaction.html",
    "title": "Recipe: Context Compaction",
    "section": "",
    "text": "This recipe shows how to automatically handle long conversations by summarizing and resetting context when token usage gets high.\n\n\nLong conversations hit context limits. Before that happens, you want to:\n\nSummarize what’s been discussed\nReset the context window\nContinue with the summary as the new starting point\n\n\n\n\nThis hook monitors token usage and triggers compaction when a threshold is reached:\nhooks:\n  - on: assistant_message\n    inline: true\n    do: |\n      #!/bin/bash\n      \n      # Configuration\n      LIMIT=80000        # Trigger compaction at this token count\n      \n      TOTAL=\"${TOKEN_USAGE_TOTAL:-0}\"\n      \n      if [[ $TOTAL -lt $LIMIT ]]; then\n        exit 0  # No output, no action\n      fi\n      \n      # The conversation body comes in on stdin.\n      # Indent it as a code block for the summarizer.\n      CONVERSATION=$(sed 's/^/    /')\n      \n      # Generate summary using a separate lectic call\n      SUMMARY=$(echo \"$CONVERSATION\" | lectic -f ~/.config/lectic/summarize.lec -S)\n      \n      # Output with reset header\n      echo \"LECTIC:reset\"\n      echo \"LECTIC:final\"\n      echo \"\"\n      echo \"**Context compacted at $TOTAL tokens.**\"\n      echo \"\"\n      echo \"Summary of previous discussion:\"\n      echo \"$SUMMARY\"\nThe LECTIC:reset header clears prior context. The LECTIC:final header prevents the assistant from responding to the compaction notice itself.\n\n\n\nCreate ~/.config/lectic/summarize.lec:\n---\ninterlocutor:\n  name: Summarizer\n  prompt: |\n    Summarize the following conversation concisely. Include:\n    - Key decisions made\n    - Important information established\n    - Current task or question being addressed\n    - Any pending items or open threads\n    \n    Be thorough but concise. This summary will be the only context\n    available for continuing the conversation.\n  model: claude-3-haiku-20240307\n  max_tokens: 1000\n---\n\nSummarize this conversation:\nThe conversation text is piped to stdin and appended to the prompt file’s content.\n\n\n\n\nAfter each assistant message, the hook checks TOKEN_USAGE_TOTAL\nThe conversation body so far is available on stdin\nIf over the limit, the hook pipes the conversation to a separate lectic instance for summarization\nThe summary is output with the reset header, clearing old context\nThe next turn starts fresh with only the summary as context\n\n\n\n\n\n\nInstead of automatic triggering, add a macro:\nmacros:\n  - name: compact\n    expansion: |\n      exec:#!/bin/bash\n      echo \":reset[]\"\n      echo \"\"\n      echo \"Please summarize our conversation so far, focusing on key\"\n      echo \"decisions, important context, and any open questions.\"\nUsage: :compact[]\nThis asks the current assistant to summarize before resetting, rather than using a separate summarization call.\n\n\n\nSave the full conversation before resetting:\nhooks:\n  - on: assistant_message\n    inline: true\n    do: |\n      #!/bin/bash\n      TOTAL=\"${TOKEN_USAGE_TOTAL:-0}\"\n      LIMIT=80000\n      \n      if [[ $TOTAL -lt $LIMIT ]]; then\n        exit 0\n      fi\n      \n      # Archive the current conversation\n      ARCHIVE_DIR=\"${LECTIC_DATA}/archives\"\n      mkdir -p \"$ARCHIVE_DIR\"\n      \n      TIMESTAMP=$(date +%Y%m%d-%H%M%S)\n      BASE=$(basename \"${LECTIC_FILE:-.lec}\" .lec)\n      ARCHIVE_FILE=\"$ARCHIVE_DIR/$BASE-$TIMESTAMP.lec\"\n      \n      # Save stdin (the conversation) to the archive\n      cat &gt; \"$ARCHIVE_FILE\"\n      \n      # Now generate summary (re-read from archive since stdin is consumed)\n      SUMMARY=$(sed 's/^/    /' \"$ARCHIVE_FILE\" | \\\n                lectic -f ~/.config/lectic/summarize.lec -S)\n      \n      echo \"LECTIC:reset\"\n      echo \"LECTIC:final\"\n      echo \"\"\n      echo \"**Context compacted. Archived to: $ARCHIVE_FILE**\"\n      echo \"\"\n      echo \"Summary:\"\n      echo \"$SUMMARY\"\n\n\n\nGive a warning at 70% capacity, compact at 90%:\nhooks:\n  - on: assistant_message\n    inline: true\n    do: |\n      #!/bin/bash\n      TOTAL=\"${TOKEN_USAGE_TOTAL:-0}\"\n      WARN_LIMIT=70000\n      HARD_LIMIT=90000\n      \n      if [[ $TOTAL -gt $HARD_LIMIT ]]; then\n        # Full compaction\n        CONVERSATION=$(sed 's/^/    /')\n        SUMMARY=$(echo \"$CONVERSATION\" | \\\n                  lectic -f ~/.config/lectic/summarize.lec -S)\n        echo \"LECTIC:reset\"\n        echo \"LECTIC:final\"\n        echo \"\"\n        echo \"**Context limit reached. Compacted.**\"\n        echo \"\"\n        echo \"$SUMMARY\"\n      elif [[ $TOTAL -gt $WARN_LIMIT ]]; then\n        # Just warn\n        cat &gt; /dev/null  # Consume stdin\n        echo \"LECTIC:final\"\n        echo \"\"\n        echo \"*Note: Context at ${TOTAL} tokens. Consider* :reset[] *soon.*\"\n      fi\n\n\n\n\n\nTest your summarizer. A bad summary loses important context. Try it manually on a few conversations first.\nUse a fast model for summarization. Haiku or similar is fine for summaries and keeps compaction quick.\nConsider what to preserve. Code snippets, file paths, and specific decisions are often more important than general discussion.\nMonitor compaction frequency. If you’re compacting every few messages, your conversations might be too verbose or you might need a larger context model.",
    "crumbs": [
      "Cookbook",
      "Context Compaction"
    ]
  },
  {
    "objectID": "cookbook/05_context_compaction.html#the-problem",
    "href": "cookbook/05_context_compaction.html#the-problem",
    "title": "Recipe: Context Compaction",
    "section": "",
    "text": "Long conversations hit context limits. Before that happens, you want to:\n\nSummarize what’s been discussed\nReset the context window\nContinue with the summary as the new starting point",
    "crumbs": [
      "Cookbook",
      "Context Compaction"
    ]
  },
  {
    "objectID": "cookbook/05_context_compaction.html#automatic-compaction-hook",
    "href": "cookbook/05_context_compaction.html#automatic-compaction-hook",
    "title": "Recipe: Context Compaction",
    "section": "",
    "text": "This hook monitors token usage and triggers compaction when a threshold is reached:\nhooks:\n  - on: assistant_message\n    inline: true\n    do: |\n      #!/bin/bash\n      \n      # Configuration\n      LIMIT=80000        # Trigger compaction at this token count\n      \n      TOTAL=\"${TOKEN_USAGE_TOTAL:-0}\"\n      \n      if [[ $TOTAL -lt $LIMIT ]]; then\n        exit 0  # No output, no action\n      fi\n      \n      # The conversation body comes in on stdin.\n      # Indent it as a code block for the summarizer.\n      CONVERSATION=$(sed 's/^/    /')\n      \n      # Generate summary using a separate lectic call\n      SUMMARY=$(echo \"$CONVERSATION\" | lectic -f ~/.config/lectic/summarize.lec -S)\n      \n      # Output with reset header\n      echo \"LECTIC:reset\"\n      echo \"LECTIC:final\"\n      echo \"\"\n      echo \"**Context compacted at $TOTAL tokens.**\"\n      echo \"\"\n      echo \"Summary of previous discussion:\"\n      echo \"$SUMMARY\"\nThe LECTIC:reset header clears prior context. The LECTIC:final header prevents the assistant from responding to the compaction notice itself.",
    "crumbs": [
      "Cookbook",
      "Context Compaction"
    ]
  },
  {
    "objectID": "cookbook/05_context_compaction.html#the-summarization-prompt",
    "href": "cookbook/05_context_compaction.html#the-summarization-prompt",
    "title": "Recipe: Context Compaction",
    "section": "",
    "text": "Create ~/.config/lectic/summarize.lec:\n---\ninterlocutor:\n  name: Summarizer\n  prompt: |\n    Summarize the following conversation concisely. Include:\n    - Key decisions made\n    - Important information established\n    - Current task or question being addressed\n    - Any pending items or open threads\n    \n    Be thorough but concise. This summary will be the only context\n    available for continuing the conversation.\n  model: claude-3-haiku-20240307\n  max_tokens: 1000\n---\n\nSummarize this conversation:\nThe conversation text is piped to stdin and appended to the prompt file’s content.",
    "crumbs": [
      "Cookbook",
      "Context Compaction"
    ]
  },
  {
    "objectID": "cookbook/05_context_compaction.html#how-it-works",
    "href": "cookbook/05_context_compaction.html#how-it-works",
    "title": "Recipe: Context Compaction",
    "section": "",
    "text": "After each assistant message, the hook checks TOKEN_USAGE_TOTAL\nThe conversation body so far is available on stdin\nIf over the limit, the hook pipes the conversation to a separate lectic instance for summarization\nThe summary is output with the reset header, clearing old context\nThe next turn starts fresh with only the summary as context",
    "crumbs": [
      "Cookbook",
      "Context Compaction"
    ]
  },
  {
    "objectID": "cookbook/05_context_compaction.html#variations",
    "href": "cookbook/05_context_compaction.html#variations",
    "title": "Recipe: Context Compaction",
    "section": "",
    "text": "Instead of automatic triggering, add a macro:\nmacros:\n  - name: compact\n    expansion: |\n      exec:#!/bin/bash\n      echo \":reset[]\"\n      echo \"\"\n      echo \"Please summarize our conversation so far, focusing on key\"\n      echo \"decisions, important context, and any open questions.\"\nUsage: :compact[]\nThis asks the current assistant to summarize before resetting, rather than using a separate summarization call.\n\n\n\nSave the full conversation before resetting:\nhooks:\n  - on: assistant_message\n    inline: true\n    do: |\n      #!/bin/bash\n      TOTAL=\"${TOKEN_USAGE_TOTAL:-0}\"\n      LIMIT=80000\n      \n      if [[ $TOTAL -lt $LIMIT ]]; then\n        exit 0\n      fi\n      \n      # Archive the current conversation\n      ARCHIVE_DIR=\"${LECTIC_DATA}/archives\"\n      mkdir -p \"$ARCHIVE_DIR\"\n      \n      TIMESTAMP=$(date +%Y%m%d-%H%M%S)\n      BASE=$(basename \"${LECTIC_FILE:-.lec}\" .lec)\n      ARCHIVE_FILE=\"$ARCHIVE_DIR/$BASE-$TIMESTAMP.lec\"\n      \n      # Save stdin (the conversation) to the archive\n      cat &gt; \"$ARCHIVE_FILE\"\n      \n      # Now generate summary (re-read from archive since stdin is consumed)\n      SUMMARY=$(sed 's/^/    /' \"$ARCHIVE_FILE\" | \\\n                lectic -f ~/.config/lectic/summarize.lec -S)\n      \n      echo \"LECTIC:reset\"\n      echo \"LECTIC:final\"\n      echo \"\"\n      echo \"**Context compacted. Archived to: $ARCHIVE_FILE**\"\n      echo \"\"\n      echo \"Summary:\"\n      echo \"$SUMMARY\"\n\n\n\nGive a warning at 70% capacity, compact at 90%:\nhooks:\n  - on: assistant_message\n    inline: true\n    do: |\n      #!/bin/bash\n      TOTAL=\"${TOKEN_USAGE_TOTAL:-0}\"\n      WARN_LIMIT=70000\n      HARD_LIMIT=90000\n      \n      if [[ $TOTAL -gt $HARD_LIMIT ]]; then\n        # Full compaction\n        CONVERSATION=$(sed 's/^/    /')\n        SUMMARY=$(echo \"$CONVERSATION\" | \\\n                  lectic -f ~/.config/lectic/summarize.lec -S)\n        echo \"LECTIC:reset\"\n        echo \"LECTIC:final\"\n        echo \"\"\n        echo \"**Context limit reached. Compacted.**\"\n        echo \"\"\n        echo \"$SUMMARY\"\n      elif [[ $TOTAL -gt $WARN_LIMIT ]]; then\n        # Just warn\n        cat &gt; /dev/null  # Consume stdin\n        echo \"LECTIC:final\"\n        echo \"\"\n        echo \"*Note: Context at ${TOTAL} tokens. Consider* :reset[] *soon.*\"\n      fi",
    "crumbs": [
      "Cookbook",
      "Context Compaction"
    ]
  },
  {
    "objectID": "cookbook/05_context_compaction.html#tips",
    "href": "cookbook/05_context_compaction.html#tips",
    "title": "Recipe: Context Compaction",
    "section": "",
    "text": "Test your summarizer. A bad summary loses important context. Try it manually on a few conversations first.\nUse a fast model for summarization. Haiku or similar is fine for summaries and keeps compaction quick.\nConsider what to preserve. Code snippets, file paths, and specific decisions are often more important than general discussion.\nMonitor compaction frequency. If you’re compacting every few messages, your conversations might be too verbose or you might need a larger context model.",
    "crumbs": [
      "Cookbook",
      "Context Compaction"
    ]
  },
  {
    "objectID": "cookbook/index.html",
    "href": "cookbook/index.html",
    "title": "Cookbook",
    "section": "",
    "text": "This section contains practical recipes showing how to combine Lectic’s primitives to build useful workflows. Each recipe is self-contained and can be adapted to your needs.\n\n\nIf you’re new to Lectic, start with these:\n\nCoding Assistant — Give your LLM shell access, type checking, and linting. Includes a confirmation dialog so you approve tool calls before they run.\nGit Commit Messages — A lectic commit subcommand that generates conventional commit messages from your staged changes. A good example of building small, focused tools.\n\n\n\n\n\nResearch with Multiple Perspectives — Define multiple interlocutors with different personalities (researcher, critic, synthesizer) and use :ask and :aside to get different viewpoints on a problem.\n\n\n\n\n\nConversation Memory — Two approaches to persistence: automatically recording everything to SQLite, or giving the LLM an explicit “remember” tool. An advanced recipe that shows the power of hooks.\nContext Compaction — Automatically summarize and reset context when token usage gets high. Useful for long-running conversations that would otherwise hit context limits.\n\n\n\n\n\nCustom Sandboxing — Isolate tool execution using wrapper scripts. Covers logging, Bubblewrap isolation, and stateful “shadow workspaces.” Essential reading if you’re giving LLMs write access to your system.\n\n\n\n\n\nControl Flow with Macros — Implement loops, conditionals, and map operations using recursive macros. This is power-user territory—most workflows don’t need this, but it shows what’s possible.\nAgent Skills Support — Build a subcommand that exposes the Agent Skills format, letting your LLM load capabilities on demand through progressive disclosure.",
    "crumbs": [
      "Cookbook",
      "Overview"
    ]
  },
  {
    "objectID": "cookbook/index.html#getting-started",
    "href": "cookbook/index.html#getting-started",
    "title": "Cookbook",
    "section": "",
    "text": "If you’re new to Lectic, start with these:\n\nCoding Assistant — Give your LLM shell access, type checking, and linting. Includes a confirmation dialog so you approve tool calls before they run.\nGit Commit Messages — A lectic commit subcommand that generates conventional commit messages from your staged changes. A good example of building small, focused tools.",
    "crumbs": [
      "Cookbook",
      "Overview"
    ]
  },
  {
    "objectID": "cookbook/index.html#multi-agent-workflows",
    "href": "cookbook/index.html#multi-agent-workflows",
    "title": "Cookbook",
    "section": "",
    "text": "Research with Multiple Perspectives — Define multiple interlocutors with different personalities (researcher, critic, synthesizer) and use :ask and :aside to get different viewpoints on a problem.",
    "crumbs": [
      "Cookbook",
      "Overview"
    ]
  },
  {
    "objectID": "cookbook/index.html#state-and-memory",
    "href": "cookbook/index.html#state-and-memory",
    "title": "Cookbook",
    "section": "",
    "text": "Conversation Memory — Two approaches to persistence: automatically recording everything to SQLite, or giving the LLM an explicit “remember” tool. An advanced recipe that shows the power of hooks.\nContext Compaction — Automatically summarize and reset context when token usage gets high. Useful for long-running conversations that would otherwise hit context limits.",
    "crumbs": [
      "Cookbook",
      "Overview"
    ]
  },
  {
    "objectID": "cookbook/index.html#security-and-isolation",
    "href": "cookbook/index.html#security-and-isolation",
    "title": "Cookbook",
    "section": "",
    "text": "Custom Sandboxing — Isolate tool execution using wrapper scripts. Covers logging, Bubblewrap isolation, and stateful “shadow workspaces.” Essential reading if you’re giving LLMs write access to your system.",
    "crumbs": [
      "Cookbook",
      "Overview"
    ]
  },
  {
    "objectID": "cookbook/index.html#advanced-techniques",
    "href": "cookbook/index.html#advanced-techniques",
    "title": "Cookbook",
    "section": "",
    "text": "Control Flow with Macros — Implement loops, conditionals, and map operations using recursive macros. This is power-user territory—most workflows don’t need this, but it shows what’s possible.\nAgent Skills Support — Build a subcommand that exposes the Agent Skills format, letting your LLM load capabilities on demand through progressive disclosure.",
    "crumbs": [
      "Cookbook",
      "Overview"
    ]
  },
  {
    "objectID": "automation/02_hooks.html",
    "href": "automation/02_hooks.html",
    "title": "Automation: Hooks",
    "section": "",
    "text": "Hooks are a powerful automation feature that let you run custom commands and scripts in response to events in Lectic’s lifecycle. Use them for logging, notifications, post‑processing, or integrating with other tools and workflows.\nHooks are defined in your YAML configuration under the hooks key, per-tool in the hooks key of a tool specification, or per-interlocutor in the hooks key of an interlocutor specification.\n\n\nA hook has five possible fields:\n\non: (Required) A single event name or a list of event names to listen for.\ndo: (Required) The command or inline script to run when the event fires.\ninline: (Optional) A boolean. If true, the standard output of the command is captured and injected into the conversation. Defaults to false. Only applicable to assistant_message and user_message.\nname: (Optional) A string name for the hook. If multiple hooks have the same name (e.g., one in your global config and one in a project config), the one defined later (or with higher precedence) overrides the earlier one. This allows you to replace default hooks with custom behavior.\nenv: (Optional) A map of environment variables to inject into the hook’s execution environment.\n\nhooks:\n  - name: logger\n    on: [assistant_message, user_message]\n    env:\n      LOG_FILE: /tmp/lectic.log\n    do: ./log-activity.sh\nIf do contains multiple lines, it is treated as a script and must begin with a shebang (e.g., #!/bin/bash). If it is a single line, it is treated as a command. Commands are executed directly (not through a shell), so shell features like command substitution will not work.\nHook commands run synchronously. By default, their stdout, stderr, and exit status are ignored by Lectic. However, if you set inline: true, the standard output is captured and added to the conversation.\n\nFor user_message events, the output is injected as context for the LLM before it generates a response. It also appears at the top of the assistant’s response block.\nFor assistant_message events, the output is appended to the end of the assistant’s response block. This will trigger another reply from the assistant, so be careful to only fire an inline hook when you want the assistant to generate more content.\n\nIn the .lec file, inline hook output is stored as an XML &lt;inline-attachment kind=\"hook\"&gt; block. The &lt;command&gt; element records the hook’s do field so you can see what produced the output.\n&lt;inline-attachment kind=\"hook\"&gt;\n&lt;command&gt;./my-hook.sh&lt;/command&gt;\n&lt;content type=\"text/plain\"&gt;\n┆System check complete.\n&lt;/content&gt;\n&lt;/inline-attachment&gt;\n\n\n\nLectic emits three hook events. When an event fires, the hook process receives its context as environment variables. No positional arguments are passed. However, the hook may receive content via standard input.\n\nuser_message\n\nEnvironment:\n\nUSER_MESSAGE: The text of the most recent user message.\nStandard Lectic variables like LECTIC_FILE, LECTIC_CONFIG, LECTIC_DATA, LECTIC_CACHE, LECTIC_STATE, and LECTIC_TEMP are also set when available.\n\nWhen: Just before the request is sent to the LLM provider.\n\nassistant_message\n\nStandard Input: The raw markdown text of the conversation body up to this point.\nEnvironment:\n\nASSISTANT_MESSAGE: The full text of the assistant’s response that was just produced.\nLECTIC_INTERLOCUTOR: The name of the interlocutor who spoke.\nLECTIC_MODEL: The model of the interlocutor who spoke.\nTOOL_USE_DONE: Set to 1 when the assistant has finished using tools and is ready to conclude. Not set if there are pending tool calls. This lets inline hooks decide whether to inject follow-up content only when all work is complete.\nTOKEN_USAGE_INPUT: Count of total input tokens used for this turn.\nTOKEN_USAGE_CACHED: Count of cached input tokens used for this turn.\nTOKEN_USAGE_OUTPUT: Count of output tokens used for this turn.\nTOKEN_USAGE_TOTAL: Total tokens used for this turn.\nLOOP_COUNT: How many times the tool calling loop has run (0-indexed).\nFINAL_PASS_COUNT: How many times the assistant has finished work but was kept alive by an inline hook.\nStandard Lectic variables as above.\n\nWhen: Immediately after the assistant’s message is streamed.\n\ntool_use_pre\n\nEnvironment:\n\nTOOL_NAME: The name of the tool being called.\nTOOL_ARGS: A JSON string containing the tool arguments.\nStandard Lectic variables as above.\n\nWhen: After tool parameters are collected but before execution.\nBehavior: If the hook exits with a non-zero status code, the tool call is blocked, and the LLM receives a “permission denied” error.\n\nerror\n\nEnvironment:\n\nERROR_MESSAGE: A descriptive error message.\nStandard Lectic variables as above.\n\nWhen: Whenever an uncaught error is encountered.\n\n\n\n\n\nHooks can pass metadata back to Lectic by including headers at the very beginning of their output. Headers follow the format LECTIC:KEY:VALUE or simply LECTIC:KEY (where the value defaults to “true”) and must appear before any other content. The headers are stripped from the visible output and stored as attributes on the inline attachment block.\n#!/usr/bin/env bash\necho \"LECTIC:final\"\necho \"\"\necho \"System check complete. One issue found.\"\nThis would be recorded roughly like this:\n&lt;inline-attachment kind=\"hook\" final=\"true\"&gt;\n&lt;command&gt;./my-hook.sh&lt;/command&gt;\n&lt;content type=\"text/plain\"&gt;\n┆System check complete. One issue found.\n&lt;/content&gt;\n&lt;/inline-attachment&gt;\nTwo headers affect control flow:\n\nfinal: When an inline hook generates output, Lectic normally continues the tool calling loop so that the assistant can see and respond to the new information. If the final header is present, Lectic prevents this extra pass, allowing the conversation turn to end immediately (unless the assistant explicitly called a tool).\nreset: When present, this header clears the conversation context up to the current message. The accumulated history sent to the provider is discarded, and the context effectively restarts from the message containing the hook output. This is useful for implementing custom context compaction or archival strategies when token limits are reached.\n\n\n\n\nLet’s start with the simplest possible hook: logging every message to a file. This helps you understand the basics before moving to more complex examples.\nhooks:\n  - on: [user_message, assistant_message]\n    do: |\n      #!/usr/bin/env bash\n      echo \"$(date): Message received\" &gt;&gt; /tmp/lectic.log\nThis hook fires on both user and assistant messages. It appends a timestamp to a log file. That’s it—no return value, no interaction with the conversation.\n\n\n\nThis example uses tool_use_pre to require confirmation before any tool execution. It uses zenity to show a dialog box with the tool name and arguments.\nhooks:\n  - on: tool_use_pre\n    do: |\n      #!/usr/bin/env bash\n      # Display a confirmation dialog\n      zenity --question \\\n             --title=\"Allow Tool Use?\" \\\n             --text=\"Tool: $TOOL_NAME\\nArgs: $TOOL_ARGS\"\n      # Zenity exits with 0 for Yes/OK and 1 for No/Cancel\n      exit $?\n\n\n\nThis example persists every user and assistant message to an SQLite database located in your Lectic data directory. You can later query this for personal memory, project history, or analytics.\nConfiguration:\nhooks:\n  - on: [user_message, assistant_message]\n    do: |\n      #!/usr/bin/env bash\n      set -euo pipefail\n      DB_ROOT=\"${LECTIC_DATA:-$HOME/.local/share/lectic}\"\n      DB_PATH=\"${DB_ROOT}/memory.sqlite3\"\n      mkdir -p \"${DB_ROOT}\"\n\n      # Determine role and text from available variables\n      if [[ -n \"${ASSISTANT_MESSAGE:-}\" ]]; then\n        ROLE=\"assistant\"\n        TEXT=\"$ASSISTANT_MESSAGE\"\n      else\n        ROLE=\"user\"\n        TEXT=\"${USER_MESSAGE:-}\"\n      fi\n\n      # Basic sanitizer for single quotes for SQL literal\n      esc_sq() { printf %s \"$1\" | sed \"s/'/''/g\"; }\n\n      TS=$(date -Is)\n      FILE_PATH=\"${LECTIC_FILE:-}\"\n      NAME=\"${LECTIC_INTERLOCUTOR:-}\"\n\n      sqlite3 \"$DB_PATH\" &lt;&lt;SQL\n      CREATE TABLE IF NOT EXISTS memory (\n        id INTEGER PRIMARY KEY,\n        ts TEXT NOT NULL,\n        role TEXT NOT NULL,\n        interlocutor TEXT,\n        file TEXT,\n        text TEXT NOT NULL\n      );\n      INSERT INTO memory(ts, role, interlocutor, file, text)\n      VALUES ('${TS}', '${ROLE}', '$(esc_sq \"$NAME\")',\n              '$(esc_sq \"$FILE_PATH\")', '$(esc_sq \"$TEXT\")');\n      SQL\nNotes:\n\nRequires the sqlite3 command-line tool to be installed and on your PATH.\nThe hook inspects which variable is set to decide whether the event was a user or assistant message.\nLECTIC_FILE is populated when using -f/-i and may be empty when streaming from stdin.\nAdjust the table schema to suit your use case.\n\n\n\n\nThis example automatically runs date before every user message and injects the output into the context. This allows the LLM to always know the date and time without you needing to run :cmd[date]\nhooks:\n  - on: user_message\n    inline: true\n    do: \n      #!/usr/bin/env bash\n      echo \"&lt;date-and-time&gt;\"\n      date\n      echo \"&lt;/date-and-time&gt;\"\n\n\n\nThis example sends a desktop notification when the assistant finishes a tool-use workflow. The hook checks TOOL_USE_DONE so you only get notified once the work is actually done, not after each intermediate step.\nhooks:\n  - on: assistant_message\n    do: |\n      #!/usr/bin/env bash\n      if [[ \"${TOOL_USE_DONE:-}\" == \"1\" ]]; then\n        notify-send \"Lectic\" \"Assistant finished working\"\n      fi\nThis is especially useful for long-running agentic tasks where you want to step away and be alerted when the assistant is done.\n\n\n\nWhen using the lectic.nvim plugin, the NVIM environment variable is set to Neovim’s RPC server address. This allows hooks to communicate directly with your editor—sending notifications, opening windows, or triggering any Neovim Lua API.\nThis example sends a notification to Neovim when the assistant finishes working:\nhooks:\n  - on: assistant_message\n    do: |\n      #!/usr/bin/env bash\n      if [[ \"${TOOL_USE_DONE:-}\" == \"1\" && -n \"${NVIM:-}\" ]]; then\n        nvim --server \"$NVIM\" --remote-expr \\\n          \"luaeval('vim.notify(\\\"Lectic: Assistant finished working\\\", vim.log.levels.INFO)')\"\n      fi\nThe pattern nvim --server \"$NVIM\" --remote-expr \"luaeval('...')\" lets you execute arbitrary Lua in the running Neovim instance. Some ideas:\n\nPlay a sound: vim.fn.system('paplay /usr/share/sounds/...')\nFlash the screen: vim.cmd('sleep 100m | redraw!')\nUpdate a status line variable\nTrigger a custom autocommand: vim.api.nvim_exec_autocmds('User', {pattern = 'LecticDone'})\n\n\n\n\nThis example checks the total token usage and, if it exceeds a limit, resets the conversation context. It also uses the final header to stop the assistant from responding to the reset message immediately.\nhooks:\n  - on: assistant_message\n    inline: true\n    do: |\n      #!/usr/bin/env bash\n      LIMIT=100000\n      TOTAL=\"${TOKEN_USAGE_TOTAL:-0}\"\n      \n      if [ \"$TOTAL\" -gt \"$LIMIT\" ]; then\n        echo \"LECTIC:reset\"\n        echo \"LECTIC:final\"\n        echo \"\"\n        echo \"**Context cleared (usage: $TOTAL tokens).**\"\n      fi",
    "crumbs": [
      "Automation",
      "Hooks"
    ]
  },
  {
    "objectID": "automation/02_hooks.html#hook-configuration",
    "href": "automation/02_hooks.html#hook-configuration",
    "title": "Automation: Hooks",
    "section": "",
    "text": "A hook has five possible fields:\n\non: (Required) A single event name or a list of event names to listen for.\ndo: (Required) The command or inline script to run when the event fires.\ninline: (Optional) A boolean. If true, the standard output of the command is captured and injected into the conversation. Defaults to false. Only applicable to assistant_message and user_message.\nname: (Optional) A string name for the hook. If multiple hooks have the same name (e.g., one in your global config and one in a project config), the one defined later (or with higher precedence) overrides the earlier one. This allows you to replace default hooks with custom behavior.\nenv: (Optional) A map of environment variables to inject into the hook’s execution environment.\n\nhooks:\n  - name: logger\n    on: [assistant_message, user_message]\n    env:\n      LOG_FILE: /tmp/lectic.log\n    do: ./log-activity.sh\nIf do contains multiple lines, it is treated as a script and must begin with a shebang (e.g., #!/bin/bash). If it is a single line, it is treated as a command. Commands are executed directly (not through a shell), so shell features like command substitution will not work.\nHook commands run synchronously. By default, their stdout, stderr, and exit status are ignored by Lectic. However, if you set inline: true, the standard output is captured and added to the conversation.\n\nFor user_message events, the output is injected as context for the LLM before it generates a response. It also appears at the top of the assistant’s response block.\nFor assistant_message events, the output is appended to the end of the assistant’s response block. This will trigger another reply from the assistant, so be careful to only fire an inline hook when you want the assistant to generate more content.\n\nIn the .lec file, inline hook output is stored as an XML &lt;inline-attachment kind=\"hook\"&gt; block. The &lt;command&gt; element records the hook’s do field so you can see what produced the output.\n&lt;inline-attachment kind=\"hook\"&gt;\n&lt;command&gt;./my-hook.sh&lt;/command&gt;\n&lt;content type=\"text/plain\"&gt;\n┆System check complete.\n&lt;/content&gt;\n&lt;/inline-attachment&gt;",
    "crumbs": [
      "Automation",
      "Hooks"
    ]
  },
  {
    "objectID": "automation/02_hooks.html#available-events-and-environment",
    "href": "automation/02_hooks.html#available-events-and-environment",
    "title": "Automation: Hooks",
    "section": "",
    "text": "Lectic emits three hook events. When an event fires, the hook process receives its context as environment variables. No positional arguments are passed. However, the hook may receive content via standard input.\n\nuser_message\n\nEnvironment:\n\nUSER_MESSAGE: The text of the most recent user message.\nStandard Lectic variables like LECTIC_FILE, LECTIC_CONFIG, LECTIC_DATA, LECTIC_CACHE, LECTIC_STATE, and LECTIC_TEMP are also set when available.\n\nWhen: Just before the request is sent to the LLM provider.\n\nassistant_message\n\nStandard Input: The raw markdown text of the conversation body up to this point.\nEnvironment:\n\nASSISTANT_MESSAGE: The full text of the assistant’s response that was just produced.\nLECTIC_INTERLOCUTOR: The name of the interlocutor who spoke.\nLECTIC_MODEL: The model of the interlocutor who spoke.\nTOOL_USE_DONE: Set to 1 when the assistant has finished using tools and is ready to conclude. Not set if there are pending tool calls. This lets inline hooks decide whether to inject follow-up content only when all work is complete.\nTOKEN_USAGE_INPUT: Count of total input tokens used for this turn.\nTOKEN_USAGE_CACHED: Count of cached input tokens used for this turn.\nTOKEN_USAGE_OUTPUT: Count of output tokens used for this turn.\nTOKEN_USAGE_TOTAL: Total tokens used for this turn.\nLOOP_COUNT: How many times the tool calling loop has run (0-indexed).\nFINAL_PASS_COUNT: How many times the assistant has finished work but was kept alive by an inline hook.\nStandard Lectic variables as above.\n\nWhen: Immediately after the assistant’s message is streamed.\n\ntool_use_pre\n\nEnvironment:\n\nTOOL_NAME: The name of the tool being called.\nTOOL_ARGS: A JSON string containing the tool arguments.\nStandard Lectic variables as above.\n\nWhen: After tool parameters are collected but before execution.\nBehavior: If the hook exits with a non-zero status code, the tool call is blocked, and the LLM receives a “permission denied” error.\n\nerror\n\nEnvironment:\n\nERROR_MESSAGE: A descriptive error message.\nStandard Lectic variables as above.\n\nWhen: Whenever an uncaught error is encountered.",
    "crumbs": [
      "Automation",
      "Hooks"
    ]
  },
  {
    "objectID": "automation/02_hooks.html#hook-headers-and-attributes",
    "href": "automation/02_hooks.html#hook-headers-and-attributes",
    "title": "Automation: Hooks",
    "section": "",
    "text": "Hooks can pass metadata back to Lectic by including headers at the very beginning of their output. Headers follow the format LECTIC:KEY:VALUE or simply LECTIC:KEY (where the value defaults to “true”) and must appear before any other content. The headers are stripped from the visible output and stored as attributes on the inline attachment block.\n#!/usr/bin/env bash\necho \"LECTIC:final\"\necho \"\"\necho \"System check complete. One issue found.\"\nThis would be recorded roughly like this:\n&lt;inline-attachment kind=\"hook\" final=\"true\"&gt;\n&lt;command&gt;./my-hook.sh&lt;/command&gt;\n&lt;content type=\"text/plain\"&gt;\n┆System check complete. One issue found.\n&lt;/content&gt;\n&lt;/inline-attachment&gt;\nTwo headers affect control flow:\n\nfinal: When an inline hook generates output, Lectic normally continues the tool calling loop so that the assistant can see and respond to the new information. If the final header is present, Lectic prevents this extra pass, allowing the conversation turn to end immediately (unless the assistant explicitly called a tool).\nreset: When present, this header clears the conversation context up to the current message. The accumulated history sent to the provider is discarded, and the context effectively restarts from the message containing the hook output. This is useful for implementing custom context compaction or archival strategies when token limits are reached.",
    "crumbs": [
      "Automation",
      "Hooks"
    ]
  },
  {
    "objectID": "automation/02_hooks.html#example-a-simple-logging-hook",
    "href": "automation/02_hooks.html#example-a-simple-logging-hook",
    "title": "Automation: Hooks",
    "section": "",
    "text": "Let’s start with the simplest possible hook: logging every message to a file. This helps you understand the basics before moving to more complex examples.\nhooks:\n  - on: [user_message, assistant_message]\n    do: |\n      #!/usr/bin/env bash\n      echo \"$(date): Message received\" &gt;&gt; /tmp/lectic.log\nThis hook fires on both user and assistant messages. It appends a timestamp to a log file. That’s it—no return value, no interaction with the conversation.",
    "crumbs": [
      "Automation",
      "Hooks"
    ]
  },
  {
    "objectID": "automation/02_hooks.html#example-human-in-the-loop-tool-confirmation",
    "href": "automation/02_hooks.html#example-human-in-the-loop-tool-confirmation",
    "title": "Automation: Hooks",
    "section": "",
    "text": "This example uses tool_use_pre to require confirmation before any tool execution. It uses zenity to show a dialog box with the tool name and arguments.\nhooks:\n  - on: tool_use_pre\n    do: |\n      #!/usr/bin/env bash\n      # Display a confirmation dialog\n      zenity --question \\\n             --title=\"Allow Tool Use?\" \\\n             --text=\"Tool: $TOOL_NAME\\nArgs: $TOOL_ARGS\"\n      # Zenity exits with 0 for Yes/OK and 1 for No/Cancel\n      exit $?",
    "crumbs": [
      "Automation",
      "Hooks"
    ]
  },
  {
    "objectID": "automation/02_hooks.html#example-persisting-messages-to-sqlite",
    "href": "automation/02_hooks.html#example-persisting-messages-to-sqlite",
    "title": "Automation: Hooks",
    "section": "",
    "text": "This example persists every user and assistant message to an SQLite database located in your Lectic data directory. You can later query this for personal memory, project history, or analytics.\nConfiguration:\nhooks:\n  - on: [user_message, assistant_message]\n    do: |\n      #!/usr/bin/env bash\n      set -euo pipefail\n      DB_ROOT=\"${LECTIC_DATA:-$HOME/.local/share/lectic}\"\n      DB_PATH=\"${DB_ROOT}/memory.sqlite3\"\n      mkdir -p \"${DB_ROOT}\"\n\n      # Determine role and text from available variables\n      if [[ -n \"${ASSISTANT_MESSAGE:-}\" ]]; then\n        ROLE=\"assistant\"\n        TEXT=\"$ASSISTANT_MESSAGE\"\n      else\n        ROLE=\"user\"\n        TEXT=\"${USER_MESSAGE:-}\"\n      fi\n\n      # Basic sanitizer for single quotes for SQL literal\n      esc_sq() { printf %s \"$1\" | sed \"s/'/''/g\"; }\n\n      TS=$(date -Is)\n      FILE_PATH=\"${LECTIC_FILE:-}\"\n      NAME=\"${LECTIC_INTERLOCUTOR:-}\"\n\n      sqlite3 \"$DB_PATH\" &lt;&lt;SQL\n      CREATE TABLE IF NOT EXISTS memory (\n        id INTEGER PRIMARY KEY,\n        ts TEXT NOT NULL,\n        role TEXT NOT NULL,\n        interlocutor TEXT,\n        file TEXT,\n        text TEXT NOT NULL\n      );\n      INSERT INTO memory(ts, role, interlocutor, file, text)\n      VALUES ('${TS}', '${ROLE}', '$(esc_sq \"$NAME\")',\n              '$(esc_sq \"$FILE_PATH\")', '$(esc_sq \"$TEXT\")');\n      SQL\nNotes:\n\nRequires the sqlite3 command-line tool to be installed and on your PATH.\nThe hook inspects which variable is set to decide whether the event was a user or assistant message.\nLECTIC_FILE is populated when using -f/-i and may be empty when streaming from stdin.\nAdjust the table schema to suit your use case.",
    "crumbs": [
      "Automation",
      "Hooks"
    ]
  },
  {
    "objectID": "automation/02_hooks.html#example-automatically-injecting-context",
    "href": "automation/02_hooks.html#example-automatically-injecting-context",
    "title": "Automation: Hooks",
    "section": "",
    "text": "This example automatically runs date before every user message and injects the output into the context. This allows the LLM to always know the date and time without you needing to run :cmd[date]\nhooks:\n  - on: user_message\n    inline: true\n    do: \n      #!/usr/bin/env bash\n      echo \"&lt;date-and-time&gt;\"\n      date\n      echo \"&lt;/date-and-time&gt;\"",
    "crumbs": [
      "Automation",
      "Hooks"
    ]
  },
  {
    "objectID": "automation/02_hooks.html#example-notification-when-work-completes",
    "href": "automation/02_hooks.html#example-notification-when-work-completes",
    "title": "Automation: Hooks",
    "section": "",
    "text": "This example sends a desktop notification when the assistant finishes a tool-use workflow. The hook checks TOOL_USE_DONE so you only get notified once the work is actually done, not after each intermediate step.\nhooks:\n  - on: assistant_message\n    do: |\n      #!/usr/bin/env bash\n      if [[ \"${TOOL_USE_DONE:-}\" == \"1\" ]]; then\n        notify-send \"Lectic\" \"Assistant finished working\"\n      fi\nThis is especially useful for long-running agentic tasks where you want to step away and be alerted when the assistant is done.",
    "crumbs": [
      "Automation",
      "Hooks"
    ]
  },
  {
    "objectID": "automation/02_hooks.html#example-neovim-notification-from-hooks",
    "href": "automation/02_hooks.html#example-neovim-notification-from-hooks",
    "title": "Automation: Hooks",
    "section": "",
    "text": "When using the lectic.nvim plugin, the NVIM environment variable is set to Neovim’s RPC server address. This allows hooks to communicate directly with your editor—sending notifications, opening windows, or triggering any Neovim Lua API.\nThis example sends a notification to Neovim when the assistant finishes working:\nhooks:\n  - on: assistant_message\n    do: |\n      #!/usr/bin/env bash\n      if [[ \"${TOOL_USE_DONE:-}\" == \"1\" && -n \"${NVIM:-}\" ]]; then\n        nvim --server \"$NVIM\" --remote-expr \\\n          \"luaeval('vim.notify(\\\"Lectic: Assistant finished working\\\", vim.log.levels.INFO)')\"\n      fi\nThe pattern nvim --server \"$NVIM\" --remote-expr \"luaeval('...')\" lets you execute arbitrary Lua in the running Neovim instance. Some ideas:\n\nPlay a sound: vim.fn.system('paplay /usr/share/sounds/...')\nFlash the screen: vim.cmd('sleep 100m | redraw!')\nUpdate a status line variable\nTrigger a custom autocommand: vim.api.nvim_exec_autocmds('User', {pattern = 'LecticDone'})",
    "crumbs": [
      "Automation",
      "Hooks"
    ]
  },
  {
    "objectID": "automation/02_hooks.html#example-reset-context-on-token-limit",
    "href": "automation/02_hooks.html#example-reset-context-on-token-limit",
    "title": "Automation: Hooks",
    "section": "",
    "text": "This example checks the total token usage and, if it exceeds a limit, resets the conversation context. It also uses the final header to stop the assistant from responding to the reset message immediately.\nhooks:\n  - on: assistant_message\n    inline: true\n    do: |\n      #!/usr/bin/env bash\n      LIMIT=100000\n      TOTAL=\"${TOKEN_USAGE_TOTAL:-0}\"\n      \n      if [ \"$TOTAL\" -gt \"$LIMIT\" ]; then\n        echo \"LECTIC:reset\"\n        echo \"LECTIC:final\"\n        echo \"\"\n        echo \"**Context cleared (usage: $TOTAL tokens).**\"\n      fi",
    "crumbs": [
      "Automation",
      "Hooks"
    ]
  },
  {
    "objectID": "tools/04_mcp.html",
    "href": "tools/04_mcp.html",
    "title": "Tools: Model Context Protocol (mcp)",
    "section": "",
    "text": "Lectic can act as a client for servers that implement the Model Context Protocol (MCP). This allows you to connect your LLM to a vast and growing ecosystem of pre-built tools and services.\n\n\nMCP is a standard protocol for connecting LLMs to external tools and data sources. Instead of writing custom integrations for every service, you can use any MCP-compatible server. Servers exist for GitHub, databases, file systems, web browsers, and much more.\nThe protocol handles the details of how tools describe themselves and how results are returned. Lectic speaks MCP, so you can drop in any compatible server and the LLM can use its tools immediately.\nYou can find lists of available servers here:\n\nOfficial MCP Server List\nAwesome MCP Servers\n\n\n\n\nNote: The snippets below show only the tool definition. They assume you have an interlocutor with a valid prompt and model configuration. See Getting Started for a full header example.\nYou can connect to an MCP server in three ways: by running a local server as a command, or by connecting to a remote server over WebSockets or SSE.\n\n\nThis is the most common way to run an MCP server. You provide the command to start the server, and Lectic manages its lifecycle.\ntools:\n  - name: brave\n    mcp_command: npx\n    args:\n      - \"-y\"\n      - \"@modelcontextprotocol/server-brave-search\"\n    env:\n      BRAVE_API_KEY: \"your_key_here\"\n    roots:\n      - /home/user/research-docs/\nLocal MCP servers are started on demand for the active interlocutor and managed by Lectic for the duration of the session.\n\n\n\nYou can also connect to running MCP servers.\n\nmcp_ws: The URL for a remote server using a WebSocket connection.\nmcp_sse: The URL for a remote server using Server-Sent Events.\nmcp_shttp: The URL for a remote server using Streamable HTTP.\n\nFor example:\ntools:\n  - name: documentation_search \n    mcp_shttp: https://mcp.context7.com/mcp\n\n\nThe mcp_shttp transport supports both custom headers and dynamic OAuth 2.0.\nCustom headers can be provided using the headers key. Header values support file: and exec: sources, which is useful for securely loading tokens or computing authentication headers on the fly.\ntools:\n  - name: github_mcp\n    mcp_shttp: https://api.githubcopilot.com/mcp\n    headers:\n      Authorization: exec:bash -c 'echo \"Bearer $(pass github_token)\"'\nIf a server requires OAuth 2.0 and supports the MCP OAuth flow, Lectic will automatically handle the authorization process. When an unauthorized request is made, Lectic will open your default browser to complete the login, and then securely persist the resulting tokens in your data directory.\n\n\n\n\nIf you give an MCP tool a name (e.g., name: brave), you can access any resources it provides using a special content reference syntax. The scheme is the server’s name plus the resource type.\nFor example, to access a repo resource from a server named github: [README](github+repo://gleachkr/Lectic/contents/README.md)\nThe LLM is also given a tool to list the available resources from the server.\n\n\n\nYou can hide specific tools that a server exposes by listing their names under exclude.\ntools:\n  - name: github\n    mcp_ws: wss://example.org/mcp\n    exclude:\n      - dangerous_tool\n      - low_value_tool\nYou can also limit access to a specific set of tools that a server exposes by listing their names under only.\ntools:\n  - name: github\n    mcp_ws: wss://example.org/mcp\n    only:\n      - safe_tool\n      - high_value_tool\nIf you specify both options at once, you’ll get exactly the tools from the only list that aren’t also excluded.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhile powerful, the MCP protocol carries significant security risks. Treat MCP integration as a high-trust capability. Never connect to untrusted servers; a malicious server could exfiltrate data or perform unwanted actions. Lectic’s safety mechanisms reduce mistakes from a well‑behaved LLM, not attacks from a hostile server.\n\n\n\n\nJust like with the exec tool, you can use the tool_use_pre hook to implement confirmation dialogs or logic. See Hooks for examples.\n\n\n\nFor local mcp_command tools, you can specify a sandbox command. This command will be used to launch the MCP server process in a controlled and isolated environment, limiting its access to your system. Arguments are supported (e.g. sandbox: wrapper.sh --strict).\nSee the documentation for the Exec Tool for more details on how sandboxing scripts work, and the Custom Sandboxing recipe for examples.\nYou can also set a default sandbox at the top level (sandbox) or on the interlocutor object. If set, it applies to all local mcp_command tools that don’t specify their own. Tool-level sandbox wins over both defaults.",
    "crumbs": [
      "Tools",
      "MCP"
    ]
  },
  {
    "objectID": "tools/04_mcp.html#what-is-mcp",
    "href": "tools/04_mcp.html#what-is-mcp",
    "title": "Tools: Model Context Protocol (mcp)",
    "section": "",
    "text": "MCP is a standard protocol for connecting LLMs to external tools and data sources. Instead of writing custom integrations for every service, you can use any MCP-compatible server. Servers exist for GitHub, databases, file systems, web browsers, and much more.\nThe protocol handles the details of how tools describe themselves and how results are returned. Lectic speaks MCP, so you can drop in any compatible server and the LLM can use its tools immediately.\nYou can find lists of available servers here:\n\nOfficial MCP Server List\nAwesome MCP Servers",
    "crumbs": [
      "Tools",
      "MCP"
    ]
  },
  {
    "objectID": "tools/04_mcp.html#configuration",
    "href": "tools/04_mcp.html#configuration",
    "title": "Tools: Model Context Protocol (mcp)",
    "section": "",
    "text": "Note: The snippets below show only the tool definition. They assume you have an interlocutor with a valid prompt and model configuration. See Getting Started for a full header example.\nYou can connect to an MCP server in three ways: by running a local server as a command, or by connecting to a remote server over WebSockets or SSE.\n\n\nThis is the most common way to run an MCP server. You provide the command to start the server, and Lectic manages its lifecycle.\ntools:\n  - name: brave\n    mcp_command: npx\n    args:\n      - \"-y\"\n      - \"@modelcontextprotocol/server-brave-search\"\n    env:\n      BRAVE_API_KEY: \"your_key_here\"\n    roots:\n      - /home/user/research-docs/\nLocal MCP servers are started on demand for the active interlocutor and managed by Lectic for the duration of the session.\n\n\n\nYou can also connect to running MCP servers.\n\nmcp_ws: The URL for a remote server using a WebSocket connection.\nmcp_sse: The URL for a remote server using Server-Sent Events.\nmcp_shttp: The URL for a remote server using Streamable HTTP.\n\nFor example:\ntools:\n  - name: documentation_search \n    mcp_shttp: https://mcp.context7.com/mcp\n\n\nThe mcp_shttp transport supports both custom headers and dynamic OAuth 2.0.\nCustom headers can be provided using the headers key. Header values support file: and exec: sources, which is useful for securely loading tokens or computing authentication headers on the fly.\ntools:\n  - name: github_mcp\n    mcp_shttp: https://api.githubcopilot.com/mcp\n    headers:\n      Authorization: exec:bash -c 'echo \"Bearer $(pass github_token)\"'\nIf a server requires OAuth 2.0 and supports the MCP OAuth flow, Lectic will automatically handle the authorization process. When an unauthorized request is made, Lectic will open your default browser to complete the login, and then securely persist the resulting tokens in your data directory.\n\n\n\n\nIf you give an MCP tool a name (e.g., name: brave), you can access any resources it provides using a special content reference syntax. The scheme is the server’s name plus the resource type.\nFor example, to access a repo resource from a server named github: [README](github+repo://gleachkr/Lectic/contents/README.md)\nThe LLM is also given a tool to list the available resources from the server.\n\n\n\nYou can hide specific tools that a server exposes by listing their names under exclude.\ntools:\n  - name: github\n    mcp_ws: wss://example.org/mcp\n    exclude:\n      - dangerous_tool\n      - low_value_tool\nYou can also limit access to a specific set of tools that a server exposes by listing their names under only.\ntools:\n  - name: github\n    mcp_ws: wss://example.org/mcp\n    only:\n      - safe_tool\n      - high_value_tool\nIf you specify both options at once, you’ll get exactly the tools from the only list that aren’t also excluded.",
    "crumbs": [
      "Tools",
      "MCP"
    ]
  },
  {
    "objectID": "tools/04_mcp.html#safety-and-trust",
    "href": "tools/04_mcp.html#safety-and-trust",
    "title": "Tools: Model Context Protocol (mcp)",
    "section": "",
    "text": "Warning\n\n\n\nWhile powerful, the MCP protocol carries significant security risks. Treat MCP integration as a high-trust capability. Never connect to untrusted servers; a malicious server could exfiltrate data or perform unwanted actions. Lectic’s safety mechanisms reduce mistakes from a well‑behaved LLM, not attacks from a hostile server.\n\n\n\n\nJust like with the exec tool, you can use the tool_use_pre hook to implement confirmation dialogs or logic. See Hooks for examples.\n\n\n\nFor local mcp_command tools, you can specify a sandbox command. This command will be used to launch the MCP server process in a controlled and isolated environment, limiting its access to your system. Arguments are supported (e.g. sandbox: wrapper.sh --strict).\nSee the documentation for the Exec Tool for more details on how sandboxing scripts work, and the Custom Sandboxing recipe for examples.\nYou can also set a default sandbox at the top level (sandbox) or on the interlocutor object. If set, it applies to all local mcp_command tools that don’t specify their own. Tool-level sandbox wins over both defaults.",
    "crumbs": [
      "Tools",
      "MCP"
    ]
  },
  {
    "objectID": "tools/01_overview.html",
    "href": "tools/01_overview.html",
    "title": "Tools Overview",
    "section": "",
    "text": "Tools let your LLM do things. Instead of stopping at text, it can run a command, query a database, call another agent, or reach out to a service.\n\n\n\n\n\nTool\nPurpose\nMinimal Config\n\n\n\n\nexec\nRun commands and scripts\nexec: date\n\n\nsqlite\nQuery SQLite databases\nsqlite: ./data.db\n\n\nmcp\nConnect to MCP servers\nmcp_command: npx ...\n\n\nagent\nCall another interlocutor\nagent: OtherName\n\n\nthink\nPrivate reasoning scratchpad\nthink_about: the problem\n\n\nserve\nServe HTML to browser\nserve_on_port: 8080\n\n\nnative\nProvider built-ins (search, code)\nnative: search\n\n\n\n\n\n\nIn Lectic, you configure tools for each interlocutor in the YAML frontmatter. A tool call follows a four-step process:\n\nUser Prompt: You ask something that requires a tool.\nLLM Tool Call: The LLM outputs a block indicating which tool to use.\nLectic Executes: Lectic runs the tool and captures output.\nLLM Response: The tool output goes back to the LLM, which answers.\n\n\n\n\nLectic uses XML blocks for tool calls:\n&lt;tool-call with=\"tool_name\"&gt;\n&lt;arguments&gt;\n  &lt;!-- one element per parameter --&gt;\n&lt;/arguments&gt;\n&lt;results&gt;\n  &lt;!-- filled by Lectic after execution --&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\nYou’ll see these in assistant blocks. Lectic writes the block when the model requests a tool, then appends results after running it.\n\n\nConfiguration:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  tools:\n    - exec: date\n      name: get_date\n---\n\nWhat's the date today?\nResult:\n:::Assistant\n\n&lt;tool-call with=\"get_date\"&gt;\n&lt;arguments&gt;&lt;argv&gt;[ ]&lt;/argv&gt;&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆&lt;stdout&gt;Fri Mar 15 14:35:18 PDT 2024&lt;/stdout&gt;\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\nToday is March 15th, 2024.\n\n:::\n\n\n\n\nWhen an LLM uses multiple tools in one turn, Lectic runs them concurrently. This speeds up tasks that gather information from several sources.\n\n\n\nReuse tool sets across interlocutors by defining named kits.\nKits can also include an optional description, which is shown in editor hovers and autocomplete.\nkits:\n  - name: typescript_tools\n    description: TypeScript checks (tsc + eslint)\n    tools:\n      - exec: tsc --noEmit\n        name: typecheck\n      - exec: eslint\n        name: lint\n\ninterlocutor:\n  name: Assistant\n  prompt: You help with TypeScript.\n  tools:\n    - kit: typescript_tools\n    - exec: cat\n      name: read_file\n\n\n\nThe tool_use_pre hook fires after parameters are collected but before execution. If the hook exits non-zero, the call is blocked:\ninterlocutor:\n  tools:\n    - exec: rm\n      name: delete\n      hooks:\n        - on: tool_use_pre\n          do: ~/.config/lectic/confirm.sh\nSee Hooks for details.\n\n\n\nEach tool type has its own detailed guide:\n\nExec: Shell commands and scripts. The most versatile tool — anything you can run from the command line, your LLM can run too.\nSQLite: Direct database queries. Schema is auto-introspected and provided to the LLM.\nMCP: Model Context Protocol servers. Connect to a growing ecosystem of pre-built tools and services.\nAgent: Multi-LLM workflows. One interlocutor can delegate to another, enabling specialized agents.\nOther Tools: The think tool for reasoning, serve for rendering HTML, and native for provider built-ins like web search.\n\n\n\n\n\n\n\nNote\n\n\n\nNative tools (native: search, native: code) do not support hooks.",
    "crumbs": [
      "Tools",
      "Overview"
    ]
  },
  {
    "objectID": "tools/01_overview.html#quick-reference",
    "href": "tools/01_overview.html#quick-reference",
    "title": "Tools Overview",
    "section": "",
    "text": "Tool\nPurpose\nMinimal Config\n\n\n\n\nexec\nRun commands and scripts\nexec: date\n\n\nsqlite\nQuery SQLite databases\nsqlite: ./data.db\n\n\nmcp\nConnect to MCP servers\nmcp_command: npx ...\n\n\nagent\nCall another interlocutor\nagent: OtherName\n\n\nthink\nPrivate reasoning scratchpad\nthink_about: the problem\n\n\nserve\nServe HTML to browser\nserve_on_port: 8080\n\n\nnative\nProvider built-ins (search, code)\nnative: search",
    "crumbs": [
      "Tools",
      "Overview"
    ]
  },
  {
    "objectID": "tools/01_overview.html#how-tool-calls-work",
    "href": "tools/01_overview.html#how-tool-calls-work",
    "title": "Tools Overview",
    "section": "",
    "text": "In Lectic, you configure tools for each interlocutor in the YAML frontmatter. A tool call follows a four-step process:\n\nUser Prompt: You ask something that requires a tool.\nLLM Tool Call: The LLM outputs a block indicating which tool to use.\nLectic Executes: Lectic runs the tool and captures output.\nLLM Response: The tool output goes back to the LLM, which answers.",
    "crumbs": [
      "Tools",
      "Overview"
    ]
  },
  {
    "objectID": "tools/01_overview.html#tool-call-syntax",
    "href": "tools/01_overview.html#tool-call-syntax",
    "title": "Tools Overview",
    "section": "",
    "text": "Lectic uses XML blocks for tool calls:\n&lt;tool-call with=\"tool_name\"&gt;\n&lt;arguments&gt;\n  &lt;!-- one element per parameter --&gt;\n&lt;/arguments&gt;\n&lt;results&gt;\n  &lt;!-- filled by Lectic after execution --&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\nYou’ll see these in assistant blocks. Lectic writes the block when the model requests a tool, then appends results after running it.\n\n\nConfiguration:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  tools:\n    - exec: date\n      name: get_date\n---\n\nWhat's the date today?\nResult:\n:::Assistant\n\n&lt;tool-call with=\"get_date\"&gt;\n&lt;arguments&gt;&lt;argv&gt;[ ]&lt;/argv&gt;&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆&lt;stdout&gt;Fri Mar 15 14:35:18 PDT 2024&lt;/stdout&gt;\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\nToday is March 15th, 2024.\n\n:::",
    "crumbs": [
      "Tools",
      "Overview"
    ]
  },
  {
    "objectID": "tools/01_overview.html#parallel-execution",
    "href": "tools/01_overview.html#parallel-execution",
    "title": "Tools Overview",
    "section": "",
    "text": "When an LLM uses multiple tools in one turn, Lectic runs them concurrently. This speeds up tasks that gather information from several sources.",
    "crumbs": [
      "Tools",
      "Overview"
    ]
  },
  {
    "objectID": "tools/01_overview.html#tool-kits",
    "href": "tools/01_overview.html#tool-kits",
    "title": "Tools Overview",
    "section": "",
    "text": "Reuse tool sets across interlocutors by defining named kits.\nKits can also include an optional description, which is shown in editor hovers and autocomplete.\nkits:\n  - name: typescript_tools\n    description: TypeScript checks (tsc + eslint)\n    tools:\n      - exec: tsc --noEmit\n        name: typecheck\n      - exec: eslint\n        name: lint\n\ninterlocutor:\n  name: Assistant\n  prompt: You help with TypeScript.\n  tools:\n    - kit: typescript_tools\n    - exec: cat\n      name: read_file",
    "crumbs": [
      "Tools",
      "Overview"
    ]
  },
  {
    "objectID": "tools/01_overview.html#hooks",
    "href": "tools/01_overview.html#hooks",
    "title": "Tools Overview",
    "section": "",
    "text": "The tool_use_pre hook fires after parameters are collected but before execution. If the hook exits non-zero, the call is blocked:\ninterlocutor:\n  tools:\n    - exec: rm\n      name: delete\n      hooks:\n        - on: tool_use_pre\n          do: ~/.config/lectic/confirm.sh\nSee Hooks for details.",
    "crumbs": [
      "Tools",
      "Overview"
    ]
  },
  {
    "objectID": "tools/01_overview.html#tool-guides",
    "href": "tools/01_overview.html#tool-guides",
    "title": "Tools Overview",
    "section": "",
    "text": "Each tool type has its own detailed guide:\n\nExec: Shell commands and scripts. The most versatile tool — anything you can run from the command line, your LLM can run too.\nSQLite: Direct database queries. Schema is auto-introspected and provided to the LLM.\nMCP: Model Context Protocol servers. Connect to a growing ecosystem of pre-built tools and services.\nAgent: Multi-LLM workflows. One interlocutor can delegate to another, enabling specialized agents.\nOther Tools: The think tool for reasoning, serve for rendering HTML, and native for provider built-ins like web search.\n\n\n\n\n\n\n\nNote\n\n\n\nNative tools (native: search, native: code) do not support hooks.",
    "crumbs": [
      "Tools",
      "Overview"
    ]
  },
  {
    "objectID": "tools/03_sqlite.html",
    "href": "tools/03_sqlite.html",
    "title": "Tools: SQLite Query",
    "section": "",
    "text": "The sqlite tool gives your LLM the ability to query SQLite databases directly. This is a powerful way to provide access to structured data, allowing the LLM to perform data analysis, answer questions from a knowledge base, or check the state of an application.\n\n\n\n\n\n\nNoteWhy not just use exec: sqlite3?\n\n\n\nYou could give the LLM a shell command like exec: sqlite3 ./data.db, but the built-in sqlite tool does more:\n\nSchema introspection: Lectic reads the database schema and includes it in the tool description, so the LLM knows what tables and columns exist without you having to explain them.\nResult limiting: Large query results can overwhelm the context window. The limit parameter caps response size and returns an error if exceeded, prompting the LLM to write a more selective query.\nYAML output: Results are formatted as YAML, which LLMs tend to parse more reliably than raw SQL output.\nAtomic transactions: Each tool call runs in a transaction. If anything fails, changes are rolled back.\nNo external binary: Lectic uses Bun’s built-in SQLite support, so you don’t need sqlite3 installed.\n\n\n\n\n\nThe snippets below show only the tool definition. They assume you have an interlocutor with a valid prompt and model configuration. See Getting Started for a full header example.\nTo configure the tool, you must provide the path to the SQLite database file. The database schema is automatically introspected and provided to the LLM, so it knows what tables and columns are available.\ntools:\n  - sqlite: ./products.db\n    name: db_query\n    limit: 10000\n    readonly: true\n    details: &gt;\n      Contains the full product catalog and inventory levels. Use this to\n      answer questions about what is in stock.\n    extensions:\n      - ./lib/vector0\n      - ./lib/math\nThe path can include environment variables (for example, $DATA_DIR/main.db), which Lectic expands.\n\n\n\nsqlite: (Required) Path to the SQLite database file.\nname: A custom tool name.\nreadonly: To set the database as read only.\nlimit: Maximum size of the serialized response in bytes. Large results raise an error instead of flooding the model.\ndetails: Extra high‑level context for the model. String, file:, or exec: are accepted. See External Prompts.\nextensions: A list of SQLite extension libraries to load before queries.\n\n\n\n\nConfiguration:\ntools:\n  - sqlite: ./chinook.db\n    name: chinook\nConversation:\nWho are the top 5 artists by number of tracks?\n\n:::Assistant\n\nI will query the database to find out.\n\n&lt;tool-call with=\"chinook\"&gt;\n&lt;arguments&gt;\n&lt;query&gt;\n┆SELECT\n┆ar.Name,\n┆COUNT(t.TrackId) AS TrackCount\n┆FROM Artists ar\n┆JOIN Albums al ON ar.ArtistId = al.ArtistId\n┆JOIN Tracks t ON al.AlbumId = t.AlbumId\n┆GROUP BY ar.Name\n┆ORDER BY TrackCount DESC\n┆LIMIT 5;\n&lt;/query&gt;\n&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆- Name: Iron Maiden\n┆TrackCount: 213\n┆- Name: Led Zeppelin\n┆TrackCount: 114\n┆- Name: Metallica\n┆TrackCount: 112\n┆- Name: U2\n┆TrackCount: 110\n┆- Name: Deep Purple\n┆TrackCount: 92\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\nBased on the data, the top artists by track count are Iron Maiden, Led\nZeppelin, Metallica, U2, and Deep Purple.\n\n:::\n\n\n\n\nWrites are allowed by default. Each tool call runs inside a transaction and is atomic. If any statement in the call fails, Lectic rolls back the entire call, so the database is unchanged.\n\n\n\nThe limit parameter caps the size of the serialized YAML that Lectic returns. If a result exceeds the cap, the tool raises an error. Tighten your query (for example, add LIMIT or select fewer columns) to stay under the cap.\n\n\n\nYou can load extensions by path before queries run. On macOS, note that the system SQLite build may restrict loading extensions. Consult the Bun SQLite extension documentation if you hit issues.",
    "crumbs": [
      "Tools",
      "SQLite"
    ]
  },
  {
    "objectID": "tools/03_sqlite.html#configuration",
    "href": "tools/03_sqlite.html#configuration",
    "title": "Tools: SQLite Query",
    "section": "",
    "text": "The snippets below show only the tool definition. They assume you have an interlocutor with a valid prompt and model configuration. See Getting Started for a full header example.\nTo configure the tool, you must provide the path to the SQLite database file. The database schema is automatically introspected and provided to the LLM, so it knows what tables and columns are available.\ntools:\n  - sqlite: ./products.db\n    name: db_query\n    limit: 10000\n    readonly: true\n    details: &gt;\n      Contains the full product catalog and inventory levels. Use this to\n      answer questions about what is in stock.\n    extensions:\n      - ./lib/vector0\n      - ./lib/math\nThe path can include environment variables (for example, $DATA_DIR/main.db), which Lectic expands.\n\n\n\nsqlite: (Required) Path to the SQLite database file.\nname: A custom tool name.\nreadonly: To set the database as read only.\nlimit: Maximum size of the serialized response in bytes. Large results raise an error instead of flooding the model.\ndetails: Extra high‑level context for the model. String, file:, or exec: are accepted. See External Prompts.\nextensions: A list of SQLite extension libraries to load before queries.\n\n\n\n\nConfiguration:\ntools:\n  - sqlite: ./chinook.db\n    name: chinook\nConversation:\nWho are the top 5 artists by number of tracks?\n\n:::Assistant\n\nI will query the database to find out.\n\n&lt;tool-call with=\"chinook\"&gt;\n&lt;arguments&gt;\n&lt;query&gt;\n┆SELECT\n┆ar.Name,\n┆COUNT(t.TrackId) AS TrackCount\n┆FROM Artists ar\n┆JOIN Albums al ON ar.ArtistId = al.ArtistId\n┆JOIN Tracks t ON al.AlbumId = t.AlbumId\n┆GROUP BY ar.Name\n┆ORDER BY TrackCount DESC\n┆LIMIT 5;\n&lt;/query&gt;\n&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆- Name: Iron Maiden\n┆TrackCount: 213\n┆- Name: Led Zeppelin\n┆TrackCount: 114\n┆- Name: Metallica\n┆TrackCount: 112\n┆- Name: U2\n┆TrackCount: 110\n┆- Name: Deep Purple\n┆TrackCount: 92\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\nBased on the data, the top artists by track count are Iron Maiden, Led\nZeppelin, Metallica, U2, and Deep Purple.\n\n:::",
    "crumbs": [
      "Tools",
      "SQLite"
    ]
  },
  {
    "objectID": "tools/03_sqlite.html#writes-and-transactions",
    "href": "tools/03_sqlite.html#writes-and-transactions",
    "title": "Tools: SQLite Query",
    "section": "",
    "text": "Writes are allowed by default. Each tool call runs inside a transaction and is atomic. If any statement in the call fails, Lectic rolls back the entire call, so the database is unchanged.",
    "crumbs": [
      "Tools",
      "SQLite"
    ]
  },
  {
    "objectID": "tools/03_sqlite.html#limits-and-large-results",
    "href": "tools/03_sqlite.html#limits-and-large-results",
    "title": "Tools: SQLite Query",
    "section": "",
    "text": "The limit parameter caps the size of the serialized YAML that Lectic returns. If a result exceeds the cap, the tool raises an error. Tighten your query (for example, add LIMIT or select fewer columns) to stay under the cap.",
    "crumbs": [
      "Tools",
      "SQLite"
    ]
  },
  {
    "objectID": "tools/03_sqlite.html#extensions",
    "href": "tools/03_sqlite.html#extensions",
    "title": "Tools: SQLite Query",
    "section": "",
    "text": "You can load extensions by path before queries run. On macOS, note that the system SQLite build may restrict loading extensions. Consult the Bun SQLite extension documentation if you hit issues.",
    "crumbs": [
      "Tools",
      "SQLite"
    ]
  },
  {
    "objectID": "03_editor_integration.html",
    "href": "03_editor_integration.html",
    "title": "Editor Integration",
    "section": "",
    "text": "Lectic is designed to work with your editor, not replace it. The core workflow is: edit a file, press a key, watch the response stream in. No mode switching, no separate chat window—you stay in your editor the whole time.\nThis page covers setup for several editors. If you just want to get started quickly:\n\nNeovim: Install the plugin from extra/lectic.nvim, add the LSP config below, and use &lt;localleader&gt;l to submit.\nVS Code: Install the extension from releases and use Cmd+L / Alt+L.\nOther editors: Run lectic -i file.lec from a keybinding.\n\n\n\nLectic includes a Language Server Protocol (LSP) server. You don’t strictly need it—Lectic works fine as a command-line tool—but the LSP makes editing .lec files much more pleasant:\n\nCompletions for directives, macro names, YAML fields, model names, and tool types. Start typing and the LSP suggests what comes next.\nDiagnostics that catch YAML errors, missing configuration, and duplicate names before you run the file.\nFolding for tool-call blocks. Tool output can be verbose; folding keeps the conversation readable.\nHover information showing what a directive does or what a macro expands to.\nGo to definition for macros, kits, and interlocutors defined in config files.\n\nStart it with:\nlectic lsp\nThe server uses stdio transport and works with any LSP-capable editor.\n\n\n\nThe repository includes a full-featured plugin at extra/lectic.nvim.\n\n\nlazy.nvim:\n{\n  'gleachkr/lectic',\n  name = 'lectic.nvim',\n  config = function(plugin)\n    vim.opt.rtp:append(plugin.dir .. \"/extra/lectic.nvim\")\n  end\n}\nvim-plug:\nPlug 'gleachkr/lectic', { 'rtp': 'extra/lectic.nvim' }\n\n\n\n\nFiletype detection for .lec and .lectic files\nAsync submission — send conversations without blocking\nStreaming responses — watch the response appear in real-time\nVisual feedback — spinner while processing\nResponse highlighting — distinguish LLM blocks from your text\nTool call folding — collapsed by default, showing tool name\nSelection explanation — select text, ask for elaboration\n\n\n\n\n\n\n\nKey\nMode\nAction\n\n\n\n\n&lt;localleader&gt;l\nNormal\nSubmit conversation\n\n\n&lt;localleader&gt;c\nNormal\nCancel generation\n\n\n&lt;localleader&gt;e\nVisual\nExplain selection\n\n\n\nCustomize with:\nvim.g.lectic_key_submit = '&lt;Leader&gt;l'\nvim.g.lectic_key_cancel_submit = '&lt;Leader&gt;c'\nvim.g.lectic_key_explain = '&lt;Leader&gt;e'\n\n\n\nThe plugin handles filetype detection. For LSP features, add:\nvim.api.nvim_create_autocmd(\"FileType\", {\n  pattern = { \"lectic\", \"markdown.lectic\", \"lectic.markdown\" },\n  callback = function(args)\n    vim.lsp.start({\n      name = \"lectic\",\n      cmd = { \"lectic\", \"lsp\" },\n      root_dir = vim.fs.root(args.buf, { \".git\", \"lectic.yaml\" })\n                 or vim.fn.getcwd(),\n      single_file_support = true,\n    })\n  end,\n})\nFor LSP-based folding:\nvim.opt.foldexpr = 'vim.lsp.foldexpr()'\n\n\n\n\nAn extension is available at extra/lectic.vscode. VSIX files are distributed with releases.\n\n\n\nGenerate Next Response — stream LLM output into the editor\nExplain Selection — rewrite selected text with more detail\nBlock highlighting — visual distinction for response blocks\nTool call folding — collapse verbose tool output\nLSP integration — completions, diagnostics, and hovers\n\n\n\n\n\n\n\nKey\nAction\n\n\n\n\nAlt+L (Cmd+L on macOS)\nGenerate next response\n\n\nAlt+C (Cmd+C on macOS)\nConsolidate\n\n\nAlt+E (Cmd+E on macOS)\nExplain selection\n\n\n\n\n\n\n\nlectic.executablePath: Path to lectic if not in PATH\nlectic.blockBackgroundColor: Background color for ::: blocks\n\n\n\n\n\nAny editor that can run an external command on the current buffer works with Lectic. The basic pattern:\ncat file.lec | lectic &gt; file.lec\nOr use -i for in-place updates:\nlectic -i file.lec\n\n\nA minimal setup using shell-command-on-region:\n(defun lectic-submit ()\n  \"Send the buffer to lectic and replace with output.\"\n  (interactive)\n  (shell-command-on-region\n   (point-min) (point-max)\n   \"lectic\"\n   nil t))\n\n(add-to-list 'auto-mode-alist '(\"\\\\.lec\\\\'\" . markdown-mode))\n(add-hook 'markdown-mode-hook\n          (lambda ()\n            (when (string-match-p \"\\\\.lec\\\\'\" (buffer-file-name))\n              (local-set-key (kbd \"C-c C-l\") 'lectic-submit))))\nFor LSP support, use eglot or lsp-mode:\n;; With eglot\n(add-to-list 'eglot-server-programs\n             '((markdown-mode :language-id \"lectic\")\n               . (\"lectic\" \"lsp\")))\n\n\n\nAdd to languages.toml:\n[[language]]\nname = \"lectic\"\nscope = \"source.lectic\"\nfile-types = [\"lec\", \"lectic\"]\nlanguage-servers = [\"lectic-lsp\"]\ngrammar = \"markdown\"\n\n[language-server.lectic-lsp]\ncommand = \"lectic\"\nargs = [\"lsp\"]\n\n\n\nInstall the LSP package, then add to LSP settings:\n{\n  \"clients\": {\n    \"lectic\": {\n      \"command\": [\"lectic\", \"lsp\"],\n      \"selector\": \"text.html.markdown\",\n      \"file_patterns\": [\"*.lec\"]\n    }\n  }\n}\n\n\n\n\n\nWorking directory matters. When you run lectic -i file.lec, tools and file references resolve relative to the file’s directory. Most editor plugins handle this automatically.\nUse the LSP for header editing. Completions for model names, tool types, and interlocutor properties save time and catch typos.\nFold tool calls. Long tool outputs can obscure the conversation. Both the Neovim and VS Code plugins fold these by default.\nStream for long responses. The -s flag outputs just the new response, which editor plugins use to stream incrementally.",
    "crumbs": [
      "Editor Integration"
    ]
  },
  {
    "objectID": "03_editor_integration.html#the-lsp-server",
    "href": "03_editor_integration.html#the-lsp-server",
    "title": "Editor Integration",
    "section": "",
    "text": "Lectic includes a Language Server Protocol (LSP) server. You don’t strictly need it—Lectic works fine as a command-line tool—but the LSP makes editing .lec files much more pleasant:\n\nCompletions for directives, macro names, YAML fields, model names, and tool types. Start typing and the LSP suggests what comes next.\nDiagnostics that catch YAML errors, missing configuration, and duplicate names before you run the file.\nFolding for tool-call blocks. Tool output can be verbose; folding keeps the conversation readable.\nHover information showing what a directive does or what a macro expands to.\nGo to definition for macros, kits, and interlocutors defined in config files.\n\nStart it with:\nlectic lsp\nThe server uses stdio transport and works with any LSP-capable editor.",
    "crumbs": [
      "Editor Integration"
    ]
  },
  {
    "objectID": "03_editor_integration.html#neovim",
    "href": "03_editor_integration.html#neovim",
    "title": "Editor Integration",
    "section": "",
    "text": "The repository includes a full-featured plugin at extra/lectic.nvim.\n\n\nlazy.nvim:\n{\n  'gleachkr/lectic',\n  name = 'lectic.nvim',\n  config = function(plugin)\n    vim.opt.rtp:append(plugin.dir .. \"/extra/lectic.nvim\")\n  end\n}\nvim-plug:\nPlug 'gleachkr/lectic', { 'rtp': 'extra/lectic.nvim' }\n\n\n\n\nFiletype detection for .lec and .lectic files\nAsync submission — send conversations without blocking\nStreaming responses — watch the response appear in real-time\nVisual feedback — spinner while processing\nResponse highlighting — distinguish LLM blocks from your text\nTool call folding — collapsed by default, showing tool name\nSelection explanation — select text, ask for elaboration\n\n\n\n\n\n\n\nKey\nMode\nAction\n\n\n\n\n&lt;localleader&gt;l\nNormal\nSubmit conversation\n\n\n&lt;localleader&gt;c\nNormal\nCancel generation\n\n\n&lt;localleader&gt;e\nVisual\nExplain selection\n\n\n\nCustomize with:\nvim.g.lectic_key_submit = '&lt;Leader&gt;l'\nvim.g.lectic_key_cancel_submit = '&lt;Leader&gt;c'\nvim.g.lectic_key_explain = '&lt;Leader&gt;e'\n\n\n\nThe plugin handles filetype detection. For LSP features, add:\nvim.api.nvim_create_autocmd(\"FileType\", {\n  pattern = { \"lectic\", \"markdown.lectic\", \"lectic.markdown\" },\n  callback = function(args)\n    vim.lsp.start({\n      name = \"lectic\",\n      cmd = { \"lectic\", \"lsp\" },\n      root_dir = vim.fs.root(args.buf, { \".git\", \"lectic.yaml\" })\n                 or vim.fn.getcwd(),\n      single_file_support = true,\n    })\n  end,\n})\nFor LSP-based folding:\nvim.opt.foldexpr = 'vim.lsp.foldexpr()'",
    "crumbs": [
      "Editor Integration"
    ]
  },
  {
    "objectID": "03_editor_integration.html#vs-code",
    "href": "03_editor_integration.html#vs-code",
    "title": "Editor Integration",
    "section": "",
    "text": "An extension is available at extra/lectic.vscode. VSIX files are distributed with releases.\n\n\n\nGenerate Next Response — stream LLM output into the editor\nExplain Selection — rewrite selected text with more detail\nBlock highlighting — visual distinction for response blocks\nTool call folding — collapse verbose tool output\nLSP integration — completions, diagnostics, and hovers\n\n\n\n\n\n\n\nKey\nAction\n\n\n\n\nAlt+L (Cmd+L on macOS)\nGenerate next response\n\n\nAlt+C (Cmd+C on macOS)\nConsolidate\n\n\nAlt+E (Cmd+E on macOS)\nExplain selection\n\n\n\n\n\n\n\nlectic.executablePath: Path to lectic if not in PATH\nlectic.blockBackgroundColor: Background color for ::: blocks",
    "crumbs": [
      "Editor Integration"
    ]
  },
  {
    "objectID": "03_editor_integration.html#other-editors",
    "href": "03_editor_integration.html#other-editors",
    "title": "Editor Integration",
    "section": "",
    "text": "Any editor that can run an external command on the current buffer works with Lectic. The basic pattern:\ncat file.lec | lectic &gt; file.lec\nOr use -i for in-place updates:\nlectic -i file.lec\n\n\nA minimal setup using shell-command-on-region:\n(defun lectic-submit ()\n  \"Send the buffer to lectic and replace with output.\"\n  (interactive)\n  (shell-command-on-region\n   (point-min) (point-max)\n   \"lectic\"\n   nil t))\n\n(add-to-list 'auto-mode-alist '(\"\\\\.lec\\\\'\" . markdown-mode))\n(add-hook 'markdown-mode-hook\n          (lambda ()\n            (when (string-match-p \"\\\\.lec\\\\'\" (buffer-file-name))\n              (local-set-key (kbd \"C-c C-l\") 'lectic-submit))))\nFor LSP support, use eglot or lsp-mode:\n;; With eglot\n(add-to-list 'eglot-server-programs\n             '((markdown-mode :language-id \"lectic\")\n               . (\"lectic\" \"lsp\")))\n\n\n\nAdd to languages.toml:\n[[language]]\nname = \"lectic\"\nscope = \"source.lectic\"\nfile-types = [\"lec\", \"lectic\"]\nlanguage-servers = [\"lectic-lsp\"]\ngrammar = \"markdown\"\n\n[language-server.lectic-lsp]\ncommand = \"lectic\"\nargs = [\"lsp\"]\n\n\n\nInstall the LSP package, then add to LSP settings:\n{\n  \"clients\": {\n    \"lectic\": {\n      \"command\": [\"lectic\", \"lsp\"],\n      \"selector\": \"text.html.markdown\",\n      \"file_patterns\": [\"*.lec\"]\n    }\n  }\n}",
    "crumbs": [
      "Editor Integration"
    ]
  },
  {
    "objectID": "03_editor_integration.html#tips",
    "href": "03_editor_integration.html#tips",
    "title": "Editor Integration",
    "section": "",
    "text": "Working directory matters. When you run lectic -i file.lec, tools and file references resolve relative to the file’s directory. Most editor plugins handle this automatically.\nUse the LSP for header editing. Completions for model names, tool types, and interlocutor properties save time and catch typos.\nFold tool calls. Long tool outputs can obscure the conversation. Both the Neovim and VS Code plugins fold these by default.\nStream for long responses. The -s flag outputs just the new response, which editor plugins use to stream incrementally.",
    "crumbs": [
      "Editor Integration"
    ]
  },
  {
    "objectID": "reference/02_configuration.html",
    "href": "reference/02_configuration.html",
    "title": "Reference: Configuration Keys",
    "section": "",
    "text": "This document provides a reference for all the keys available in Lectic’s YAML configuration, including the main .lec file frontmatter and any included configuration files.\n\n\n\ninterlocutor: A single object defining the primary LLM speaker.\ninterlocutors: A list of interlocutor objects for multiparty conversations.\nkits: A list of named tool kits you can reference from interlocutors.\nmacros: A list of macro definitions. See Macros.\nhooks: A list of hook definitions. See Hooks.\nsandbox: A default sandbox command string applied to all exec tools and local mcp_command tools, unless overridden by interlocutor.sandbox or a tool’s own sandbox setting.\n\n\n\n\n\nA kit is a named list of tools that can be reused from an interlocutor’s tools array using - kit: &lt;name&gt;.\n\nname: (Required) The kit name.\ntools: (Required) An array of tool definitions.\ndescription: (Optional) Short documentation shown in editor hovers and autocomplete.\n\n\n\n\n\nAn interlocutor object defines a single LLM “personality” or configuration.\n\nname: (Required) The name of the speaker, used in the :::Name response blocks.\nprompt: (Required) The base system prompt that defines the LLM’s personality and instructions. The value can be a string, or it can be loaded from a file (file:./path.txt) or a command (exec:get-prompt). See External Prompts for details and examples.\nhooks: A list of hook definitions. See Hooks. These hooks fire only when this interlocutor is active.\nsandbox: A command string (e.g. /path/to/script.sh or wrapper.sh arg1) to wrap execution for all exec tools and local mcp_command tools used by this interlocutor, unless overridden by the tool’s own sandbox setting. This overrides any top-level sandbox setting.\n\n\n\n\nprovider: The LLM provider to use. Supported values include anthropic, anthropic/bedrock, openai (Responses API), openai/chat (legacy Chat Completions), gemini, ollama, and openrouter.\nmodel: The specific model to use, e.g., claude-3-opus-20240229.\ntemperature: A number between 0 and 1 controlling the randomness of the output.\nmax_tokens: The maximum number of tokens to generate in a response.\nmax_tool_use: The maximum number of tool calls the LLM is allowed to make in a single turn.\nthinking_effort: Optional hint (used by the openai Responses provider, and by gemini-3-pro) about how much effort to spend reasoning. One of none, low, medium, or high.\nthinking_budget: Optional integer token budget for providers that support structured thinking phases (Anthropic, Anthropic/Bedrock, Gemini). Ignored by the openai and openai/chat providers.\n\n\n\nIf you don’t specify provider, Lectic picks a default based on your environment. It checks for known API keys in this order and uses the first one it finds:\n\nANTHROPIC_API_KEY\nGEMINI_API_KEY\nOPENAI_API_KEY\nOPENROUTER_API_KEY\n\nAWS credentials for Bedrock are not considered for auto‑selection. If you want Anthropic via Bedrock, set provider: anthropic/bedrock explicitly and ensure your AWS environment is configured.\nOpenAI has two provider options:\n\nopenai uses the Responses API. You’ll want this for native tools like search and code.\nopenai/chat uses the legacy Chat Completions API. You’ll need this for certain audio workflows that still require chat‑style models.\n\nFor a more detailed discussion of provider and model options, see Providers and Models.\n\n\n\n\n\ntools: A list of tool definitions that this interlocutor can use. The format of each object in the list depends on the tool type. See the Tools section for detailed configuration guides. All tools support a hooks array for tool_use_pre hooks scoped to that particular tool.\n\n\n\nThese keys are shared across multiple tool types:\n\nname: A custom name for the tool. If omitted, a default is derived from the tool type.\nusage: Instructions for the LLM on when and how to use the tool. Accepts a string, file:, or exec: source.\nhooks: A list of hooks scoped to this tool (typically tool_use_pre).\n\n\n\n\nRun commands and scripts.\n\nexec: (Required) The command or inline script to execute. Multi-line values must start with a shebang.\nschema: A map of parameter name → description. When present, the tool takes named string parameters (exposed as env vars). When absent, the tool takes a required arguments array of strings.\nsandbox: Command string to wrap execution. Arguments supported.\ntimeoutSeconds: Seconds to wait before aborting.\nenv: Environment variables to set for the subprocess.\n\n\n\n\nQuery SQLite databases.\n\nsqlite: (Required) Path to the SQLite database file.\nreadonly: Boolean. If true, opens the database in read-only mode.\nlimit: Maximum size of serialized response in bytes.\ndetails: Extra context for the model. Accepts string, file:, or exec:.\nextensions: A list of SQLite extension libraries to load.\n\n\n\n\nCall another interlocutor as a tool.\n\nagent: (Required) The name of the interlocutor to call.\nraw_output: Boolean. If true, includes raw tool call results in the output rather than sanitized text.\n\n\n\n\nConnect to Model Context Protocol servers.\n\nOne of: mcp_command, mcp_ws, mcp_sse, or mcp_shttp.\nargs: Arguments for mcp_command.\nenv: Environment variables for mcp_command.\nheaders: A map of custom headers for mcp_shttp. Values support file: and exec:.\nsandbox: Optional wrapper command to isolate mcp_command servers.\nroots: Optional list of root objects for file access (each with uri and optional name).\nexclude: Optional list of server tool names to blacklist.\nonly: Optional list of server tool names to whitelist.\n\n\n\n\n\nthink_about: (String) Creates a thinking/scratchpad tool with the given prompt.\nserve_on_port: (Integer) Creates a single-use web server on the given port.\nnative: One of search or code. Enables provider built-in tools.\nkit: Name of a tool kit to include.\n\nIf you add keys to an interlocutor object that are not listed in this section, Lectic will still parse the YAML, but the LSP marks those properties as unknown with a warning. This is usually a sign of a typo in a key name.\n\n\n\n\n\n\n\nname: (Required) The name of the macro, used when invoking it with :name[] or :name[args].\nexpansion: (Optional) The content to be expanded. Can be a string, or loaded via file: or exec:. Equivalent to post if provided. See External Prompts for details about file: and exec:.\npre: (Optional) Expansion content for the pre-order phase.\npost: (Optional) Expansion content for the post-order phase.\nenv: (Optional) A dictionary of environment variables to be set during the macro’s execution. These are merged with any arguments provided at the call site.\n\n\n\n\n\n\non: (Required) A single event name or a list of event names to trigger the hook. Supported events are user_message, assistant_message, error, and tool_use_pre.\ndo: (Required) The command or inline script to run when the event occurs. If multi‑line, it must start with a shebang (e.g., #!/bin/bash). Event context is provided as environment variables. See the Hooks guide for details.\ninline: (Optional) Boolean. If true, the output of the hook is captured and injected into the conversation. Defaults to false.\nname: (Optional) A name for the hook. Used for merging and overriding hooks from different configuration sources.\nenv: (Optional) A dictionary of environment variables to be set when the hook runs.",
    "crumbs": [
      "Reference",
      "Configuration Keys"
    ]
  },
  {
    "objectID": "reference/02_configuration.html#top-level-keys",
    "href": "reference/02_configuration.html#top-level-keys",
    "title": "Reference: Configuration Keys",
    "section": "",
    "text": "interlocutor: A single object defining the primary LLM speaker.\ninterlocutors: A list of interlocutor objects for multiparty conversations.\nkits: A list of named tool kits you can reference from interlocutors.\nmacros: A list of macro definitions. See Macros.\nhooks: A list of hook definitions. See Hooks.\nsandbox: A default sandbox command string applied to all exec tools and local mcp_command tools, unless overridden by interlocutor.sandbox or a tool’s own sandbox setting.",
    "crumbs": [
      "Reference",
      "Configuration Keys"
    ]
  },
  {
    "objectID": "reference/02_configuration.html#the-kit-object",
    "href": "reference/02_configuration.html#the-kit-object",
    "title": "Reference: Configuration Keys",
    "section": "",
    "text": "A kit is a named list of tools that can be reused from an interlocutor’s tools array using - kit: &lt;name&gt;.\n\nname: (Required) The kit name.\ntools: (Required) An array of tool definitions.\ndescription: (Optional) Short documentation shown in editor hovers and autocomplete.",
    "crumbs": [
      "Reference",
      "Configuration Keys"
    ]
  },
  {
    "objectID": "reference/02_configuration.html#the-interlocutor-object",
    "href": "reference/02_configuration.html#the-interlocutor-object",
    "title": "Reference: Configuration Keys",
    "section": "",
    "text": "An interlocutor object defines a single LLM “personality” or configuration.\n\nname: (Required) The name of the speaker, used in the :::Name response blocks.\nprompt: (Required) The base system prompt that defines the LLM’s personality and instructions. The value can be a string, or it can be loaded from a file (file:./path.txt) or a command (exec:get-prompt). See External Prompts for details and examples.\nhooks: A list of hook definitions. See Hooks. These hooks fire only when this interlocutor is active.\nsandbox: A command string (e.g. /path/to/script.sh or wrapper.sh arg1) to wrap execution for all exec tools and local mcp_command tools used by this interlocutor, unless overridden by the tool’s own sandbox setting. This overrides any top-level sandbox setting.\n\n\n\n\nprovider: The LLM provider to use. Supported values include anthropic, anthropic/bedrock, openai (Responses API), openai/chat (legacy Chat Completions), gemini, ollama, and openrouter.\nmodel: The specific model to use, e.g., claude-3-opus-20240229.\ntemperature: A number between 0 and 1 controlling the randomness of the output.\nmax_tokens: The maximum number of tokens to generate in a response.\nmax_tool_use: The maximum number of tool calls the LLM is allowed to make in a single turn.\nthinking_effort: Optional hint (used by the openai Responses provider, and by gemini-3-pro) about how much effort to spend reasoning. One of none, low, medium, or high.\nthinking_budget: Optional integer token budget for providers that support structured thinking phases (Anthropic, Anthropic/Bedrock, Gemini). Ignored by the openai and openai/chat providers.\n\n\n\nIf you don’t specify provider, Lectic picks a default based on your environment. It checks for known API keys in this order and uses the first one it finds:\n\nANTHROPIC_API_KEY\nGEMINI_API_KEY\nOPENAI_API_KEY\nOPENROUTER_API_KEY\n\nAWS credentials for Bedrock are not considered for auto‑selection. If you want Anthropic via Bedrock, set provider: anthropic/bedrock explicitly and ensure your AWS environment is configured.\nOpenAI has two provider options:\n\nopenai uses the Responses API. You’ll want this for native tools like search and code.\nopenai/chat uses the legacy Chat Completions API. You’ll need this for certain audio workflows that still require chat‑style models.\n\nFor a more detailed discussion of provider and model options, see Providers and Models.\n\n\n\n\n\ntools: A list of tool definitions that this interlocutor can use. The format of each object in the list depends on the tool type. See the Tools section for detailed configuration guides. All tools support a hooks array for tool_use_pre hooks scoped to that particular tool.\n\n\n\nThese keys are shared across multiple tool types:\n\nname: A custom name for the tool. If omitted, a default is derived from the tool type.\nusage: Instructions for the LLM on when and how to use the tool. Accepts a string, file:, or exec: source.\nhooks: A list of hooks scoped to this tool (typically tool_use_pre).\n\n\n\n\nRun commands and scripts.\n\nexec: (Required) The command or inline script to execute. Multi-line values must start with a shebang.\nschema: A map of parameter name → description. When present, the tool takes named string parameters (exposed as env vars). When absent, the tool takes a required arguments array of strings.\nsandbox: Command string to wrap execution. Arguments supported.\ntimeoutSeconds: Seconds to wait before aborting.\nenv: Environment variables to set for the subprocess.\n\n\n\n\nQuery SQLite databases.\n\nsqlite: (Required) Path to the SQLite database file.\nreadonly: Boolean. If true, opens the database in read-only mode.\nlimit: Maximum size of serialized response in bytes.\ndetails: Extra context for the model. Accepts string, file:, or exec:.\nextensions: A list of SQLite extension libraries to load.\n\n\n\n\nCall another interlocutor as a tool.\n\nagent: (Required) The name of the interlocutor to call.\nraw_output: Boolean. If true, includes raw tool call results in the output rather than sanitized text.\n\n\n\n\nConnect to Model Context Protocol servers.\n\nOne of: mcp_command, mcp_ws, mcp_sse, or mcp_shttp.\nargs: Arguments for mcp_command.\nenv: Environment variables for mcp_command.\nheaders: A map of custom headers for mcp_shttp. Values support file: and exec:.\nsandbox: Optional wrapper command to isolate mcp_command servers.\nroots: Optional list of root objects for file access (each with uri and optional name).\nexclude: Optional list of server tool names to blacklist.\nonly: Optional list of server tool names to whitelist.\n\n\n\n\n\nthink_about: (String) Creates a thinking/scratchpad tool with the given prompt.\nserve_on_port: (Integer) Creates a single-use web server on the given port.\nnative: One of search or code. Enables provider built-in tools.\nkit: Name of a tool kit to include.\n\nIf you add keys to an interlocutor object that are not listed in this section, Lectic will still parse the YAML, but the LSP marks those properties as unknown with a warning. This is usually a sign of a typo in a key name.",
    "crumbs": [
      "Reference",
      "Configuration Keys"
    ]
  },
  {
    "objectID": "reference/02_configuration.html#the-macro-object",
    "href": "reference/02_configuration.html#the-macro-object",
    "title": "Reference: Configuration Keys",
    "section": "",
    "text": "name: (Required) The name of the macro, used when invoking it with :name[] or :name[args].\nexpansion: (Optional) The content to be expanded. Can be a string, or loaded via file: or exec:. Equivalent to post if provided. See External Prompts for details about file: and exec:.\npre: (Optional) Expansion content for the pre-order phase.\npost: (Optional) Expansion content for the post-order phase.\nenv: (Optional) A dictionary of environment variables to be set during the macro’s execution. These are merged with any arguments provided at the call site.",
    "crumbs": [
      "Reference",
      "Configuration Keys"
    ]
  },
  {
    "objectID": "reference/02_configuration.html#the-hook-object",
    "href": "reference/02_configuration.html#the-hook-object",
    "title": "Reference: Configuration Keys",
    "section": "",
    "text": "on: (Required) A single event name or a list of event names to trigger the hook. Supported events are user_message, assistant_message, error, and tool_use_pre.\ndo: (Required) The command or inline script to run when the event occurs. If multi‑line, it must start with a shebang (e.g., #!/bin/bash). Event context is provided as environment variables. See the Hooks guide for details.\ninline: (Optional) Boolean. If true, the output of the hook is captured and injected into the conversation. Defaults to false.\nname: (Optional) A name for the hook. Used for merging and overriding hooks from different configuration sources.\nenv: (Optional) A dictionary of environment variables to be set when the hook runs.",
    "crumbs": [
      "Reference",
      "Configuration Keys"
    ]
  },
  {
    "objectID": "reference/01_cli.html",
    "href": "reference/01_cli.html",
    "title": "Reference: Command‑Line Interface",
    "section": "",
    "text": "The lectic command is the primary way to interact with Lectic. It can read from a file or from standard input, and it offers flags to control how the result is printed or saved.\n\n\nlectic [FLAGS] [OPTIONS] [SUBCOMMAND] [ARGS...]\n\n\n\n\nlectic lsp Start the LSP server. Transport: stdio.\nlectic parse Parse a lectic file into a JSON representation of the parsed file structure. Useful for programmatic analysis and modification.\nFlags:\n\n--yaml: Emit YAML instead of JSON.\n--reverse: Ingest JSON (or YAML) output and reconstruct the original lectic file.\n\nlectic models List available models for providers with detected credentials.\nProviders with API keys in the environment are queried. The chatgpt provider is listed if you have previously logged in (it is not an API key-based provider).\nlectic script Run an ES module file using Lectic’s internal Bun JS runtime. Works as a hashbang interpreter, useful for writing subcommands (see below), hooks, and exec tools. For example:\n#!/bin/env -S lectic script\nconsole.log(\"Hello from a lectic script!\")\n\n\n\n\nLectic supports git-style custom subcommands. If you invoke lectic &lt;command&gt;, Lectic will look for an executable named lectic-&lt;command&gt; in your configuration directory, data directory, or PATH.\nSee Custom Subcommands for a full guide on creating subcommands and adding tab completion for them.\n\n\n\nThe repository includes a bash completion script. See Getting Started for installation instructions.\nThe completion system is extensible. You can write plugins to provide completions for your custom subcommands. See the Custom Subcommands guide for details.\n\n\n\n\n-v, --version Prints the version string.\n-f, --file &lt;PATH&gt; Path to the conversation file (.lec) to process. If omitted, Lectic reads from standard input.\n-i, --inplace &lt;PATH&gt; Read from the given file and update it in place. Mutually exclusive with --file.\n-s, --short Only emit the newly generated assistant message, not the full updated conversation.\n-S, --Short Like --short, but emits only the raw message text (without the :::Speaker wrapper).\n-l, --log &lt;PATH&gt; Write detailed debug logs to the given file.\n-q, --quiet Suppress printing the assistant’s response to stdout.\n-h, --help Show help for all flags and options.\n\n\n\n\n\n–inplace cannot be combined with –file.\n–quiet cannot be combined with –short or –Short.\n\n\n\n\n\nGenerate the next message in a file and update it in place:\nlectic -i conversation.lec\nRead from stdin and write the full result to stdout:\ncat conversation.lec | lectic\nStream just the new assistant message:\nlectic -s -f conversation.lec\nAdd a message from the command line and update the file:\necho \"This is a new message.\" | lectic -i conversation.lec\nList available models for detected providers:\nlectic models\nStart the LSP server (stdio transport):\nlectic lsp\nParse a file to JSON:\nlectic parse -f conversation.lec\nRound-trip a file through parsing and reconstruction:\nlectic parse -f conversation.lec | lectic parse --reverse",
    "crumbs": [
      "Reference",
      "CLI"
    ]
  },
  {
    "objectID": "reference/01_cli.html#usage",
    "href": "reference/01_cli.html#usage",
    "title": "Reference: Command‑Line Interface",
    "section": "",
    "text": "lectic [FLAGS] [OPTIONS] [SUBCOMMAND] [ARGS...]",
    "crumbs": [
      "Reference",
      "CLI"
    ]
  },
  {
    "objectID": "reference/01_cli.html#subcommands",
    "href": "reference/01_cli.html#subcommands",
    "title": "Reference: Command‑Line Interface",
    "section": "",
    "text": "lectic lsp Start the LSP server. Transport: stdio.\nlectic parse Parse a lectic file into a JSON representation of the parsed file structure. Useful for programmatic analysis and modification.\nFlags:\n\n--yaml: Emit YAML instead of JSON.\n--reverse: Ingest JSON (or YAML) output and reconstruct the original lectic file.\n\nlectic models List available models for providers with detected credentials.\nProviders with API keys in the environment are queried. The chatgpt provider is listed if you have previously logged in (it is not an API key-based provider).\nlectic script Run an ES module file using Lectic’s internal Bun JS runtime. Works as a hashbang interpreter, useful for writing subcommands (see below), hooks, and exec tools. For example:\n#!/bin/env -S lectic script\nconsole.log(\"Hello from a lectic script!\")",
    "crumbs": [
      "Reference",
      "CLI"
    ]
  },
  {
    "objectID": "reference/01_cli.html#custom-subcommands",
    "href": "reference/01_cli.html#custom-subcommands",
    "title": "Reference: Command‑Line Interface",
    "section": "",
    "text": "Lectic supports git-style custom subcommands. If you invoke lectic &lt;command&gt;, Lectic will look for an executable named lectic-&lt;command&gt; in your configuration directory, data directory, or PATH.\nSee Custom Subcommands for a full guide on creating subcommands and adding tab completion for them.",
    "crumbs": [
      "Reference",
      "CLI"
    ]
  },
  {
    "objectID": "reference/01_cli.html#bash-completion",
    "href": "reference/01_cli.html#bash-completion",
    "title": "Reference: Command‑Line Interface",
    "section": "",
    "text": "The repository includes a bash completion script. See Getting Started for installation instructions.\nThe completion system is extensible. You can write plugins to provide completions for your custom subcommands. See the Custom Subcommands guide for details.",
    "crumbs": [
      "Reference",
      "CLI"
    ]
  },
  {
    "objectID": "reference/01_cli.html#flags-and-options",
    "href": "reference/01_cli.html#flags-and-options",
    "title": "Reference: Command‑Line Interface",
    "section": "",
    "text": "-v, --version Prints the version string.\n-f, --file &lt;PATH&gt; Path to the conversation file (.lec) to process. If omitted, Lectic reads from standard input.\n-i, --inplace &lt;PATH&gt; Read from the given file and update it in place. Mutually exclusive with --file.\n-s, --short Only emit the newly generated assistant message, not the full updated conversation.\n-S, --Short Like --short, but emits only the raw message text (without the :::Speaker wrapper).\n-l, --log &lt;PATH&gt; Write detailed debug logs to the given file.\n-q, --quiet Suppress printing the assistant’s response to stdout.\n-h, --help Show help for all flags and options.",
    "crumbs": [
      "Reference",
      "CLI"
    ]
  },
  {
    "objectID": "reference/01_cli.html#constraints",
    "href": "reference/01_cli.html#constraints",
    "title": "Reference: Command‑Line Interface",
    "section": "",
    "text": "–inplace cannot be combined with –file.\n–quiet cannot be combined with –short or –Short.",
    "crumbs": [
      "Reference",
      "CLI"
    ]
  },
  {
    "objectID": "reference/01_cli.html#common-examples",
    "href": "reference/01_cli.html#common-examples",
    "title": "Reference: Command‑Line Interface",
    "section": "",
    "text": "Generate the next message in a file and update it in place:\nlectic -i conversation.lec\nRead from stdin and write the full result to stdout:\ncat conversation.lec | lectic\nStream just the new assistant message:\nlectic -s -f conversation.lec\nAdd a message from the command line and update the file:\necho \"This is a new message.\" | lectic -i conversation.lec\nList available models for detected providers:\nlectic models\nStart the LSP server (stdio transport):\nlectic lsp\nParse a file to JSON:\nlectic parse -f conversation.lec\nRound-trip a file through parsing and reconstruction:\nlectic parse -f conversation.lec | lectic parse --reverse",
    "crumbs": [
      "Reference",
      "CLI"
    ]
  },
  {
    "objectID": "06_providers_and_models.html",
    "href": "06_providers_and_models.html",
    "title": "Providers and Models",
    "section": "",
    "text": "Lectic speaks to several providers. You pick a provider and a model in your YAML header, or let Lectic choose a default based on which credentials are available in your environment.\n\n\nIf you do not set provider, Lectic checks for keys in this order and uses the first one it finds:\nAnthropic → Gemini → OpenAI → OpenRouter.\nSet one of these environment variables before you run Lectic:\n\nANTHROPIC_API_KEY\nGEMINI_API_KEY\nOPENAI_API_KEY\nOPENROUTER_API_KEY\n\nAWS credentials for Bedrock are not used for auto-selection. If you want Anthropic via Bedrock, set provider: anthropic/bedrock explicitly and make sure your AWS environment is configured.\n\n\n\nLectic also supports provider: chatgpt, which uses your ChatGPT subscription via the official Codex / ChatGPT OAuth flow.\nNotes:\n\nThis provider is not auto-selected (there is no API key env var to check). If you want it by default, set provider: chatgpt in your lectic.yaml or in the .lec frontmatter.\nOn first use, Lectic opens a browser window for login and stores tokens at $LECTIC_STATE/chatgpt_auth.json (for example, ~/.local/state/lectic/chatgpt_auth.json on Linux).\nLogin starts a local callback server on port 1455. If that port is in use, stop the other process and try again.\nIf you want to “log out”, delete that file.\n\n\n\n\nNot sure which models are available? Run:\nlectic models\nThis queries each provider you have credentials for and prints the available models.\n\n\n\n\n\n\nTip\n\n\n\nThe LSP can also autocomplete model names as you type in the YAML header. See Editor Integration.\n\n\n\n\n\n\n\n\nNote\n\n\n\nprovider: chatgpt models only show up in lectic models after you have logged in at least once (since the login is browser-based, not an API key). If you have not logged in yet, run a .lec file with provider: chatgpt first.\n\n\n\n\n\nOpenAI has two modes in Lectic today.\n\nopenai selects the Responses API. Choose this when you want native tools like search and code.\nopenai/chat selects the legacy Chat Completions API.\n\n\n\n\nThese examples show the minimal configuration for each provider. You can omit provider and model if you want Lectic to pick defaults based on your environment.\nAnthropic (direct API):\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic\nAnthropic via Bedrock:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic/bedrock\n  model: anthropic.claude-3-haiku-20240307-v1:0\nOpenAI (Responses API):\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: openai\nOpenAI Chat Completions (legacy API):\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: openai/chat\nChatGPT (subscription via Codex backend):\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: chatgpt\n  model: gpt-5.1-codex\nGemini:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: gemini\nOpenRouter:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: openrouter\n  model: meta-llama/llama-3.1-70b-instruct\nOllama (local inference):\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: ollama\n  model: llama3.1\n\n\n\nProviders differ in what they accept as input. Here’s a rough guide:\n\n\n\nProvider\nText\nImages\nPDFs\nAudio\nVideo\n\n\n\n\nAnthropic\n✓\n✓\n✓\n✗\n✗\n\n\nGemini\n✓\n✓\n✓\n✓\n✓\n\n\nOpenAI\n✓\n✓\n✓\nvaries*\n✗\n\n\nChatGPT\n✓\n✓\n✓\nvaries*\n✗\n\n\nOpenRouter\nvaries by model\n\n\n\n\n\n\nOllama\n✓\nvaries\n✗\n✗\n✗\n\n\n\n* Audio support depends on model. For OpenAI audio workflows you may need provider: openai/chat with an audio-capable model.\nSupport changes quickly. Consult each provider’s documentation for current limits on formats, sizes, and rate limits.\nIn Lectic, you attach external content by linking files in the user message body. Lectic packages these and sends them to the provider in a way that fits that provider’s API. See External Content for examples and tips.",
    "crumbs": [
      "Core Concepts",
      "Providers and Models"
    ]
  },
  {
    "objectID": "06_providers_and_models.html#picking-a-default-provider",
    "href": "06_providers_and_models.html#picking-a-default-provider",
    "title": "Providers and Models",
    "section": "",
    "text": "If you do not set provider, Lectic checks for keys in this order and uses the first one it finds:\nAnthropic → Gemini → OpenAI → OpenRouter.\nSet one of these environment variables before you run Lectic:\n\nANTHROPIC_API_KEY\nGEMINI_API_KEY\nOPENAI_API_KEY\nOPENROUTER_API_KEY\n\nAWS credentials for Bedrock are not used for auto-selection. If you want Anthropic via Bedrock, set provider: anthropic/bedrock explicitly and make sure your AWS environment is configured.",
    "crumbs": [
      "Core Concepts",
      "Providers and Models"
    ]
  },
  {
    "objectID": "06_providers_and_models.html#chatgpt-subscription-no-api-key",
    "href": "06_providers_and_models.html#chatgpt-subscription-no-api-key",
    "title": "Providers and Models",
    "section": "",
    "text": "Lectic also supports provider: chatgpt, which uses your ChatGPT subscription via the official Codex / ChatGPT OAuth flow.\nNotes:\n\nThis provider is not auto-selected (there is no API key env var to check). If you want it by default, set provider: chatgpt in your lectic.yaml or in the .lec frontmatter.\nOn first use, Lectic opens a browser window for login and stores tokens at $LECTIC_STATE/chatgpt_auth.json (for example, ~/.local/state/lectic/chatgpt_auth.json on Linux).\nLogin starts a local callback server on port 1455. If that port is in use, stop the other process and try again.\nIf you want to “log out”, delete that file.",
    "crumbs": [
      "Core Concepts",
      "Providers and Models"
    ]
  },
  {
    "objectID": "06_providers_and_models.html#discover-models",
    "href": "06_providers_and_models.html#discover-models",
    "title": "Providers and Models",
    "section": "",
    "text": "Not sure which models are available? Run:\nlectic models\nThis queries each provider you have credentials for and prints the available models.\n\n\n\n\n\n\nTip\n\n\n\nThe LSP can also autocomplete model names as you type in the YAML header. See Editor Integration.\n\n\n\n\n\n\n\n\nNote\n\n\n\nprovider: chatgpt models only show up in lectic models after you have logged in at least once (since the login is browser-based, not an API key). If you have not logged in yet, run a .lec file with provider: chatgpt first.",
    "crumbs": [
      "Core Concepts",
      "Providers and Models"
    ]
  },
  {
    "objectID": "06_providers_and_models.html#openai-two-provider-strings",
    "href": "06_providers_and_models.html#openai-two-provider-strings",
    "title": "Providers and Models",
    "section": "",
    "text": "OpenAI has two modes in Lectic today.\n\nopenai selects the Responses API. Choose this when you want native tools like search and code.\nopenai/chat selects the legacy Chat Completions API.",
    "crumbs": [
      "Core Concepts",
      "Providers and Models"
    ]
  },
  {
    "objectID": "06_providers_and_models.html#examples",
    "href": "06_providers_and_models.html#examples",
    "title": "Providers and Models",
    "section": "",
    "text": "These examples show the minimal configuration for each provider. You can omit provider and model if you want Lectic to pick defaults based on your environment.\nAnthropic (direct API):\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic\nAnthropic via Bedrock:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic/bedrock\n  model: anthropic.claude-3-haiku-20240307-v1:0\nOpenAI (Responses API):\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: openai\nOpenAI Chat Completions (legacy API):\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: openai/chat\nChatGPT (subscription via Codex backend):\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: chatgpt\n  model: gpt-5.1-codex\nGemini:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: gemini\nOpenRouter:\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: openrouter\n  model: meta-llama/llama-3.1-70b-instruct\nOllama (local inference):\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: ollama\n  model: llama3.1",
    "crumbs": [
      "Core Concepts",
      "Providers and Models"
    ]
  },
  {
    "objectID": "06_providers_and_models.html#capabilities-and-media",
    "href": "06_providers_and_models.html#capabilities-and-media",
    "title": "Providers and Models",
    "section": "",
    "text": "Providers differ in what they accept as input. Here’s a rough guide:\n\n\n\nProvider\nText\nImages\nPDFs\nAudio\nVideo\n\n\n\n\nAnthropic\n✓\n✓\n✓\n✗\n✗\n\n\nGemini\n✓\n✓\n✓\n✓\n✓\n\n\nOpenAI\n✓\n✓\n✓\nvaries*\n✗\n\n\nChatGPT\n✓\n✓\n✓\nvaries*\n✗\n\n\nOpenRouter\nvaries by model\n\n\n\n\n\n\nOllama\n✓\nvaries\n✗\n✗\n✗\n\n\n\n* Audio support depends on model. For OpenAI audio workflows you may need provider: openai/chat with an audio-capable model.\nSupport changes quickly. Consult each provider’s documentation for current limits on formats, sizes, and rate limits.\nIn Lectic, you attach external content by linking files in the user message body. Lectic packages these and sends them to the provider in a way that fits that provider’s API. See External Content for examples and tips.",
    "crumbs": [
      "Core Concepts",
      "Providers and Models"
    ]
  },
  {
    "objectID": "05_configuration.html",
    "href": "05_configuration.html",
    "title": "Lectic Configuration",
    "section": "",
    "text": "Lectic offers a flexible configuration system that lets you set global defaults, create per-project settings, and make conversation-specific overrides. This is managed through a hierarchy of YAML files.\n\n\nConfiguration settings are merged from multiple sources. Each source in the list below overrides the one before it, with the .lec file’s own header always having the final say.\n\nSystem Config Directory: Lectic first looks for a configuration file at lectic/lectic.yaml within your system’s standard config location (e.g., ~/.config/lectic/lectic.yaml on Linux). This is the ideal place for your global, user-level defaults.\nWorking Directory: Next, it looks for a lectic.yaml file in the current working directory. This is useful for project-level configuration that you might commit to a git repository.\nLectic File Header: The YAML frontmatter within your .lec file is the final and highest-precedence source of configuration.\n\n\n\n\nYou can change the default locations for Lectic’s data directories by setting the following environment variables:\n\n$LECTIC_CONFIG: Overrides the base configuration directory path.\n$LECTIC_DATA: Overrides the data directory path.\n$LECTIC_CACHE: Overrides the cache directory path.\n$LECTIC_STATE: Overrides the state directory path.\n\nThese variables, along with $LECTIC_TEMP (a temporary directory) and $LECTIC_FILE (the path to the active .lec file), are automatically passed into the environment of any subprocesses Lectic spawns, such as exec tools. This ensures your custom scripts have access to the same context as the main process.\n\n\n\nWhen combining settings from multiple configuration files, Lectic follows specific rules:\n\nObjects (Mappings): If the higher precedence source has a name attribute, and the name doesn’t match the name attribute of the lower precedence source, then the higher precedence source replaces the lower precedence source. Otherwise, the objects are combined and matching keys merged recursively.\nArrays (Lists): Merged based on the name attribute of their elements. If two objects in an array share the same name, they are merged. Otherwise, the elements are simply combined. This is especially useful for managing lists of tools and interlocutors. When duplicate named items appear within a single file, later entries override earlier ones. The LSP warns on duplicate names in the document header to help catch mistakes.\nOther Values: For simple values (strings, numbers) or if the types don’t match, the value from the highest-precedence source is used without any merging.\n\nAdditionally, the LSP validates header fields. It reports errors for missing or mistyped interlocutor properties and warns when you add unknown properties to an interlocutor mapping, which helps catch typos in keys like model or max_tokens.\n\n\nImagine you have a global config in ~/.config/lectic/lectic.yaml:\n# ~/.config/lectic/lectic.yaml\ninterlocutors:\n    - name: opus\n      provider: anthropic\n      model: claude-3-opus-20240229\nAnd a project-specific file, project.yaml:\n# ./project.yaml\ninterlocutor:\n    name: haiku\n    model: claude-3-haiku-20240307\n    tools:\n        - exec: bash\n        - agent: opus\nIf you place this project config at ./lectic.yaml in your working directory, Lectic will merge it with your system defaults and the document header using the precedence above. The haiku interlocutor will be configured with the claude-3-haiku model, and it will have access to a bash tool and an agent tool that can call opus. You can switch to opus within the conversation if needed, using an :ask[] directive.",
    "crumbs": [
      "Core Concepts",
      "Configuration"
    ]
  },
  {
    "objectID": "05_configuration.html#configuration-hierarchy",
    "href": "05_configuration.html#configuration-hierarchy",
    "title": "Lectic Configuration",
    "section": "",
    "text": "Configuration settings are merged from multiple sources. Each source in the list below overrides the one before it, with the .lec file’s own header always having the final say.\n\nSystem Config Directory: Lectic first looks for a configuration file at lectic/lectic.yaml within your system’s standard config location (e.g., ~/.config/lectic/lectic.yaml on Linux). This is the ideal place for your global, user-level defaults.\nWorking Directory: Next, it looks for a lectic.yaml file in the current working directory. This is useful for project-level configuration that you might commit to a git repository.\nLectic File Header: The YAML frontmatter within your .lec file is the final and highest-precedence source of configuration.",
    "crumbs": [
      "Core Concepts",
      "Configuration"
    ]
  },
  {
    "objectID": "05_configuration.html#overriding-default-directories",
    "href": "05_configuration.html#overriding-default-directories",
    "title": "Lectic Configuration",
    "section": "",
    "text": "You can change the default locations for Lectic’s data directories by setting the following environment variables:\n\n$LECTIC_CONFIG: Overrides the base configuration directory path.\n$LECTIC_DATA: Overrides the data directory path.\n$LECTIC_CACHE: Overrides the cache directory path.\n$LECTIC_STATE: Overrides the state directory path.\n\nThese variables, along with $LECTIC_TEMP (a temporary directory) and $LECTIC_FILE (the path to the active .lec file), are automatically passed into the environment of any subprocesses Lectic spawns, such as exec tools. This ensures your custom scripts have access to the same context as the main process.",
    "crumbs": [
      "Core Concepts",
      "Configuration"
    ]
  },
  {
    "objectID": "05_configuration.html#merging-logic",
    "href": "05_configuration.html#merging-logic",
    "title": "Lectic Configuration",
    "section": "",
    "text": "When combining settings from multiple configuration files, Lectic follows specific rules:\n\nObjects (Mappings): If the higher precedence source has a name attribute, and the name doesn’t match the name attribute of the lower precedence source, then the higher precedence source replaces the lower precedence source. Otherwise, the objects are combined and matching keys merged recursively.\nArrays (Lists): Merged based on the name attribute of their elements. If two objects in an array share the same name, they are merged. Otherwise, the elements are simply combined. This is especially useful for managing lists of tools and interlocutors. When duplicate named items appear within a single file, later entries override earlier ones. The LSP warns on duplicate names in the document header to help catch mistakes.\nOther Values: For simple values (strings, numbers) or if the types don’t match, the value from the highest-precedence source is used without any merging.\n\nAdditionally, the LSP validates header fields. It reports errors for missing or mistyped interlocutor properties and warns when you add unknown properties to an interlocutor mapping, which helps catch typos in keys like model or max_tokens.\n\n\nImagine you have a global config in ~/.config/lectic/lectic.yaml:\n# ~/.config/lectic/lectic.yaml\ninterlocutors:\n    - name: opus\n      provider: anthropic\n      model: claude-3-opus-20240229\nAnd a project-specific file, project.yaml:\n# ./project.yaml\ninterlocutor:\n    name: haiku\n    model: claude-3-haiku-20240307\n    tools:\n        - exec: bash\n        - agent: opus\nIf you place this project config at ./lectic.yaml in your working directory, Lectic will merge it with your system defaults and the document header using the precedence above. The haiku interlocutor will be configured with the claude-3-haiku model, and it will have access to a bash tool and an agent tool that can call opus. You can switch to opus within the conversation if needed, using an :ask[] directive.",
    "crumbs": [
      "Core Concepts",
      "Configuration"
    ]
  },
  {
    "objectID": "context_management/03_external_prompts.html",
    "href": "context_management/03_external_prompts.html",
    "title": "External Prompts and Instructions",
    "section": "",
    "text": "Many Lectic fields can load their text from outside the document. You can point a field at a file on disk, or run a command (or script) and use its output. This lets you keep prompts, usage text, and notes in one place, or compute them on demand.\nWhat supports external sources:\n\ninterlocutor.prompt\nmacros[].expansion\ntools[].usage (for tools that accept a usage string)\ntools[].details (for tools that provide extra details)\n\nEach of these accepts either a plain string, or a string beginning with one of the prefixes below.\n\n\nLoads the contents of PATH and uses that as the field value. Environment variables in the path are expanded before reading.\nExamples\ninterlocutor:\n  name: Assistant\n  prompt: file:./prompts/assistant.md\nmacros:\n  - name: summarize\n    expansion: file:$HOME/.config/lectic/prompts/summarize.txt\n\n\n\nRuns the command and uses its stdout as the field value. There are two forms:\n\nSingle line: executed directly, not through a shell. Shell features like globbing and command substitution do not work. If you need them, invoke a shell explicitly (for example, bash -lc '...').\nMulti‑line: treated as a script. The first line must be a shebang (for example, #!/usr/bin/env bash). The script is written to a temporary file and executed with the interpreter from the shebang.\n\nEnvironment variables in a single‑line command are expanded before running. For multi‑line scripts, variables are available via the process environment at runtime.\n\n\n\nSingle line\ninterlocutor:\n  name: Assistant\n  prompt: exec:echo \"You are a helpful assistant.\"\nMulti‑line script\ninterlocutor:\n  name: Assistant\n  prompt: |\n    exec:#!/usr/bin/env bash\n    cat &lt;&lt;'PREFACE'\n    You are a helpful assistant.\n    You will incorporate recent memory below.\n    PREFACE\n    echo\n    echo \"Recent memory:\"\n    sqlite3 \"$LECTIC_DATA/memory.sqlite3\" \\\n      \"SELECT printf('- %s (%s)', text, ts) FROM memory \\\n       ORDER BY ts DESC LIMIT 5;\"\n\n\n\n\nfile: and exec: resolve relative paths and run commands in the current working directory of the lectic process (the directory from which you invoked the command). If you used -f or -i, note that the working directory does not automatically switch to the .lec file’s directory for these expansions. Use absolute paths or cd if you need a different base.\nStandard Lectic environment variables are provided, including LECTIC_CONFIG, LECTIC_DATA, LECTIC_CACHE, LECTIC_STATE, LECTIC_TEMP, and LECTIC_FILE (when using -f or -i). Your shell environment is also passed through.\nMacro expansions can inject additional variables into exec: via directive attributes. See the Macros guide for details.\n\n\n\n\n\nThe value is recomputed on each run. This makes it easy to incorporate recent state (for example, “memory” from a local database) into a prompt.\nIf a file cannot be read or a command fails, Lectic reports an error and aborts the run. Fix the source and try again.\n\nSee also\n\nExternal Content for attaching files to user messages.\nMacros for passing variables into exec: expansions within macros.",
    "crumbs": [
      "Context Management",
      "External Prompts"
    ]
  },
  {
    "objectID": "context_management/03_external_prompts.html#filepath",
    "href": "context_management/03_external_prompts.html#filepath",
    "title": "External Prompts and Instructions",
    "section": "",
    "text": "Loads the contents of PATH and uses that as the field value. Environment variables in the path are expanded before reading.\nExamples\ninterlocutor:\n  name: Assistant\n  prompt: file:./prompts/assistant.md\nmacros:\n  - name: summarize\n    expansion: file:$HOME/.config/lectic/prompts/summarize.txt",
    "crumbs": [
      "Context Management",
      "External Prompts"
    ]
  },
  {
    "objectID": "context_management/03_external_prompts.html#execcommand-or-execscript",
    "href": "context_management/03_external_prompts.html#execcommand-or-execscript",
    "title": "External Prompts and Instructions",
    "section": "",
    "text": "Runs the command and uses its stdout as the field value. There are two forms:\n\nSingle line: executed directly, not through a shell. Shell features like globbing and command substitution do not work. If you need them, invoke a shell explicitly (for example, bash -lc '...').\nMulti‑line: treated as a script. The first line must be a shebang (for example, #!/usr/bin/env bash). The script is written to a temporary file and executed with the interpreter from the shebang.\n\nEnvironment variables in a single‑line command are expanded before running. For multi‑line scripts, variables are available via the process environment at runtime.",
    "crumbs": [
      "Context Management",
      "External Prompts"
    ]
  },
  {
    "objectID": "context_management/03_external_prompts.html#examples",
    "href": "context_management/03_external_prompts.html#examples",
    "title": "External Prompts and Instructions",
    "section": "",
    "text": "Single line\ninterlocutor:\n  name: Assistant\n  prompt: exec:echo \"You are a helpful assistant.\"\nMulti‑line script\ninterlocutor:\n  name: Assistant\n  prompt: |\n    exec:#!/usr/bin/env bash\n    cat &lt;&lt;'PREFACE'\n    You are a helpful assistant.\n    You will incorporate recent memory below.\n    PREFACE\n    echo\n    echo \"Recent memory:\"\n    sqlite3 \"$LECTIC_DATA/memory.sqlite3\" \\\n      \"SELECT printf('- %s (%s)', text, ts) FROM memory \\\n       ORDER BY ts DESC LIMIT 5;\"",
    "crumbs": [
      "Context Management",
      "External Prompts"
    ]
  },
  {
    "objectID": "context_management/03_external_prompts.html#working-directory-and-environment",
    "href": "context_management/03_external_prompts.html#working-directory-and-environment",
    "title": "External Prompts and Instructions",
    "section": "",
    "text": "file: and exec: resolve relative paths and run commands in the current working directory of the lectic process (the directory from which you invoked the command). If you used -f or -i, note that the working directory does not automatically switch to the .lec file’s directory for these expansions. Use absolute paths or cd if you need a different base.\nStandard Lectic environment variables are provided, including LECTIC_CONFIG, LECTIC_DATA, LECTIC_CACHE, LECTIC_STATE, LECTIC_TEMP, and LECTIC_FILE (when using -f or -i). Your shell environment is also passed through.\nMacro expansions can inject additional variables into exec: via directive attributes. See the Macros guide for details.",
    "crumbs": [
      "Context Management",
      "External Prompts"
    ]
  },
  {
    "objectID": "context_management/03_external_prompts.html#behavior-and-errors",
    "href": "context_management/03_external_prompts.html#behavior-and-errors",
    "title": "External Prompts and Instructions",
    "section": "",
    "text": "The value is recomputed on each run. This makes it easy to incorporate recent state (for example, “memory” from a local database) into a prompt.\nIf a file cannot be read or a command fails, Lectic reports an error and aborts the run. Fix the source and try again.\n\nSee also\n\nExternal Content for attaching files to user messages.\nMacros for passing variables into exec: expansions within macros.",
    "crumbs": [
      "Context Management",
      "External Prompts"
    ]
  },
  {
    "objectID": "context_management/02_conversation_control.html",
    "href": "context_management/02_conversation_control.html",
    "title": "Managing Context: Conversation Control",
    "section": "",
    "text": "Beyond simply adding external data, Lectic provides directives that allow you to actively manage the flow of a conversation. You can switch between different LLM interlocutors and control the context window that is sent to the model.\n\n\nLectic allows you to define multiple interlocutors in the YAML frontmatter. This enables you to bring different “personalities” or models with different capabilities into a single conversation.\nTo do this, use the interlocutors key (instead of interlocutor) and provide a list of configurations.\n---\ninterlocutors:\n  - name: Boggle\n    provider: anthropic\n    model: claude-3-sonnet-20240229\n    prompt: You are an expert on personal finance.\n  - name: Oggle\n    provider: gemini\n    prompt: You are very skeptical of conventional financial advice.\n---\nOnce multiple interlocutors are defined, Lectic will continue with whichever one was last active. To direct your message to a specific interlocutor, you use the :ask and :aside directives.\n\n\nThe :ask[InterlocutorName] directive changes the active speaker. All subsequent conversation turns will be handled by this new interlocutor until another :ask directive is used.\n:ask[Boggle] What is the best way to save for retirement?\n\n:::Boggle\nThe most common advice is to invest in a diversified portfolio of low-cost\nindex funds.\n:::\n\n:ask[Oggle] What's your take on that?\n\n:::Oggle\n\"Common advice\" is often just dogma. That strategy leaves you completely\nexposed to market downturns.\n:::\n\n\n\nThe :aside[InterlocutorName] directive is for a one-off question or comment. It directs the current message to the specified interlocutor, but the conversation then reverts back to the previously active speaker for the next user message.\nThis is useful for quick interjections or for getting a second opinion without derailing the main flow of the conversation.\n\n\n\n\nAs a conversation grows longer, the context sent to the LLM on each turn also grows. This can increase costs and, in some cases, lead to the model getting “stuck” on earlier parts of the dialogue.\nThe :reset[] directive gives you manual control over the context window. When Lectic processes a message containing :reset[], it instructs the LLM to ignore all preceding conversation history. The context for that turn effectively begins with the message containing the directive.\n...a very long conversation happens here...\n\nOkay, let's start fresh. I'm going to :reset[] your context now. Please\nsummarize our previous discussion and then we can move on to the next topic.\n\n:::Assistant\nUnderstood. I have cleared my context.\n\nPreviously, we discussed the history of the Roman Empire, focusing on the\nreign of Augustus and the establishment of the Pax Romana. We concluded that\neconomic stability was a key factor in the era's success.\n\nI am ready for the next topic.\n:::\n\nExcellent. Now, how did this conversation begin?\n\n:::Assistant\nThis conversation began with you instructing me to reset my context and provide\na summary of our previous discussion about the Roman Empire.\n:::\nThis is a powerful tool for managing long-running conversations, allowing you to “compact” the context manually or with the help of the LLM.\n\n\n\nSometimes you need to change the configuration of the conversation on the fly. Lectic provides two directives for this purpose, allowing you to merge new YAML configuration into the header.\n\n\nThe :merge_yaml directive allows you to permanently update the conversation’s configuration. The provided YAML is merged with the existing header, overriding any existing keys.\n:merge_yaml[{ interlocutor: { model: \"claude-3-opus-20240229\" } }]\nThis change persists for all subsequent turns in the conversation.\n\n\n\nThe :temp_merge_yaml directive updates the configuration for the current turn only. This is useful for one-off changes, such as temporarily increasing the max_tokens limit or enabling a specific tool.\n:temp_merge_yaml[{ interlocutor: { max_tokens: 4000 } }]\nOnce the turn is complete, the configuration reverts to its previous state for subsequent messages.",
    "crumbs": [
      "Context Management",
      "Conversation Control"
    ]
  },
  {
    "objectID": "context_management/02_conversation_control.html#multiparty-conversations-with-ask-and-aside",
    "href": "context_management/02_conversation_control.html#multiparty-conversations-with-ask-and-aside",
    "title": "Managing Context: Conversation Control",
    "section": "",
    "text": "Lectic allows you to define multiple interlocutors in the YAML frontmatter. This enables you to bring different “personalities” or models with different capabilities into a single conversation.\nTo do this, use the interlocutors key (instead of interlocutor) and provide a list of configurations.\n---\ninterlocutors:\n  - name: Boggle\n    provider: anthropic\n    model: claude-3-sonnet-20240229\n    prompt: You are an expert on personal finance.\n  - name: Oggle\n    provider: gemini\n    prompt: You are very skeptical of conventional financial advice.\n---\nOnce multiple interlocutors are defined, Lectic will continue with whichever one was last active. To direct your message to a specific interlocutor, you use the :ask and :aside directives.\n\n\nThe :ask[InterlocutorName] directive changes the active speaker. All subsequent conversation turns will be handled by this new interlocutor until another :ask directive is used.\n:ask[Boggle] What is the best way to save for retirement?\n\n:::Boggle\nThe most common advice is to invest in a diversified portfolio of low-cost\nindex funds.\n:::\n\n:ask[Oggle] What's your take on that?\n\n:::Oggle\n\"Common advice\" is often just dogma. That strategy leaves you completely\nexposed to market downturns.\n:::\n\n\n\nThe :aside[InterlocutorName] directive is for a one-off question or comment. It directs the current message to the specified interlocutor, but the conversation then reverts back to the previously active speaker for the next user message.\nThis is useful for quick interjections or for getting a second opinion without derailing the main flow of the conversation.",
    "crumbs": [
      "Context Management",
      "Conversation Control"
    ]
  },
  {
    "objectID": "context_management/02_conversation_control.html#context-management-with-reset",
    "href": "context_management/02_conversation_control.html#context-management-with-reset",
    "title": "Managing Context: Conversation Control",
    "section": "",
    "text": "As a conversation grows longer, the context sent to the LLM on each turn also grows. This can increase costs and, in some cases, lead to the model getting “stuck” on earlier parts of the dialogue.\nThe :reset[] directive gives you manual control over the context window. When Lectic processes a message containing :reset[], it instructs the LLM to ignore all preceding conversation history. The context for that turn effectively begins with the message containing the directive.\n...a very long conversation happens here...\n\nOkay, let's start fresh. I'm going to :reset[] your context now. Please\nsummarize our previous discussion and then we can move on to the next topic.\n\n:::Assistant\nUnderstood. I have cleared my context.\n\nPreviously, we discussed the history of the Roman Empire, focusing on the\nreign of Augustus and the establishment of the Pax Romana. We concluded that\neconomic stability was a key factor in the era's success.\n\nI am ready for the next topic.\n:::\n\nExcellent. Now, how did this conversation begin?\n\n:::Assistant\nThis conversation began with you instructing me to reset my context and provide\na summary of our previous discussion about the Roman Empire.\n:::\nThis is a powerful tool for managing long-running conversations, allowing you to “compact” the context manually or with the help of the LLM.",
    "crumbs": [
      "Context Management",
      "Conversation Control"
    ]
  },
  {
    "objectID": "context_management/02_conversation_control.html#dynamic-configuration-with-merge_yaml-and-temp_merge_yaml",
    "href": "context_management/02_conversation_control.html#dynamic-configuration-with-merge_yaml-and-temp_merge_yaml",
    "title": "Managing Context: Conversation Control",
    "section": "",
    "text": "Sometimes you need to change the configuration of the conversation on the fly. Lectic provides two directives for this purpose, allowing you to merge new YAML configuration into the header.\n\n\nThe :merge_yaml directive allows you to permanently update the conversation’s configuration. The provided YAML is merged with the existing header, overriding any existing keys.\n:merge_yaml[{ interlocutor: { model: \"claude-3-opus-20240229\" } }]\nThis change persists for all subsequent turns in the conversation.\n\n\n\nThe :temp_merge_yaml directive updates the configuration for the current turn only. This is useful for one-off changes, such as temporarily increasing the max_tokens limit or enabling a specific tool.\n:temp_merge_yaml[{ interlocutor: { max_tokens: 4000 } }]\nOnce the turn is complete, the configuration reverts to its previous state for subsequent messages.",
    "crumbs": [
      "Context Management",
      "Conversation Control"
    ]
  },
  {
    "objectID": "context_management/01_external_content.html",
    "href": "context_management/01_external_content.html",
    "title": "Managing Context: External Content",
    "section": "",
    "text": "Lectic aims to make it easy to pull external information into the conversation, providing the LLM with the context it needs to answer questions, analyze data, or perform tasks.\nThis is done in two primary ways: by referencing files and URIs using standard Markdown links, and by executing shell commands with the :cmd directive.\n\n\nYou can include local or remote content using the standard Markdown link syntax, [Title](URI). Lectic will fetch the content from the URI and include it in the context for the LLM.\nPlease summarize this local document: [Notes](./notes.md)\n\nAnalyze the data in this S3 bucket: [Dataset](s3://my_bucket/dataset.csv)\n\nWhat does this README say?\n[Repo](github+repo://gleachkr/Lectic/contents/README.md)\n\n\n\nText: Plain text files are included directly.\nImages: PNG, JPEG, GIF, and WebP images are supported.\nPDFs: Content from PDF files can be extracted (requires a provider that supports PDF ingestion, such as Anthropic, Gemini, or OpenAI).\nAudio: Gemini and OpenAI support audio inputs. For OpenAI, use provider: openai/chat with an audio‑capable model; supported formats include MP3, MPEG, and WAV. Gemini supports a broader set of audio types.\nVideo: Gemini supports video understanding. See supported formats in Google’s docs: https://ai.google.dev/gemini-api/docs/video-understanding#supported-formats\n\n\n\n\nLectic supports several URI schemes for referencing content:\n\nLocal Files: Simple relative paths like ./src/main.rs or absolute file:///path/to/file.txt URIs.\nRemote Content: http:// and https:// for web pages and other online resources.\nAmazon S3: s3:// for referencing objects in S3 buckets. This requires AWS credentials to be configured in your environment.\nMCP Resources: You can reference resources provided by an MCP server using a custom scheme, like github+repo://..., where github is the name of the MCP server (provided in the tool specification), and the rest is the resource URL.\n\nA few convenience rules apply:\n\nFor local file references using file://, use absolute paths. A portable way to build these is with $PWD (e.g., file://$PWD/papers/some.pdf).\nEnvironment variables in URIs use the $VAR form; ${VAR} is not supported. Expansion happens before any globbing.\nEnvironment variable expansion also applies to bare local paths (non‑URL links), such as ./$DATA_ROOT/file.txt. Expansion happens before any globbing.\n\nYou can use glob patterns to include multiple files at once. This is useful for providing the entire source code of a project as context.\n[All source code](./src/**/*.ts)\n[All images in this directory](./images/*.jpg)\nLectic uses Bun’s Glob API for matching.\n\n\n\nUsing full file:// URIs for local content enables additional capabilities.\n\n\nLectic supports environment variable expansion in URIs. This helps in creating portable .lec files that don’t rely on hardcoded absolute paths.\n[My dataset](file://$DATA_ROOT/my_project/data.csv)\n[Log file](file://$PWD/logs/latest.log)\n\n\n\nWhen referencing a PDF, you can point to a specific page or a range of pages by adding a fragment to the URI. Page numbering starts at 1.\n\nSingle Page: [See page 5](file.pdf#page=5)\nPage Range: [See Chapter 2](book.pdf#pages=20-35)\n\nIf both page and pages are supplied, pages takes precedence. If a page or range is malformed or out of bounds, Lectic will surface an error that is visible to the LLM.\n\n\n\n\n\nUse :cmd[...] to execute a shell command and insert its output directly into your message. Think of it as a built-in macro that runs a command and pastes in the result.\nWhat can you tell me about my system? :cmd[uname -a]\nWhen Lectic processes this, it runs uname -a and replaces the :cmd[...] directive with the command’s output wrapped in XML:\n&lt;stdout from=\"uname -a\"&gt;Linux myhost 6.1.0 ...&lt;/stdout&gt;\nIf the command fails (non-zero exit code), you get an error wrapper instead:\n&lt;error&gt;Something went wrong when executing a command:&lt;stdout from=\"bad-cmd\"&gt;\n&lt;/stdout&gt;&lt;stderr from=\"bad-cmd\"&gt;bad-cmd: command not found&lt;/stderr&gt;&lt;/error&gt;\n\n\n:cmd directives are expanded at the beginning of each user turn. You can also expand them with an LSP code action. If you want a :cmd directive to expand only once, you can wrap it in :attach[..] (see below) which will store the results in the lectic document as an attachment, or you can implement some other caching mechanism using the macro system.\n\n\n\n\n:cmd runs with Bun’s $ shell in the current working directory.\nStandard Lectic environment variables like LECTIC_FILE are available.\nLine breaks inside :cmd[...] are ignored, so wrapped commands work:\n\n:cmd[find . -name \"*.ts\" \n     | head -20]\n\n\n\n\nSystem information: What can you tell me about my system? :cmd[uname -a]\nProject state: Write a commit message: :cmd[git diff --staged]\nData snippets: Analyze this: :cmd[head -50 data.csv]\n\n\n\n\n\nWhile :cmd inserts command output directly into your message text, sometimes you want to provide context that appears as a separate attachment — for example, to keep the conversation transcript cleaner or to control caching behavior.\nThe :attach[...] directive creates an inline attachment. The content inside the brackets is stored in the assistant’s response block (after macro expansion) and sent to the LLM as additional user context.\nHere's the current state of the config:\n\n:attach[:config_status_macro[]]\n\nWhat do you think of this configuration?\nWhen Lectic processes this, it creates an inline attachment that appears at the top of the assistant’s response:\n&lt;inline-attachment kind=\"attach\"&gt;\n&lt;command&gt;&lt;/command&gt;\n&lt;content type=\"text/plain\"&gt;\n┆server:\n┆  port: 8080\n┆  host: localhost\n&lt;/content&gt;\n&lt;/inline-attachment&gt;\n\n\nYou can compose :attach and :cmd to get the best of both worlds — run a command and store its output as an attachment:\nReview this diff: :attach[:cmd[git diff --staged]]\nThis executes git diff --staged, then wraps the result as an inline attachment. The attachment is cached in the transcript, so re-running Lectic won’t re-execute the command.\n\n\n\n\n\n\nTipWhen to use :cmd vs :attach[:cmd[...]]\n\n\n\nUse :cmd[...] when you want command output inlined directly in your message. The output becomes part of the message text.\nUse :attach[:cmd[...]] when you want the output stored as a cached attachment. This is useful for large outputs or when you want to preserve provider cache efficiency (for example, if the output of :cmd might change in between user turns).\n\n\n\n\n\nInline attachments serve two purposes:\n\nCaching: Results are stored in the file, so re-running Lectic doesn’t re-execute commands or re-process content. Only :attach directives in the most recent user message are processed.\nContext positioning: When sending the conversation to the provider, attachments are treated as if they were a user message immediately preceding the assistant’s response. This keeps provider caches stable.\n\nInline attachments appear in the .lec file as XML blocks inside the assistant response. Editor plugins typically fold them by default to reduce visual clutter.\n\n\n\n\n\n\nTip\n\n\n\nInline attachments are managed by Lectic. Don’t edit them by hand — if you need to re-run a command, delete the attachment and add a new :attach directive in your latest message.",
    "crumbs": [
      "Context Management",
      "External Content"
    ]
  },
  {
    "objectID": "context_management/01_external_content.html#content-references-via-markdown-links",
    "href": "context_management/01_external_content.html#content-references-via-markdown-links",
    "title": "Managing Context: External Content",
    "section": "",
    "text": "You can include local or remote content using the standard Markdown link syntax, [Title](URI). Lectic will fetch the content from the URI and include it in the context for the LLM.\nPlease summarize this local document: [Notes](./notes.md)\n\nAnalyze the data in this S3 bucket: [Dataset](s3://my_bucket/dataset.csv)\n\nWhat does this README say?\n[Repo](github+repo://gleachkr/Lectic/contents/README.md)\n\n\n\nText: Plain text files are included directly.\nImages: PNG, JPEG, GIF, and WebP images are supported.\nPDFs: Content from PDF files can be extracted (requires a provider that supports PDF ingestion, such as Anthropic, Gemini, or OpenAI).\nAudio: Gemini and OpenAI support audio inputs. For OpenAI, use provider: openai/chat with an audio‑capable model; supported formats include MP3, MPEG, and WAV. Gemini supports a broader set of audio types.\nVideo: Gemini supports video understanding. See supported formats in Google’s docs: https://ai.google.dev/gemini-api/docs/video-understanding#supported-formats\n\n\n\n\nLectic supports several URI schemes for referencing content:\n\nLocal Files: Simple relative paths like ./src/main.rs or absolute file:///path/to/file.txt URIs.\nRemote Content: http:// and https:// for web pages and other online resources.\nAmazon S3: s3:// for referencing objects in S3 buckets. This requires AWS credentials to be configured in your environment.\nMCP Resources: You can reference resources provided by an MCP server using a custom scheme, like github+repo://..., where github is the name of the MCP server (provided in the tool specification), and the rest is the resource URL.\n\nA few convenience rules apply:\n\nFor local file references using file://, use absolute paths. A portable way to build these is with $PWD (e.g., file://$PWD/papers/some.pdf).\nEnvironment variables in URIs use the $VAR form; ${VAR} is not supported. Expansion happens before any globbing.\nEnvironment variable expansion also applies to bare local paths (non‑URL links), such as ./$DATA_ROOT/file.txt. Expansion happens before any globbing.\n\nYou can use glob patterns to include multiple files at once. This is useful for providing the entire source code of a project as context.\n[All source code](./src/**/*.ts)\n[All images in this directory](./images/*.jpg)\nLectic uses Bun’s Glob API for matching.\n\n\n\nUsing full file:// URIs for local content enables additional capabilities.\n\n\nLectic supports environment variable expansion in URIs. This helps in creating portable .lec files that don’t rely on hardcoded absolute paths.\n[My dataset](file://$DATA_ROOT/my_project/data.csv)\n[Log file](file://$PWD/logs/latest.log)\n\n\n\nWhen referencing a PDF, you can point to a specific page or a range of pages by adding a fragment to the URI. Page numbering starts at 1.\n\nSingle Page: [See page 5](file.pdf#page=5)\nPage Range: [See Chapter 2](book.pdf#pages=20-35)\n\nIf both page and pages are supplied, pages takes precedence. If a page or range is malformed or out of bounds, Lectic will surface an error that is visible to the LLM.",
    "crumbs": [
      "Context Management",
      "External Content"
    ]
  },
  {
    "objectID": "context_management/01_external_content.html#command-output-via-cmd",
    "href": "context_management/01_external_content.html#command-output-via-cmd",
    "title": "Managing Context: External Content",
    "section": "",
    "text": "Use :cmd[...] to execute a shell command and insert its output directly into your message. Think of it as a built-in macro that runs a command and pastes in the result.\nWhat can you tell me about my system? :cmd[uname -a]\nWhen Lectic processes this, it runs uname -a and replaces the :cmd[...] directive with the command’s output wrapped in XML:\n&lt;stdout from=\"uname -a\"&gt;Linux myhost 6.1.0 ...&lt;/stdout&gt;\nIf the command fails (non-zero exit code), you get an error wrapper instead:\n&lt;error&gt;Something went wrong when executing a command:&lt;stdout from=\"bad-cmd\"&gt;\n&lt;/stdout&gt;&lt;stderr from=\"bad-cmd\"&gt;bad-cmd: command not found&lt;/stderr&gt;&lt;/error&gt;\n\n\n:cmd directives are expanded at the beginning of each user turn. You can also expand them with an LSP code action. If you want a :cmd directive to expand only once, you can wrap it in :attach[..] (see below) which will store the results in the lectic document as an attachment, or you can implement some other caching mechanism using the macro system.\n\n\n\n\n:cmd runs with Bun’s $ shell in the current working directory.\nStandard Lectic environment variables like LECTIC_FILE are available.\nLine breaks inside :cmd[...] are ignored, so wrapped commands work:\n\n:cmd[find . -name \"*.ts\" \n     | head -20]\n\n\n\n\nSystem information: What can you tell me about my system? :cmd[uname -a]\nProject state: Write a commit message: :cmd[git diff --staged]\nData snippets: Analyze this: :cmd[head -50 data.csv]",
    "crumbs": [
      "Context Management",
      "External Content"
    ]
  },
  {
    "objectID": "context_management/01_external_content.html#inline-attachments-with-attach",
    "href": "context_management/01_external_content.html#inline-attachments-with-attach",
    "title": "Managing Context: External Content",
    "section": "",
    "text": "While :cmd inserts command output directly into your message text, sometimes you want to provide context that appears as a separate attachment — for example, to keep the conversation transcript cleaner or to control caching behavior.\nThe :attach[...] directive creates an inline attachment. The content inside the brackets is stored in the assistant’s response block (after macro expansion) and sent to the LLM as additional user context.\nHere's the current state of the config:\n\n:attach[:config_status_macro[]]\n\nWhat do you think of this configuration?\nWhen Lectic processes this, it creates an inline attachment that appears at the top of the assistant’s response:\n&lt;inline-attachment kind=\"attach\"&gt;\n&lt;command&gt;&lt;/command&gt;\n&lt;content type=\"text/plain\"&gt;\n┆server:\n┆  port: 8080\n┆  host: localhost\n&lt;/content&gt;\n&lt;/inline-attachment&gt;\n\n\nYou can compose :attach and :cmd to get the best of both worlds — run a command and store its output as an attachment:\nReview this diff: :attach[:cmd[git diff --staged]]\nThis executes git diff --staged, then wraps the result as an inline attachment. The attachment is cached in the transcript, so re-running Lectic won’t re-execute the command.\n\n\n\n\n\n\nTipWhen to use :cmd vs :attach[:cmd[...]]\n\n\n\nUse :cmd[...] when you want command output inlined directly in your message. The output becomes part of the message text.\nUse :attach[:cmd[...]] when you want the output stored as a cached attachment. This is useful for large outputs or when you want to preserve provider cache efficiency (for example, if the output of :cmd might change in between user turns).\n\n\n\n\n\nInline attachments serve two purposes:\n\nCaching: Results are stored in the file, so re-running Lectic doesn’t re-execute commands or re-process content. Only :attach directives in the most recent user message are processed.\nContext positioning: When sending the conversation to the provider, attachments are treated as if they were a user message immediately preceding the assistant’s response. This keeps provider caches stable.\n\nInline attachments appear in the .lec file as XML blocks inside the assistant response. Editor plugins typically fold them by default to reduce visual clutter.\n\n\n\n\n\n\nTip\n\n\n\nInline attachments are managed by Lectic. Don’t edit them by hand — if you need to re-run a command, delete the attachment and add a new :attach directive in your latest message.",
    "crumbs": [
      "Context Management",
      "External Content"
    ]
  },
  {
    "objectID": "01_introduction.html",
    "href": "01_introduction.html",
    "title": "Introduction to Lectic",
    "section": "",
    "text": "Lectic is a unixy LLM toolbox. It treats conversations as plain text files, which means you can version control them, grep them, pipe them, email them, and edit them in whatever editor you like.\n\n\nMost LLM tools ask you to work in their environment—a chat window, a dedicated IDE, a web app. Lectic takes the opposite approach: it brings LLMs into your environment. The conversation is a file. You edit it with your editor. You run a command. The response appears.\nThis matters when you want to:\n\nKeep your workflow. You already have an editor you like, a terminal you’ve customized, scripts you’ve written. Lectic fits into that setup instead of replacing it.\nControl your data. Conversations are human-readable files on your disk. Back them up however you want. Delete them when you’re done. Email them if you want to. No cloud sync you didn’t ask for.\nAutomate LLM interactions. Because Lectic is a command-line tool, you can script it, pipe to it, run it from cron jobs or git hooks.\nExperiment freely. Branch a conversation with git. Try different prompts. Diff the results.\n\nThe “conversation as file” approach is underexplored. Many “agentic” coding tools are reinventing editor affordances in awkward ways—custom UIs for editing, bespoke diff viewers, their own version of undo, or evan a vim mode. Lectic sidesteps all of that. Your editor already does those things.\n\n\n\n\n\nEvery conversation is a markdown file (.lec). Because your conversations are files, you can do anything with them that you can do with files:\n\nVersion control: Track changes with git, branch experiments, diff conversations.\nSearch: grep across your conversation history.\nProcess: Pipe conversations through other tools. Combine Lectic with sed, pandoc, or anything else.\nBack up: Copy files. Sync with rsync. Store wherever you want.\n\n\n\n\nLectic includes an LSP server that provides completions, diagnostics, hover information, go-to-definition, and folding for .lec files. You can use Lectic with Neovim, VS Code, or any editor that speaks LSP.\nFor editors without LSP support, the basic workflow still works: edit a file, run lectic -i file.lec, and the response is appended.\n\n\n\nTools, hooks, and macros are executables. Write them in whatever language you prefer. If it can read environment variables and write to stdout, it works with Lectic.\nThis means you’re not locked into a plugin ecosystem or a specific scripting language. Your existing scripts and tools integrate directly.\n\n\n\nLectic provides a small set of building blocks:\n\n:cmd: Run a command and include its output.\n:attach: Create an inline attachment from expanded content.\n:env: Read an environment variable.\n:verbatim: Include text without macro expansion.\n:once: Expand content only in the final message.\n:discard: Evaluate content for side effects, discard output.\n:ask / :aside: Switch between interlocutors.\n:reset: Clear conversation context.\nMacros: Reusable text expansions.\nHooks: Run code on events (message sent, tool called, etc.).\nTools: Give the LLM capabilities (shell, database, MCP servers).\n\n\n\n\n\n\n\nNoteWhat’s an “interlocutor”?\n\n\n\nLectic calls LLM configurations “interlocutors” because they’re participants in a conversation. An interlocutor has a name, a prompt, and optionally tools and other settings. You can have multiple interlocutors in one conversation—each with different capabilities or personalities.\n\n\nYou combine these to build what you need: coding assistants, research workflows, multi-agent systems, or simple one-shot text processing. See the Cookbook for detailed recipes.\n\n\n\n\nA minimal conversation file:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n---\n\nWhat's 2 + 2?\n\n:::Assistant\n\n4\n\n:::\nThe --- block at the top is YAML configuration. Below it, you write your message. The :::Assistant block is where the LLM’s response appears—Lectic appends it when you run the command.\nA more interesting setup with tools:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a code reviewer.\n  tools:\n    - exec: cat\n      name: read_file\n    - exec: rg --json\n      name: search\n---\nReview the error handling in src/main.ts. Are there any uncaught\nexceptions?\n\n:::Assistant\n\n&lt;tool-call with=\"read_file\"&gt;\n&lt;arguments&gt;&lt;argv&gt;[\"src/main.ts\"]&lt;/argv&gt;&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆&lt;stdout&gt;// contents of main.ts...&lt;/stdout&gt;\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\nLooking at the code, I can see several places where exceptions might not\nbe caught...\n\n:::\nThe assistant reads files and searches the codebase to answer your question. Tool calls appear inline in the response so you can see exactly what happened.\n\n\n\n\nGetting Started: Install Lectic and run your first conversation.\nEditor Integration: Set up your editor for the best experience.\nConfiguration: Learn about the configuration system.\nTools: Give your LLM capabilities.\nCookbook: Ready-to-use recipes for common workflows.\n\nAll documentation concatenated into a single markdown file can be found here.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01_introduction.html#what-problems-does-lectic-solve",
    "href": "01_introduction.html#what-problems-does-lectic-solve",
    "title": "Introduction to Lectic",
    "section": "",
    "text": "Most LLM tools ask you to work in their environment—a chat window, a dedicated IDE, a web app. Lectic takes the opposite approach: it brings LLMs into your environment. The conversation is a file. You edit it with your editor. You run a command. The response appears.\nThis matters when you want to:\n\nKeep your workflow. You already have an editor you like, a terminal you’ve customized, scripts you’ve written. Lectic fits into that setup instead of replacing it.\nControl your data. Conversations are human-readable files on your disk. Back them up however you want. Delete them when you’re done. Email them if you want to. No cloud sync you didn’t ask for.\nAutomate LLM interactions. Because Lectic is a command-line tool, you can script it, pipe to it, run it from cron jobs or git hooks.\nExperiment freely. Branch a conversation with git. Try different prompts. Diff the results.\n\nThe “conversation as file” approach is underexplored. Many “agentic” coding tools are reinventing editor affordances in awkward ways—custom UIs for editing, bespoke diff viewers, their own version of undo, or evan a vim mode. Lectic sidesteps all of that. Your editor already does those things.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01_introduction.html#core-principles",
    "href": "01_introduction.html#core-principles",
    "title": "Introduction to Lectic",
    "section": "",
    "text": "Every conversation is a markdown file (.lec). Because your conversations are files, you can do anything with them that you can do with files:\n\nVersion control: Track changes with git, branch experiments, diff conversations.\nSearch: grep across your conversation history.\nProcess: Pipe conversations through other tools. Combine Lectic with sed, pandoc, or anything else.\nBack up: Copy files. Sync with rsync. Store wherever you want.\n\n\n\n\nLectic includes an LSP server that provides completions, diagnostics, hover information, go-to-definition, and folding for .lec files. You can use Lectic with Neovim, VS Code, or any editor that speaks LSP.\nFor editors without LSP support, the basic workflow still works: edit a file, run lectic -i file.lec, and the response is appended.\n\n\n\nTools, hooks, and macros are executables. Write them in whatever language you prefer. If it can read environment variables and write to stdout, it works with Lectic.\nThis means you’re not locked into a plugin ecosystem or a specific scripting language. Your existing scripts and tools integrate directly.\n\n\n\nLectic provides a small set of building blocks:\n\n:cmd: Run a command and include its output.\n:attach: Create an inline attachment from expanded content.\n:env: Read an environment variable.\n:verbatim: Include text without macro expansion.\n:once: Expand content only in the final message.\n:discard: Evaluate content for side effects, discard output.\n:ask / :aside: Switch between interlocutors.\n:reset: Clear conversation context.\nMacros: Reusable text expansions.\nHooks: Run code on events (message sent, tool called, etc.).\nTools: Give the LLM capabilities (shell, database, MCP servers).\n\n\n\n\n\n\n\nNoteWhat’s an “interlocutor”?\n\n\n\nLectic calls LLM configurations “interlocutors” because they’re participants in a conversation. An interlocutor has a name, a prompt, and optionally tools and other settings. You can have multiple interlocutors in one conversation—each with different capabilities or personalities.\n\n\nYou combine these to build what you need: coding assistants, research workflows, multi-agent systems, or simple one-shot text processing. See the Cookbook for detailed recipes.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01_introduction.html#quick-example",
    "href": "01_introduction.html#quick-example",
    "title": "Introduction to Lectic",
    "section": "",
    "text": "A minimal conversation file:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n---\n\nWhat's 2 + 2?\n\n:::Assistant\n\n4\n\n:::\nThe --- block at the top is YAML configuration. Below it, you write your message. The :::Assistant block is where the LLM’s response appears—Lectic appends it when you run the command.\nA more interesting setup with tools:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a code reviewer.\n  tools:\n    - exec: cat\n      name: read_file\n    - exec: rg --json\n      name: search\n---\nReview the error handling in src/main.ts. Are there any uncaught\nexceptions?\n\n:::Assistant\n\n&lt;tool-call with=\"read_file\"&gt;\n&lt;arguments&gt;&lt;argv&gt;[\"src/main.ts\"]&lt;/argv&gt;&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆&lt;stdout&gt;// contents of main.ts...&lt;/stdout&gt;\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\nLooking at the code, I can see several places where exceptions might not\nbe caught...\n\n:::\nThe assistant reads files and searches the codebase to answer your question. Tool calls appear inline in the response so you can see exactly what happened.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01_introduction.html#next-steps",
    "href": "01_introduction.html#next-steps",
    "title": "Introduction to Lectic",
    "section": "",
    "text": "Getting Started: Install Lectic and run your first conversation.\nEditor Integration: Set up your editor for the best experience.\nConfiguration: Learn about the configuration system.\nTools: Give your LLM capabilities.\nCookbook: Ready-to-use recipes for common workflows.\n\nAll documentation concatenated into a single markdown file can be found here.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "02_getting_started.html",
    "href": "02_getting_started.html",
    "title": "Getting Started with Lectic",
    "section": "",
    "text": "This short guide helps you install Lectic and run your first conversation. Along the way, you will verify your install, set an API key, and see a simple tool in action.\n\n\nChoose the method that fits your system.\n\n\nDownload the AppImage from the GitHub Releases page. Make it executable and put it on your PATH.\nchmod +x lectic-*.AppImage\nmv lectic-*.AppImage ~/.local/bin/lectic\n\n\n\nDownload the macOS binary from the GitHub Releases page and put it on your PATH.\n\n\n\nIf you use Nix, install directly from the repository:\nnix profile install github:gleachkr/lectic\n\n\n\n\nlectic --version\nIf you see a version number, you are ready to go.\n\n\n\nLectic talks to LLM providers. Put at least one provider key in your environment:\nexport ANTHROPIC_API_KEY=\"your-api-key-here\"\nLectic chooses a default provider by checking for keys in this order: Anthropic → Gemini → OpenAI → OpenRouter. You only need one.\n\n\n\n\n\n\nNote\n\n\n\nYou can also use your ChatGPT subscription with provider: chatgpt. This does not use an API key environment variable, so it is not auto-selected. On first use, Lectic opens a browser window for login and stores tokens at $LECTIC_STATE/chatgpt_auth.json.\nThe login flow starts a local callback server on port 1455.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nCommon issue: If you see “No API key found,” make sure you exported the key in the same shell session where you’re running Lectic. If you set it in .bashrc, you may need to restart your terminal or run source ~/.bashrc.\n\n\n\n\n\n\n\nA Lectic conversation is a markdown file with a YAML header. The header configures the LLM (which we call an “interlocutor”). Everything below the header is the conversation: your messages as plain text, and the LLM’s responses in special :::Name blocks.\nHere’s a minimal example:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n---\n\nWhat is a fun fact about the Rust programming language?\nThe name identifies who’s speaking in the response blocks. The prompt is the system prompt. Lectic picks a default provider and model based on your API keys, so you don’t need to specify them.\n\n\n\nCreate a file called hello.lec with the content above, then run:\nlectic -i hello.lec\nThe -i flag updates the file in place. Lectic sends your message to the LLM and appends the response:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n---\n\nWhat is a fun fact about the Rust programming language?\n\n:::Assistant\n\nRust's mascot is a crab named Ferris! The name is a pun on \"ferrous,\"\nrelating to iron (Fe), which connects to \"rust.\" You'll often see Ferris\nin Rust documentation and community materials.\n\n:::\nTo continue the conversation, add your next message below the ::: block and run lectic -i hello.lec again.\n\n\n\nNow let’s give the assistant a tool. Create a new file called tools.lec:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  tools:\n    - exec: date\n      name: get_date\n---\n\nWhat is today's date?\nRun Lectic again. The assistant block will now include an XML tool call and the recorded results. You will see tags like &lt;tool-call&gt;, &lt;arguments&gt;, and &lt;results&gt; in the block.\n\n\n\n\n\n\nTip\n\n\n\nYou can load prompts from files or compute them with commands using file: and exec:. See External Prompts.\n\n\n\n\n\n\nHere are solutions to common issues when getting started.\n\n\nLectic needs at least one provider key in your environment. Make sure you’ve exported it in the same shell session:\nlectic -i tools.lec\nThe response now includes an XML block showing the tool call and its results:\n:::Assistant\n\n&lt;tool-call with=\"get_date\"&gt;\n&lt;arguments&gt;&lt;argv&gt;[]&lt;/argv&gt;&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆&lt;stdout&gt;Fri Jun 13 10:42:17 PDT 2025&lt;/stdout&gt;\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\nToday is Friday, June 13th, 2025.\n\n:::\nThe &lt;tool-call&gt; block is part of the conversation record. It shows what the LLM requested, what arguments it passed, and what came back. Editor plugins typically fold these blocks to reduce clutter.\n\n\n\n\n\n\nTip\n\n\n\nYou can load prompts from files or compute them with commands using file: and exec:. See External Prompts.\n\n\n\n\n\n\nLectic has an extensible tab completion system that supports standard flags and Custom Subcommands.\nTo enable it, source the completion script in your shell configuration (e.g., ~/.bashrc):\n# Adjust path to where you cloned/extracted Lectic\nsource /path/to/lectic/extra/tab_complete/lectic_completion.bash\nOr place the script in ~/.local/share/bash-completion/completions/.\n\n\n\n\n\nCheck that your YAML header is valid. Common mistakes:\n\nIndentation errors (YAML requires consistent spacing)\nMissing colons after keys\nForgetting the closing --- after the frontmatter\n\nThe LSP server catches many of these. See Editor Integration to set it up.\n\n\n\nModel names vary by provider. Use lectic models to see what’s available for your configured API keys.\n\n\n\nMake sure tools are defined under the tools key inside interlocutor, not at the top level:\n# Correct\ninterlocutor:\n  name: Assistant\n  prompt: You are helpful.\n  tools:\n    - exec: date\n      name: get_date\n\n# Wrong — tools at top level won't work\ninterlocutor:\n  name: Assistant\n  prompt: You are helpful.\ntools:\n  - exec: date\n\n\n\n\nNow that you have Lectic working:\n\nSet up your editor. The intended workflow is to run Lectic with a single keypress. See Editor Integration for Neovim, VS Code, and other editors.\nLearn the configuration system. You can set global defaults, project-specific settings, and per-conversation overrides. See Configuration.\nExplore the cookbook. The Cookbook has ready-to-use recipes for common workflows like coding assistants, commit message generation, and multi-perspective research.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "02_getting_started.html#installation",
    "href": "02_getting_started.html#installation",
    "title": "Getting Started with Lectic",
    "section": "",
    "text": "Choose the method that fits your system.\n\n\nDownload the AppImage from the GitHub Releases page. Make it executable and put it on your PATH.\nchmod +x lectic-*.AppImage\nmv lectic-*.AppImage ~/.local/bin/lectic\n\n\n\nDownload the macOS binary from the GitHub Releases page and put it on your PATH.\n\n\n\nIf you use Nix, install directly from the repository:\nnix profile install github:gleachkr/lectic",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "02_getting_started.html#verify-the-install",
    "href": "02_getting_started.html#verify-the-install",
    "title": "Getting Started with Lectic",
    "section": "",
    "text": "lectic --version\nIf you see a version number, you are ready to go.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "02_getting_started.html#set-up-an-api-key",
    "href": "02_getting_started.html#set-up-an-api-key",
    "title": "Getting Started with Lectic",
    "section": "",
    "text": "Lectic talks to LLM providers. Put at least one provider key in your environment:\nexport ANTHROPIC_API_KEY=\"your-api-key-here\"\nLectic chooses a default provider by checking for keys in this order: Anthropic → Gemini → OpenAI → OpenRouter. You only need one.\n\n\n\n\n\n\nNote\n\n\n\nYou can also use your ChatGPT subscription with provider: chatgpt. This does not use an API key environment variable, so it is not auto-selected. On first use, Lectic opens a browser window for login and stores tokens at $LECTIC_STATE/chatgpt_auth.json.\nThe login flow starts a local callback server on port 1455.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nCommon issue: If you see “No API key found,” make sure you exported the key in the same shell session where you’re running Lectic. If you set it in .bashrc, you may need to restart your terminal or run source ~/.bashrc.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "02_getting_started.html#your-first-conversation",
    "href": "02_getting_started.html#your-first-conversation",
    "title": "Getting Started with Lectic",
    "section": "",
    "text": "A Lectic conversation is a markdown file with a YAML header. The header configures the LLM (which we call an “interlocutor”). Everything below the header is the conversation: your messages as plain text, and the LLM’s responses in special :::Name blocks.\nHere’s a minimal example:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n---\n\nWhat is a fun fact about the Rust programming language?\nThe name identifies who’s speaking in the response blocks. The prompt is the system prompt. Lectic picks a default provider and model based on your API keys, so you don’t need to specify them.\n\n\n\nCreate a file called hello.lec with the content above, then run:\nlectic -i hello.lec\nThe -i flag updates the file in place. Lectic sends your message to the LLM and appends the response:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n---\n\nWhat is a fun fact about the Rust programming language?\n\n:::Assistant\n\nRust's mascot is a crab named Ferris! The name is a pun on \"ferrous,\"\nrelating to iron (Fe), which connects to \"rust.\" You'll often see Ferris\nin Rust documentation and community materials.\n\n:::\nTo continue the conversation, add your next message below the ::: block and run lectic -i hello.lec again.\n\n\n\nNow let’s give the assistant a tool. Create a new file called tools.lec:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  tools:\n    - exec: date\n      name: get_date\n---\n\nWhat is today's date?\nRun Lectic again. The assistant block will now include an XML tool call and the recorded results. You will see tags like &lt;tool-call&gt;, &lt;arguments&gt;, and &lt;results&gt; in the block.\n\n\n\n\n\n\nTip\n\n\n\nYou can load prompts from files or compute them with commands using file: and exec:. See External Prompts.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "02_getting_started.html#troubleshooting",
    "href": "02_getting_started.html#troubleshooting",
    "title": "Getting Started with Lectic",
    "section": "",
    "text": "Here are solutions to common issues when getting started.\n\n\nLectic needs at least one provider key in your environment. Make sure you’ve exported it in the same shell session:\nlectic -i tools.lec\nThe response now includes an XML block showing the tool call and its results:\n:::Assistant\n\n&lt;tool-call with=\"get_date\"&gt;\n&lt;arguments&gt;&lt;argv&gt;[]&lt;/argv&gt;&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆&lt;stdout&gt;Fri Jun 13 10:42:17 PDT 2025&lt;/stdout&gt;\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\nToday is Friday, June 13th, 2025.\n\n:::\nThe &lt;tool-call&gt; block is part of the conversation record. It shows what the LLM requested, what arguments it passed, and what came back. Editor plugins typically fold these blocks to reduce clutter.\n\n\n\n\n\n\nTip\n\n\n\nYou can load prompts from files or compute them with commands using file: and exec:. See External Prompts.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "02_getting_started.html#tab-completion-optional",
    "href": "02_getting_started.html#tab-completion-optional",
    "title": "Getting Started with Lectic",
    "section": "",
    "text": "Lectic has an extensible tab completion system that supports standard flags and Custom Subcommands.\nTo enable it, source the completion script in your shell configuration (e.g., ~/.bashrc):\n# Adjust path to where you cloned/extracted Lectic\nsource /path/to/lectic/extra/tab_complete/lectic_completion.bash\nOr place the script in ~/.local/share/bash-completion/completions/.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "02_getting_started.html#troubleshooting-1",
    "href": "02_getting_started.html#troubleshooting-1",
    "title": "Getting Started with Lectic",
    "section": "",
    "text": "Check that your YAML header is valid. Common mistakes:\n\nIndentation errors (YAML requires consistent spacing)\nMissing colons after keys\nForgetting the closing --- after the frontmatter\n\nThe LSP server catches many of these. See Editor Integration to set it up.\n\n\n\nModel names vary by provider. Use lectic models to see what’s available for your configured API keys.\n\n\n\nMake sure tools are defined under the tools key inside interlocutor, not at the top level:\n# Correct\ninterlocutor:\n  name: Assistant\n  prompt: You are helpful.\n  tools:\n    - exec: date\n      name: get_date\n\n# Wrong — tools at top level won't work\ninterlocutor:\n  name: Assistant\n  prompt: You are helpful.\ntools:\n  - exec: date",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "02_getting_started.html#next-steps",
    "href": "02_getting_started.html#next-steps",
    "title": "Getting Started with Lectic",
    "section": "",
    "text": "Now that you have Lectic working:\n\nSet up your editor. The intended workflow is to run Lectic with a single keypress. See Editor Integration for Neovim, VS Code, and other editors.\nLearn the configuration system. You can set global defaults, project-specific settings, and per-conversation overrides. See Configuration.\nExplore the cookbook. The Cookbook has ready-to-use recipes for common workflows like coding assistants, commit message generation, and multi-perspective research.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "reference/03_lsp.html",
    "href": "reference/03_lsp.html",
    "title": "LSP Server Reference",
    "section": "",
    "text": "Lectic includes a Language Server Protocol (LSP) server that makes editing .lec files more pleasant. It provides completions, diagnostics, folding, and other features that help you write correct configurations and navigate conversations.\nStart the server with:\nlectic lsp\nThe server uses stdio transport and works with any LSP-capable editor. See Editor Integration for setup instructions.\n\n\n\n\nThe LSP suggests completions as you type:\n\nDirectives: Type : to see built-in directives (:cmd, :env, :verbatim, :once, :discard, :attach, :ask, :aside, :reset) and any macros you’ve defined.\nInterlocutor names: Inside :ask[ or :aside[, the LSP suggests names from your configuration.\nYAML header fields: In the frontmatter, get suggestions for interlocutor properties (provider, model, thinking_effort, etc.), tool types, kit names, and model names.\nTool types: Type - inside a tools: array to see available tool kinds (exec, sqlite, mcp_command, native, etc.).\n\nCompletions are case-insensitive and respect any prefix you’ve typed.\n\n\n\nThe server catches errors before you run the file:\n\nYAML syntax errors: Malformed frontmatter is flagged immediately.\nMissing required fields: An interlocutor without a name or prompt triggers an error.\nUnknown properties: A typo like promt instead of prompt shows a warning.\nDuplicate names: If you define two interlocutors or macros with the same name, the LSP warns you. (The later definition wins at runtime.)\n\n\n\n\nTool calls and inline attachments can be long. The LSP provides folding ranges so your editor can collapse them:\n:::Assistant\n\n&lt;tool-call with=\"search\"&gt;        ← Fold starts here\n...                              ← Hidden when folded\n&lt;/tool-call&gt;                     ← Fold ends here\n\nBased on the search results...\n:::\nBoth Neovim and VS Code plugins enable folding by default.\n\n\n\nHover over elements to see documentation:\n\nDirectives: Hover on built-in directives to see what they do.\nMacros: Hover on a macro directive to preview its expansion.\nTool calls: Hover on a &lt;tool-call&gt; block to see a summary.\n\n\n\n\nJump to where things are defined:\n\nPlace your cursor on a macro name and invoke “Go to Definition” to jump to the macro’s definition in your config.\nWorks for interlocutors, kits, and macros.\nIf multiple definitions exist (e.g., a local override of a system config), the LSP returns all locations, prioritized by proximity.\n\n\n\n\nThe LSP provides document symbols showing the conversation structure: messages, interlocutor responses, and configuration sections. Use your editor’s outline view to navigate long conversations.\n\n\n\n\n\n\nAuto-start the LSP for .lec files:\nvim.api.nvim_create_autocmd(\"FileType\", {\n  pattern = { \"lectic\", \"markdown.lectic\", \"lectic.markdown\" },\n  callback = function(args)\n    vim.lsp.start({\n      name = \"lectic\",\n      cmd = { \"lectic\", \"lsp\" },\n      root_dir = vim.fs.root(args.buf, { \".git\", \"lectic.yaml\" })\n                 or vim.fn.getcwd(),\n      single_file_support = true,\n    })\n  end,\n})\nFor LSP-based folding:\nvim.opt.foldmethod = 'expr'\nvim.opt.foldexpr = 'v:lua.vim.lsp.foldexpr()'\nWith nvim-cmp, completions appear automatically on : and -. For bracket completions (interlocutor names), you may need to invoke completion manually with &lt;C-Space&gt; or &lt;C-x&gt;&lt;C-o&gt;.\n\n\n\nInstall the extension from extra/lectic.vscode or download the VSIX from releases. The extension starts the LSP automatically for .lec files.\n\n\n\nAny editor that supports LSP over stdio can use the Lectic server. The command is lectic lsp with no arguments. See your editor’s documentation for how to configure external language servers.\n\n\n\n\n\n\nCompletions are merged from multiple configuration sources, with later sources taking precedence:\n\nSystem config: $LECTIC_CONFIG/lectic.yaml\nWorkspace config: lectic.yaml in the document’s directory\nThe document’s YAML header\n\nThis matches the precedence used at runtime, so what you see in completions reflects what will actually be available.\n\n\n\n\n: triggers directive and macro completions\n[ after :ask or :aside triggers interlocutor name completions\n- in a tools: array triggers tool type completions\n\n\n\n\n\nCompletion previews are static. The server doesn’t expand exec: or read file: references when showing previews.\nNo completions are offered inside ::: fences (those are response blocks, not meant for editing).",
    "crumbs": [
      "Reference",
      "LSP Server"
    ]
  },
  {
    "objectID": "reference/03_lsp.html#features",
    "href": "reference/03_lsp.html#features",
    "title": "LSP Server Reference",
    "section": "",
    "text": "The LSP suggests completions as you type:\n\nDirectives: Type : to see built-in directives (:cmd, :env, :verbatim, :once, :discard, :attach, :ask, :aside, :reset) and any macros you’ve defined.\nInterlocutor names: Inside :ask[ or :aside[, the LSP suggests names from your configuration.\nYAML header fields: In the frontmatter, get suggestions for interlocutor properties (provider, model, thinking_effort, etc.), tool types, kit names, and model names.\nTool types: Type - inside a tools: array to see available tool kinds (exec, sqlite, mcp_command, native, etc.).\n\nCompletions are case-insensitive and respect any prefix you’ve typed.\n\n\n\nThe server catches errors before you run the file:\n\nYAML syntax errors: Malformed frontmatter is flagged immediately.\nMissing required fields: An interlocutor without a name or prompt triggers an error.\nUnknown properties: A typo like promt instead of prompt shows a warning.\nDuplicate names: If you define two interlocutors or macros with the same name, the LSP warns you. (The later definition wins at runtime.)\n\n\n\n\nTool calls and inline attachments can be long. The LSP provides folding ranges so your editor can collapse them:\n:::Assistant\n\n&lt;tool-call with=\"search\"&gt;        ← Fold starts here\n...                              ← Hidden when folded\n&lt;/tool-call&gt;                     ← Fold ends here\n\nBased on the search results...\n:::\nBoth Neovim and VS Code plugins enable folding by default.\n\n\n\nHover over elements to see documentation:\n\nDirectives: Hover on built-in directives to see what they do.\nMacros: Hover on a macro directive to preview its expansion.\nTool calls: Hover on a &lt;tool-call&gt; block to see a summary.\n\n\n\n\nJump to where things are defined:\n\nPlace your cursor on a macro name and invoke “Go to Definition” to jump to the macro’s definition in your config.\nWorks for interlocutors, kits, and macros.\nIf multiple definitions exist (e.g., a local override of a system config), the LSP returns all locations, prioritized by proximity.\n\n\n\n\nThe LSP provides document symbols showing the conversation structure: messages, interlocutor responses, and configuration sections. Use your editor’s outline view to navigate long conversations.",
    "crumbs": [
      "Reference",
      "LSP Server"
    ]
  },
  {
    "objectID": "reference/03_lsp.html#editor-setup",
    "href": "reference/03_lsp.html#editor-setup",
    "title": "LSP Server Reference",
    "section": "",
    "text": "Auto-start the LSP for .lec files:\nvim.api.nvim_create_autocmd(\"FileType\", {\n  pattern = { \"lectic\", \"markdown.lectic\", \"lectic.markdown\" },\n  callback = function(args)\n    vim.lsp.start({\n      name = \"lectic\",\n      cmd = { \"lectic\", \"lsp\" },\n      root_dir = vim.fs.root(args.buf, { \".git\", \"lectic.yaml\" })\n                 or vim.fn.getcwd(),\n      single_file_support = true,\n    })\n  end,\n})\nFor LSP-based folding:\nvim.opt.foldmethod = 'expr'\nvim.opt.foldexpr = 'v:lua.vim.lsp.foldexpr()'\nWith nvim-cmp, completions appear automatically on : and -. For bracket completions (interlocutor names), you may need to invoke completion manually with &lt;C-Space&gt; or &lt;C-x&gt;&lt;C-o&gt;.\n\n\n\nInstall the extension from extra/lectic.vscode or download the VSIX from releases. The extension starts the LSP automatically for .lec files.\n\n\n\nAny editor that supports LSP over stdio can use the Lectic server. The command is lectic lsp with no arguments. See your editor’s documentation for how to configure external language servers.",
    "crumbs": [
      "Reference",
      "LSP Server"
    ]
  },
  {
    "objectID": "reference/03_lsp.html#technical-details",
    "href": "reference/03_lsp.html#technical-details",
    "title": "LSP Server Reference",
    "section": "",
    "text": "Completions are merged from multiple configuration sources, with later sources taking precedence:\n\nSystem config: $LECTIC_CONFIG/lectic.yaml\nWorkspace config: lectic.yaml in the document’s directory\nThe document’s YAML header\n\nThis matches the precedence used at runtime, so what you see in completions reflects what will actually be available.\n\n\n\n\n: triggers directive and macro completions\n[ after :ask or :aside triggers interlocutor name completions\n- in a tools: array triggers tool type completions\n\n\n\n\n\nCompletion previews are static. The server doesn’t expand exec: or read file: references when showing previews.\nNo completions are offered inside ::: fences (those are response blocks, not meant for editing).",
    "crumbs": [
      "Reference",
      "LSP Server"
    ]
  },
  {
    "objectID": "04_conversation_format.html",
    "href": "04_conversation_format.html",
    "title": "The Lectic Conversation Format",
    "section": "",
    "text": "Lectic conversations are stored in plain markdown files, typically with a .lec extension. They use a superset of CommonMark, adding two specific conventions: a YAML frontmatter block for configuration and “container directives” for assistant responses.\n\n\nEvery Lectic file begins with a YAML frontmatter block, enclosed by three dashes (---). This is where you configure the conversation, defining the interlocutor(s), their models, prompts, and any tools they might use.\nA minimal header looks like this:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n---\nLectic picks a default provider and model based on your environment variables, so you often don’t need to specify them. If you want to be explicit:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic\n  model: claude-sonnet-4-20250514\n---\nThe frontmatter can be closed with either three dashes (---) or three periods (...). For a complete guide to all available options, see the Configuration page.\n\n\n\nAnything in the file that is not part of the YAML frontmatter or an assistant response block is considered a user message. You write your prompts, questions, and instructions here as plain text or standard markdown.\nThis is a user message.\n\nSo is this. You can include any markdown you like, such as **bold text** or\n`inline code`.\n\n\n\nLectic uses “container directives” to represent messages from the LLM. These are fenced blocks that start with a run of colons, followed immediately by the name of the interlocutor.\nThe canonical form is exactly three colons on open and close, like this:\n:::Name\n\nSome content.\n\n:::\nMarkdown code fences inside assistant blocks can also use three backticks.\n\n\nLectic records some generated content directly into the transcript as inline attachments. These are created by:\n\nThe :attach[...] directive (kind=\"attach\")\nInline hooks (kind=\"hook\")\n\nInline attachments appear inside the assistant’s response block as XML. They include a &lt;command&gt; field (the hook’s do value, or empty for :attach) and a &lt;content&gt; field.\nExample (:attach[...]):\n&lt;inline-attachment kind=\"attach\"&gt;\n&lt;command&gt;&lt;/command&gt;\n&lt;content type=\"text/plain\"&gt;\n┆some content here\n&lt;/content&gt;\n&lt;/inline-attachment&gt;\nExample (:attach[:cmd[...]]):\n&lt;inline-attachment kind=\"attach\"&gt;\n&lt;command&gt;&lt;/command&gt;\n&lt;content type=\"text/plain\"&gt;\n┆&lt;stdout from=\"git diff --staged\"&gt;diff --git a/src/main.ts b/src/main.ts\n┆...\n┆&lt;/stdout&gt;\n&lt;/content&gt;\n&lt;/inline-attachment&gt;\nExample (inline hook output):\n&lt;inline-attachment kind=\"hook\" final=\"true\"&gt;\n&lt;command&gt;~/.config/lectic/my-hook.sh&lt;/command&gt;\n&lt;content type=\"text/plain\"&gt;\n┆System check complete.\n&lt;/content&gt;\n&lt;/inline-attachment&gt;\nInline attachments serve two purposes:\n\nCaching: The results are stored in the file, so re-running Lectic doesn’t re-process old directives. Only :attach directives in the most recent user message are processed.\nContext positioning: When sending the conversation to the provider, attachments are treated as if they were a user message. This keeps provider caches stable and avoids token recomputation.\n\nYou’ll see inline attachments when using :attach (often with :cmd inside) or inline hooks. They’re part of the conversation record and should generally be left alone. Editor plugins typically fold them by default to reduce visual clutter.\n\n\n\n\n\n\nTip\n\n\n\nInline attachments are managed by Lectic. Don’t edit them by hand — if you need to re-run something, delete the attachment and add a new directive in your latest message.\n\n\n\n\n\nHere is a complete, simple conversation file showing all the parts together:\n---\ninterlocutor:\n  name: Oggle\n  prompt: You are a skeptical assistant.\n---\n\nI'd like to know more about container directives.\n\n:::Oggle\n\nAre you sure? It seems like a rather niche topic. They are part of a\nproposed extension to CommonMark that allows for custom block-level\nelements.\n\nInside one of these blocks, standard markdown is still supported:\n\n```python\n# This is a regular code block\nprint(\"Hello from inside a directive!\")\n```\n\nIs that all you wanted to know?\n\n:::\nWhen you run lectic, it reads the entire file, sends the content to the LLM, and then appends the next assistant response in a new directive block.",
    "crumbs": [
      "Core Concepts",
      "Conversation Format"
    ]
  },
  {
    "objectID": "04_conversation_format.html#yaml-frontmatter",
    "href": "04_conversation_format.html#yaml-frontmatter",
    "title": "The Lectic Conversation Format",
    "section": "",
    "text": "Every Lectic file begins with a YAML frontmatter block, enclosed by three dashes (---). This is where you configure the conversation, defining the interlocutor(s), their models, prompts, and any tools they might use.\nA minimal header looks like this:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n---\nLectic picks a default provider and model based on your environment variables, so you often don’t need to specify them. If you want to be explicit:\n---\ninterlocutor:\n  name: Assistant\n  prompt: You are a helpful assistant.\n  provider: anthropic\n  model: claude-sonnet-4-20250514\n---\nThe frontmatter can be closed with either three dashes (---) or three periods (...). For a complete guide to all available options, see the Configuration page.",
    "crumbs": [
      "Core Concepts",
      "Conversation Format"
    ]
  },
  {
    "objectID": "04_conversation_format.html#user-messages",
    "href": "04_conversation_format.html#user-messages",
    "title": "The Lectic Conversation Format",
    "section": "",
    "text": "Anything in the file that is not part of the YAML frontmatter or an assistant response block is considered a user message. You write your prompts, questions, and instructions here as plain text or standard markdown.\nThis is a user message.\n\nSo is this. You can include any markdown you like, such as **bold text** or\n`inline code`.",
    "crumbs": [
      "Core Concepts",
      "Conversation Format"
    ]
  },
  {
    "objectID": "04_conversation_format.html#assistant-responses",
    "href": "04_conversation_format.html#assistant-responses",
    "title": "The Lectic Conversation Format",
    "section": "",
    "text": "Lectic uses “container directives” to represent messages from the LLM. These are fenced blocks that start with a run of colons, followed immediately by the name of the interlocutor.\nThe canonical form is exactly three colons on open and close, like this:\n:::Name\n\nSome content.\n\n:::\nMarkdown code fences inside assistant blocks can also use three backticks.\n\n\nLectic records some generated content directly into the transcript as inline attachments. These are created by:\n\nThe :attach[...] directive (kind=\"attach\")\nInline hooks (kind=\"hook\")\n\nInline attachments appear inside the assistant’s response block as XML. They include a &lt;command&gt; field (the hook’s do value, or empty for :attach) and a &lt;content&gt; field.\nExample (:attach[...]):\n&lt;inline-attachment kind=\"attach\"&gt;\n&lt;command&gt;&lt;/command&gt;\n&lt;content type=\"text/plain\"&gt;\n┆some content here\n&lt;/content&gt;\n&lt;/inline-attachment&gt;\nExample (:attach[:cmd[...]]):\n&lt;inline-attachment kind=\"attach\"&gt;\n&lt;command&gt;&lt;/command&gt;\n&lt;content type=\"text/plain\"&gt;\n┆&lt;stdout from=\"git diff --staged\"&gt;diff --git a/src/main.ts b/src/main.ts\n┆...\n┆&lt;/stdout&gt;\n&lt;/content&gt;\n&lt;/inline-attachment&gt;\nExample (inline hook output):\n&lt;inline-attachment kind=\"hook\" final=\"true\"&gt;\n&lt;command&gt;~/.config/lectic/my-hook.sh&lt;/command&gt;\n&lt;content type=\"text/plain\"&gt;\n┆System check complete.\n&lt;/content&gt;\n&lt;/inline-attachment&gt;\nInline attachments serve two purposes:\n\nCaching: The results are stored in the file, so re-running Lectic doesn’t re-process old directives. Only :attach directives in the most recent user message are processed.\nContext positioning: When sending the conversation to the provider, attachments are treated as if they were a user message. This keeps provider caches stable and avoids token recomputation.\n\nYou’ll see inline attachments when using :attach (often with :cmd inside) or inline hooks. They’re part of the conversation record and should generally be left alone. Editor plugins typically fold them by default to reduce visual clutter.\n\n\n\n\n\n\nTip\n\n\n\nInline attachments are managed by Lectic. Don’t edit them by hand — if you need to re-run something, delete the attachment and add a new directive in your latest message.\n\n\n\n\n\nHere is a complete, simple conversation file showing all the parts together:\n---\ninterlocutor:\n  name: Oggle\n  prompt: You are a skeptical assistant.\n---\n\nI'd like to know more about container directives.\n\n:::Oggle\n\nAre you sure? It seems like a rather niche topic. They are part of a\nproposed extension to CommonMark that allows for custom block-level\nelements.\n\nInside one of these blocks, standard markdown is still supported:\n\n```python\n# This is a regular code block\nprint(\"Hello from inside a directive!\")\n```\n\nIs that all you wanted to know?\n\n:::\nWhen you run lectic, it reads the entire file, sends the content to the LLM, and then appends the next assistant response in a new directive block.",
    "crumbs": [
      "Core Concepts",
      "Conversation Format"
    ]
  },
  {
    "objectID": "tools/02_exec.html",
    "href": "tools/02_exec.html",
    "title": "Tools: Command Execution (exec)",
    "section": "",
    "text": "The exec tool is one of the most versatile tools in Lectic. It allows the LLM to execute commands and scripts, enabling it to interact directly with your system, run code, and interface with other command‑line applications.\n\n\nThe snippets below show only the tool definition. They assume you have an interlocutor with a valid prompt and model configuration. See Getting Started for a full header example.\nYou configure an exec tool by providing the command to be executed. You can also provide a custom name for the LLM to use, a usage guide, and optional parameters for security and execution control.\n\n\nThis configuration allows the LLM to run the python3 interpreter.\ntools:\n  - exec: python3\n    name: python\n    usage: &gt;\n      Use this to execute Python code. The code to be executed should be\n      written inside the tool call block.\n\n\n\nYou can also provide a multi‑line script in the YAML. The first line of the script must be a shebang (for example, #!/usr/bin/env bash) to choose the interpreter.\ntools:\n  - name: line_counter\n    usage: \"Counts the number of lines in a file. Takes one argument: path.\"\n    exec: |\n      #!/usr/bin/env bash\n      # A simple script to count the lines in a file\n      wc -l \"$1\"\n\n\n\n\nexec: (Required) The command or inline script to execute.\nname: An optional name for the tool.\nusage: A string with instructions for the LLM. It also accepts file: and exec: sources. See External Prompts for semantics.\nsandbox: A command string to wrap execution (e.g. /path/to/script.sh or wrapper.sh arg1). See safety below. Overrides any interlocutor-level or top-level sandbox.\ntimeoutSeconds: Seconds to wait before aborting a long‑running call.\nenv: Environment variables to set for the subprocess.\nschema: A map of parameter name → description. When present, the tool takes named string parameters (exposed as env vars). When absent, the tool instead takes a required arguments array of strings.\n\n\n\n\n\n\nNo shell is involved when executing single line commands. The command is executed directly. Shell features like globbing or command substitution will not work unless you invoke a shell yourself.\nSingle‑line exec values have environment variables expanded before execution using the tool’s env plus standard Lectic variables.\nSingle‑line commands are split into argv using simple shell‑like rules: single ‘…’ and double “…” quotes are supported; no globbing or substitution. If you need shell features, invoke a shell explicitly, e.g., bash -lc '...'.\nMulti‑line exec values must start with a shebang. Lectic writes the script to a temporary file and executes it with that interpreter.\n\n\n\nThe current working directory for exec is:\n\nIf you run with -f or -i: the directory containing the .lec file.\nOtherwise: the directory from which you invoked the lectic command.\n\nThis means relative paths in your commands and scripts resolve relative to that directory. Temporary scripts are written into the same working directory.\n\n\n\nLectic captures stdout and stderr separately and returns both to the model. It also includes the numeric exit code when it is non‑zero. You will see these serialized inside the tool call results as XML tags like , , and .\nIf a timeout occurs, Lectic kills the subprocess and throws an error that includes any partial stdout and stderr collected so far.\n\n\n\nYou might want to control what arguments your LLM can pass to a command or script, or offer a template for correct usage. If your configuration includes a schema, the LLM will be guided to provide specific parameters when calling the script or command. Each parameter is a string and Lectic exposes it to the subprocess via an environment variable with the same name.\nThis applies to both commands and scripts:\n\nFor scripts, parameters are available as $PARAM_NAME inside the script.\nFor commands, parameters are available in the subprocess environment and also expanded in the command.\n\nExample:\n# YAML configuration\ntools:\n  - name: greeter\n    exec: |\n      #!/usr/bin/env bash\n      echo \"Hello, ${NAME}! Today is ${DAY}.\"\n    schema:\n      NAME: The name to greet.\n      DAY: The day string to include.\nor equivalently\n# YAML configuration\ntools:\n  - name: greeter\n    exec: echo \"Hello, ${NAME}! Today is ${DAY}.\"\n    schema:\n      NAME: The name to greet.\n      DAY: The day string to include.\nIf the LLM provides { NAME: \"Ada\", DAY: \"Friday\" } Lectic will fill in the results:\n&lt;tool-call with=\"greeter\"&gt;\n&lt;arguments&gt;\n&lt;NAME&gt;\n┆Ada\n&lt;/NAME&gt;\n&lt;DAY&gt;\n┆Friday\n&lt;/DAY&gt;\n&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆Hello, Ada! Today is Friday.\n┆\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\n\n\n\nWhen Lectic runs your command or script, it sets a few helpful environment variables. In particular, LECTIC_INTERLOCUTOR is set to the name of the interlocutor who invoked the tool. This makes it easy to maintain per‑interlocutor state (for example, separate scratch directories or memory stores) in your scripts or sandbox wrappers.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nGranting an LLM the ability to execute commands can be dangerous. Treat every exec tool as a capability you are delegating. Combine human‑in‑the‑ loop confirmation and sandboxing to minimize risk. Do not expose sensitive files or networks unless you fully trust the tool and its usage.\n\n\nLectic provides two mechanisms to help you keep exec tools safe: hooks and sandboxing.\n\n\nYou can use the tool_use_pre hook to implement confirmation dialogs or logic. See Hooks for examples.\n\n\n\nWhen a sandbox is configured, a tool call will actually execute the sandbox command, which will receive the original command and the LLM provided parameters as arguments. The wrapper is responsible for creating a controlled environment to run the command.\nYou can include arguments in the sandbox string (e.g. bwrap.sh --net).\nSee the Custom Sandboxing cookbook recipe for a detailed guide on writing your own sandbox scripts.\nFor example, extra/sandbox/bwrap-sandbox.sh uses Bubblewrap to create a minimal, isolated environment with a temporary home directory.\nYou can also set a default sandbox at the top level (sandbox) or on the interlocutor object. If set, it applies to all exec tools that don’t specify their own. Tool-level sandbox wins over both defaults.",
    "crumbs": [
      "Tools",
      "Exec"
    ]
  },
  {
    "objectID": "tools/02_exec.html#configuration",
    "href": "tools/02_exec.html#configuration",
    "title": "Tools: Command Execution (exec)",
    "section": "",
    "text": "The snippets below show only the tool definition. They assume you have an interlocutor with a valid prompt and model configuration. See Getting Started for a full header example.\nYou configure an exec tool by providing the command to be executed. You can also provide a custom name for the LLM to use, a usage guide, and optional parameters for security and execution control.\n\n\nThis configuration allows the LLM to run the python3 interpreter.\ntools:\n  - exec: python3\n    name: python\n    usage: &gt;\n      Use this to execute Python code. The code to be executed should be\n      written inside the tool call block.\n\n\n\nYou can also provide a multi‑line script in the YAML. The first line of the script must be a shebang (for example, #!/usr/bin/env bash) to choose the interpreter.\ntools:\n  - name: line_counter\n    usage: \"Counts the number of lines in a file. Takes one argument: path.\"\n    exec: |\n      #!/usr/bin/env bash\n      # A simple script to count the lines in a file\n      wc -l \"$1\"\n\n\n\n\nexec: (Required) The command or inline script to execute.\nname: An optional name for the tool.\nusage: A string with instructions for the LLM. It also accepts file: and exec: sources. See External Prompts for semantics.\nsandbox: A command string to wrap execution (e.g. /path/to/script.sh or wrapper.sh arg1). See safety below. Overrides any interlocutor-level or top-level sandbox.\ntimeoutSeconds: Seconds to wait before aborting a long‑running call.\nenv: Environment variables to set for the subprocess.\nschema: A map of parameter name → description. When present, the tool takes named string parameters (exposed as env vars). When absent, the tool instead takes a required arguments array of strings.",
    "crumbs": [
      "Tools",
      "Exec"
    ]
  },
  {
    "objectID": "tools/02_exec.html#execution-details",
    "href": "tools/02_exec.html#execution-details",
    "title": "Tools: Command Execution (exec)",
    "section": "",
    "text": "No shell is involved when executing single line commands. The command is executed directly. Shell features like globbing or command substitution will not work unless you invoke a shell yourself.\nSingle‑line exec values have environment variables expanded before execution using the tool’s env plus standard Lectic variables.\nSingle‑line commands are split into argv using simple shell‑like rules: single ‘…’ and double “…” quotes are supported; no globbing or substitution. If you need shell features, invoke a shell explicitly, e.g., bash -lc '...'.\nMulti‑line exec values must start with a shebang. Lectic writes the script to a temporary file and executes it with that interpreter.\n\n\n\nThe current working directory for exec is:\n\nIf you run with -f or -i: the directory containing the .lec file.\nOtherwise: the directory from which you invoked the lectic command.\n\nThis means relative paths in your commands and scripts resolve relative to that directory. Temporary scripts are written into the same working directory.\n\n\n\nLectic captures stdout and stderr separately and returns both to the model. It also includes the numeric exit code when it is non‑zero. You will see these serialized inside the tool call results as XML tags like , , and .\nIf a timeout occurs, Lectic kills the subprocess and throws an error that includes any partial stdout and stderr collected so far.\n\n\n\nYou might want to control what arguments your LLM can pass to a command or script, or offer a template for correct usage. If your configuration includes a schema, the LLM will be guided to provide specific parameters when calling the script or command. Each parameter is a string and Lectic exposes it to the subprocess via an environment variable with the same name.\nThis applies to both commands and scripts:\n\nFor scripts, parameters are available as $PARAM_NAME inside the script.\nFor commands, parameters are available in the subprocess environment and also expanded in the command.\n\nExample:\n# YAML configuration\ntools:\n  - name: greeter\n    exec: |\n      #!/usr/bin/env bash\n      echo \"Hello, ${NAME}! Today is ${DAY}.\"\n    schema:\n      NAME: The name to greet.\n      DAY: The day string to include.\nor equivalently\n# YAML configuration\ntools:\n  - name: greeter\n    exec: echo \"Hello, ${NAME}! Today is ${DAY}.\"\n    schema:\n      NAME: The name to greet.\n      DAY: The day string to include.\nIf the LLM provides { NAME: \"Ada\", DAY: \"Friday\" } Lectic will fill in the results:\n&lt;tool-call with=\"greeter\"&gt;\n&lt;arguments&gt;\n&lt;NAME&gt;\n┆Ada\n&lt;/NAME&gt;\n&lt;DAY&gt;\n┆Friday\n&lt;/DAY&gt;\n&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆Hello, Ada! Today is Friday.\n┆\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;",
    "crumbs": [
      "Tools",
      "Exec"
    ]
  },
  {
    "objectID": "tools/02_exec.html#execution-environment",
    "href": "tools/02_exec.html#execution-environment",
    "title": "Tools: Command Execution (exec)",
    "section": "",
    "text": "When Lectic runs your command or script, it sets a few helpful environment variables. In particular, LECTIC_INTERLOCUTOR is set to the name of the interlocutor who invoked the tool. This makes it easy to maintain per‑interlocutor state (for example, separate scratch directories or memory stores) in your scripts or sandbox wrappers.",
    "crumbs": [
      "Tools",
      "Exec"
    ]
  },
  {
    "objectID": "tools/02_exec.html#safety-and-trust",
    "href": "tools/02_exec.html#safety-and-trust",
    "title": "Tools: Command Execution (exec)",
    "section": "",
    "text": "Warning\n\n\n\nGranting an LLM the ability to execute commands can be dangerous. Treat every exec tool as a capability you are delegating. Combine human‑in‑the‑ loop confirmation and sandboxing to minimize risk. Do not expose sensitive files or networks unless you fully trust the tool and its usage.\n\n\nLectic provides two mechanisms to help you keep exec tools safe: hooks and sandboxing.\n\n\nYou can use the tool_use_pre hook to implement confirmation dialogs or logic. See Hooks for examples.\n\n\n\nWhen a sandbox is configured, a tool call will actually execute the sandbox command, which will receive the original command and the LLM provided parameters as arguments. The wrapper is responsible for creating a controlled environment to run the command.\nYou can include arguments in the sandbox string (e.g. bwrap.sh --net).\nSee the Custom Sandboxing cookbook recipe for a detailed guide on writing your own sandbox scripts.\nFor example, extra/sandbox/bwrap-sandbox.sh uses Bubblewrap to create a minimal, isolated environment with a temporary home directory.\nYou can also set a default sandbox at the top level (sandbox) or on the interlocutor object. If set, it applies to all exec tools that don’t specify their own. Tool-level sandbox wins over both defaults.",
    "crumbs": [
      "Tools",
      "Exec"
    ]
  },
  {
    "objectID": "tools/06_other_tools.html",
    "href": "tools/06_other_tools.html",
    "title": "Other Tools: think, serve, and native",
    "section": "",
    "text": "This document covers three distinct types of tools: a cognitive tool for the LLM, a simple web server, and a way to access the native, built-in capabilities of the model provider.\n\n\nThe think tool gives the LLM a private “scratch space” to pause and reason about a prompt before formulating its final response. This can improve the quality and thoughtfulness of the output, especially for complex or ambiguous questions.\nThis technique was inspired by a post on Anthropic’s engineering blog. The output of the think tool is hidden from the user by default in the editor plugins, though it is still present in the .lec file.\n\n\ntools:\n  - think_about: &gt;\n      What the user is really asking for, and what hidden assumptions they\n      might have.\n    name: scratchpad # Optional name\n\n\n\nWhat's the best city in the world?\n\n:::Assistant\n\n&lt;tool-call with=\"scratchpad\"&gt;\n&lt;arguments&gt;\n&lt;thought&gt;\n┆\"Best\" is subjective. The user could mean best for travel, for\n┆food, for work, etc. I need to ask for clarification.\n&lt;/thought&gt;\n&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆thought complete.\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\nThat depends on what you're looking for! Are you interested in the best city\nfor tourism, career opportunities, or something else?\n:::\n\n\n\n\nThe serve tool lets the LLM show you interactive content—HTML pages, visualizations, small web apps—directly in your browser. This is Lectic’s answer to “artifacts” features you might have seen in web-based LLM interfaces.\nWhy use this instead of just writing HTML to a file? With serve, the content appears in your browser immediately. You don’t need to find the file and open it. The LLM can generate a data visualization, a diagram, or a playable game and you see it right away.\nWhen the LLM uses this tool, Lectic starts a server on the specified port and opens the page in your default browser. The server shuts down automatically after the first request is served, and the conversation resumes once your browser has loaded the page.\n\n\ntools:\n  - serve_on_port: 8080\n    name: web_server # Optional name\n\n\n\nGenerate a simple tic-tac-toe game in HTML and serve it to me.\n\n:::Assistant\n\n&lt;tool-call with=\"web_server\"&gt;\n&lt;arguments&gt;\n&lt;pageHtml&gt;\n┆&lt;!DOCTYPE html&gt;\n┆&lt;html&gt;\n┆&lt;head&gt;\n┆&lt;title&gt;Tic-Tac-Toe&lt;/title&gt;\n┆... (rest of the HTML/JS/CSS) ...\n┆&lt;/head&gt;\n┆&lt;body&gt;\n┆...\n┆&lt;/body&gt;\n┆&lt;/html&gt;\n&lt;/pageHtml&gt;\n&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆page is now available\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\n\nI have generated the game for you. It should be opening in your browser at\nhttp://localhost:8080.\n:::\n\n\n\n\nNative tools allow you to access functionality that is built directly into the LLM provider’s backend, such as web search or a code interpreter environment for data analysis.\nSupport for native tools varies by provider.\n\n\nYou enable native tools by specifying their type.\ntools:\n  - native: search # Enable the provider's built-in web search.\n  - native: code   # Enable the provider's built-in code interpreter.\n\n\n\n\nGemini: Supports both search and code. Note that the Gemini API has a limitation where you can only use one native tool at a time, and it cannot be combined with other (non-native) tools.\nAnthropic: Supports search only.\nOpenAI: Supports both search and code via the openai provider (not the legacy openai/chat provider).\nChatGPT: Supports both search and code via the chatgpt provider (ChatGPT subscription, Codex backend).",
    "crumbs": [
      "Tools",
      "Other Tools"
    ]
  },
  {
    "objectID": "tools/06_other_tools.html#the-think-tool",
    "href": "tools/06_other_tools.html#the-think-tool",
    "title": "Other Tools: think, serve, and native",
    "section": "",
    "text": "The think tool gives the LLM a private “scratch space” to pause and reason about a prompt before formulating its final response. This can improve the quality and thoughtfulness of the output, especially for complex or ambiguous questions.\nThis technique was inspired by a post on Anthropic’s engineering blog. The output of the think tool is hidden from the user by default in the editor plugins, though it is still present in the .lec file.\n\n\ntools:\n  - think_about: &gt;\n      What the user is really asking for, and what hidden assumptions they\n      might have.\n    name: scratchpad # Optional name\n\n\n\nWhat's the best city in the world?\n\n:::Assistant\n\n&lt;tool-call with=\"scratchpad\"&gt;\n&lt;arguments&gt;\n&lt;thought&gt;\n┆\"Best\" is subjective. The user could mean best for travel, for\n┆food, for work, etc. I need to ask for clarification.\n&lt;/thought&gt;\n&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆thought complete.\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\nThat depends on what you're looking for! Are you interested in the best city\nfor tourism, career opportunities, or something else?\n:::",
    "crumbs": [
      "Tools",
      "Other Tools"
    ]
  },
  {
    "objectID": "tools/06_other_tools.html#the-serve-tool",
    "href": "tools/06_other_tools.html#the-serve-tool",
    "title": "Other Tools: think, serve, and native",
    "section": "",
    "text": "The serve tool lets the LLM show you interactive content—HTML pages, visualizations, small web apps—directly in your browser. This is Lectic’s answer to “artifacts” features you might have seen in web-based LLM interfaces.\nWhy use this instead of just writing HTML to a file? With serve, the content appears in your browser immediately. You don’t need to find the file and open it. The LLM can generate a data visualization, a diagram, or a playable game and you see it right away.\nWhen the LLM uses this tool, Lectic starts a server on the specified port and opens the page in your default browser. The server shuts down automatically after the first request is served, and the conversation resumes once your browser has loaded the page.\n\n\ntools:\n  - serve_on_port: 8080\n    name: web_server # Optional name\n\n\n\nGenerate a simple tic-tac-toe game in HTML and serve it to me.\n\n:::Assistant\n\n&lt;tool-call with=\"web_server\"&gt;\n&lt;arguments&gt;\n&lt;pageHtml&gt;\n┆&lt;!DOCTYPE html&gt;\n┆&lt;html&gt;\n┆&lt;head&gt;\n┆&lt;title&gt;Tic-Tac-Toe&lt;/title&gt;\n┆... (rest of the HTML/JS/CSS) ...\n┆&lt;/head&gt;\n┆&lt;body&gt;\n┆...\n┆&lt;/body&gt;\n┆&lt;/html&gt;\n&lt;/pageHtml&gt;\n&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆page is now available\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\n\nI have generated the game for you. It should be opening in your browser at\nhttp://localhost:8080.\n:::",
    "crumbs": [
      "Tools",
      "Other Tools"
    ]
  },
  {
    "objectID": "tools/06_other_tools.html#native-tools",
    "href": "tools/06_other_tools.html#native-tools",
    "title": "Other Tools: think, serve, and native",
    "section": "",
    "text": "Native tools allow you to access functionality that is built directly into the LLM provider’s backend, such as web search or a code interpreter environment for data analysis.\nSupport for native tools varies by provider.\n\n\nYou enable native tools by specifying their type.\ntools:\n  - native: search # Enable the provider's built-in web search.\n  - native: code   # Enable the provider's built-in code interpreter.\n\n\n\n\nGemini: Supports both search and code. Note that the Gemini API has a limitation where you can only use one native tool at a time, and it cannot be combined with other (non-native) tools.\nAnthropic: Supports search only.\nOpenAI: Supports both search and code via the openai provider (not the legacy openai/chat provider).\nChatGPT: Supports both search and code via the chatgpt provider (ChatGPT subscription, Codex backend).",
    "crumbs": [
      "Tools",
      "Other Tools"
    ]
  },
  {
    "objectID": "tools/05_agent.html",
    "href": "tools/05_agent.html",
    "title": "Tools: Agent",
    "section": "",
    "text": "The agent tool allows you to create sophisticated multi-LLM workflows by enabling one interlocutor to call another as a tool. The “agent” interlocutor receives the query from the “caller” with no other context, processes it, and returns its response as the tool’s output.\nThis is a powerful way to separate concerns. You can create specialized agents and then compose them into more complex systems. For an excellent overview of the philosophy behind this approach, see Anthropic’s blog post on their multi-agent research system.\n\n\nTo use the agent tool, you must have at least two interlocutors defined. In the configuration for one interlocutor, you add an agent tool that points to the name of the other.\n\nagent: (Required) The name of the interlocutor to be called as an agent.\nname: An optional name for the tool.\nusage: A string, file:, or exec: URI providing instructions for the calling LLM on when and how to use this agent.\nraw_output: A boolean value. Normally an agent’s output will be sanitized, so that raw tool call results are not visible to the interlocutor who called the agent. Setting raw_output to true puts the full output from the agent into the main interlocutor’s tool call results.\n\n\n\nIn this setup, Kirk is the main interlocutor. He has a tool named communicator which, when used, will call the Spock interlocutor. Spock has his own set of tools, including a think_about tool to encourage careful reasoning.\ninterlocutors:\n  - name: Kirk\n    prompt: You are Captain Kirk. You are bold and decisive.\n    tools:\n      - agent: Spock\n        name: communicator\n        usage: Use this to contact Spock for logical analysis and advice.\n\n  - name: Spock\n    prompt: &gt;\n      You are Mr. Spock. You respond with pure logic, suppressing all\n      emotion.\n    tools:\n      - think_about: how to logically solve the problem presented.\n\n\n\n\nUsing the configuration above, Captain Kirk can delegate complex analysis to Spock.\nWe've encountered an alien vessel of unknown origin. It is not responding to\nhails. What is the logical course of action?\n\n:::Kirk\n\nThis situation requires careful analysis. I will consult my science officer.\n\n&lt;tool-call with=\"communicator\"&gt;\n&lt;arguments&gt;\n&lt;content&gt;\n┆Alien vessel, unknown origin, unresponsive. Propose logical course of action.\n&lt;/content&gt;\n&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆Insufficient data. Recommend passive scans to gather\n┆information on their technological capabilities before initiating\n┆further contact. Avoid any action that could be perceived as\n┆hostile.\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\nA logical approach. We will proceed with passive scans.\n\n:::\n\n\n\nAgents can call other agents, and those agents can call back. There’s no built-in limit on recursion depth, so be thoughtful about your configurations. An agent that calls itself (or creates a cycle) will keep going until it decides to stop or hits a token limit.\nFor example, Spock could have a tool that calls Kirk:\ninterlocutors:\n  - name: Kirk\n    prompt: You are Captain Kirk.\n    tools:\n      - agent: Spock\n        name: consult_spock\n\n  - name: Spock\n    prompt: You are Mr. Spock.\n    tools:\n      - agent: Kirk\n        name: consult_captain\nThis is powerful for workflows where agents need to collaborate, but it requires clear instructions in the prompts about when to delegate and when to answer directly.",
    "crumbs": [
      "Tools",
      "Agent"
    ]
  },
  {
    "objectID": "tools/05_agent.html#configuration",
    "href": "tools/05_agent.html#configuration",
    "title": "Tools: Agent",
    "section": "",
    "text": "To use the agent tool, you must have at least two interlocutors defined. In the configuration for one interlocutor, you add an agent tool that points to the name of the other.\n\nagent: (Required) The name of the interlocutor to be called as an agent.\nname: An optional name for the tool.\nusage: A string, file:, or exec: URI providing instructions for the calling LLM on when and how to use this agent.\nraw_output: A boolean value. Normally an agent’s output will be sanitized, so that raw tool call results are not visible to the interlocutor who called the agent. Setting raw_output to true puts the full output from the agent into the main interlocutor’s tool call results.\n\n\n\nIn this setup, Kirk is the main interlocutor. He has a tool named communicator which, when used, will call the Spock interlocutor. Spock has his own set of tools, including a think_about tool to encourage careful reasoning.\ninterlocutors:\n  - name: Kirk\n    prompt: You are Captain Kirk. You are bold and decisive.\n    tools:\n      - agent: Spock\n        name: communicator\n        usage: Use this to contact Spock for logical analysis and advice.\n\n  - name: Spock\n    prompt: &gt;\n      You are Mr. Spock. You respond with pure logic, suppressing all\n      emotion.\n    tools:\n      - think_about: how to logically solve the problem presented.",
    "crumbs": [
      "Tools",
      "Agent"
    ]
  },
  {
    "objectID": "tools/05_agent.html#example-conversation",
    "href": "tools/05_agent.html#example-conversation",
    "title": "Tools: Agent",
    "section": "",
    "text": "Using the configuration above, Captain Kirk can delegate complex analysis to Spock.\nWe've encountered an alien vessel of unknown origin. It is not responding to\nhails. What is the logical course of action?\n\n:::Kirk\n\nThis situation requires careful analysis. I will consult my science officer.\n\n&lt;tool-call with=\"communicator\"&gt;\n&lt;arguments&gt;\n&lt;content&gt;\n┆Alien vessel, unknown origin, unresponsive. Propose logical course of action.\n&lt;/content&gt;\n&lt;/arguments&gt;\n&lt;results&gt;\n&lt;result type=\"text\"&gt;\n┆Insufficient data. Recommend passive scans to gather\n┆information on their technological capabilities before initiating\n┆further contact. Avoid any action that could be perceived as\n┆hostile.\n&lt;/result&gt;\n&lt;/results&gt;\n&lt;/tool-call&gt;\n\nA logical approach. We will proceed with passive scans.\n\n:::",
    "crumbs": [
      "Tools",
      "Agent"
    ]
  },
  {
    "objectID": "tools/05_agent.html#recursion",
    "href": "tools/05_agent.html#recursion",
    "title": "Tools: Agent",
    "section": "",
    "text": "Agents can call other agents, and those agents can call back. There’s no built-in limit on recursion depth, so be thoughtful about your configurations. An agent that calls itself (or creates a cycle) will keep going until it decides to stop or hits a token limit.\nFor example, Spock could have a tool that calls Kirk:\ninterlocutors:\n  - name: Kirk\n    prompt: You are Captain Kirk.\n    tools:\n      - agent: Spock\n        name: consult_spock\n\n  - name: Spock\n    prompt: You are Mr. Spock.\n    tools:\n      - agent: Kirk\n        name: consult_captain\nThis is powerful for workflows where agents need to collaborate, but it requires clear instructions in the prompts about when to delegate and when to answer directly.",
    "crumbs": [
      "Tools",
      "Agent"
    ]
  },
  {
    "objectID": "automation/01_macros.html",
    "href": "automation/01_macros.html",
    "title": "Automation: Macros",
    "section": "",
    "text": "Lectic supports a simple but powerful macro system that allows you to define and reuse snippets of text. This is useful for saving frequently used prompts, automating repetitive workflows, and composing complex, multi-step commands.\nMacros are defined in your YAML configuration (either in a .lec file’s header or in an included configuration file).\n\n\nMacros are defined under the macros key. Each macro must have a name and an expansion. You can optionally provide an env map to set default environment variables for the expansion. You can also provide an optional description, which will be is shown in the LSP hover info for the macro.\nmacros:\n  - name: summarize\n    expansion: &gt;\n      Please provide a concise, single-paragraph summary of our\n      conversation so far, focusing on the key decisions made and\n      conclusions reached.\n\n  - name: build\n    env:\n      BUILD_DIR: ./dist\n    expansion: exec:echo \"Building in $BUILD_DIR\"\n\n\nThe expansion field can be a simple string, or it can load its content from a file or from the output of a command, just like the prompt field. For full semantics of file: and exec:, see External Prompts.\n\nFile Source: expansion: file:./prompts/summarize.txt\nCommand/Script Source:\n\nSingle line: expansion: exec:get-prompt-from-db --name summarize (executed directly, not via a shell)\nMulti‑line script: start with a shebang, e.g.\nexpansion: |\n  exec:#!/usr/bin/env bash\n  echo \"Hello, ${TARGET}!\"\nMulti‑line scripts are written to a temp file and executed with the interpreter given by the shebang.\n\n\n\n\n\n\nTo use a macro, you invoke it by writing the macro name as the directive name:\n\n:name[] expands the macro.\n:name[args] expands the macro and also passes args to the expansion as the ARG environment variable.\n\nWhen Lectic processes the file, it replaces the macro directive with the full text from its expansion field.\n\n\n\n\n\n\nNoteBuilt-in Macros\n\n\n\nLectic includes several built-in macros described in the next section. Because they are macros, they compose naturally with user-defined macros. For example, you can wrap :cmd in a caching macro: :cache[:cmd[expensive-command]].\n\n\nThis was a long and productive discussion. Could you wrap it up?\n\n:summarize[]\n\n\n\nLectic provides several built-in macros for common operations. These are always available without any configuration.\n\n\nRuns a shell command and expands to the output wrapped in XML.\nWhat's my current directory? :cmd[pwd]\nExpands to:\n&lt;stdout from=\"pwd\"&gt;/home/user/project&lt;/stdout&gt;\nIf the command fails, you get an error wrapper with both stdout and stderr. See External Content for full details on execution environment and error handling.\n\n\n\nExpands to the value of an environment variable. Useful for injecting configuration or paths without running a command.\nMy home directory is :env[HOME]\nIf the variable is not set, :env expands to an empty string.\n\n\n\nReturns the raw child text without expanding any macros inside it.\nHere's an example of macro syntax: :verbatim[:cmd[echo hello]]\nExpands to:\nHere's an example of macro syntax: :cmd[echo hello]\nThe inner :cmd is not executed — it appears literally in the output.\n\n\n\nOnly expands its children when processing the final (most recent) user message. In earlier messages, it expands to nothing.\nThis is useful for commands that should only run once, not be re-executed every time context is rebuilt:\n:once[:cmd[expensive-analysis-script]]\nWhen you add a new message and re-run Lectic, the :once directive in older messages will produce no output, while the one in your latest message will execute.\n\n\n\nExpands and evaluates its children (including any commands), but discards the output entirely. Useful for side effects.\n:discard[:cmd[echo \"logged\" &gt;&gt; activity.log]]\nThe command runs and writes to the log file, but nothing appears in the conversation. You can combine :once and :discard for cases where you only want the macro to run once, and you don’t want to pass the output to the LLM.\n\n\n\nCaptures its expanded children as an inline attachment stored in the assistant’s response block. Only processed in the final message.\n:attach[:cmd[git diff --staged]]\nSee External Content for full details on how inline attachments work and when to use them.\n\n\n\n\nWhen a macro expands via exec, the script being executed can be pased information via environment variables.\n\n\nThe text inside the directive brackets is passed to the macro expansion as the ARG environment variable.\nThis works for both single-line exec: commands and multi-line exec: scripts.\n\n:name[hello] sets ARG=hello.\nIf you explicitly set an ARG attribute, it overrides the bracket content: :name[hello]{ARG=\"override\"}.\n\n\n\n\nYou can pass environment variables to a macro’s expansion by adding attributes to the macro directive. These attributes are injected into the environment of exec: expansions when they run.\n\n:name[]{FOO=\"bar\"} sets the variable FOO to bar.\n:name[]{EMPTY} sets the variable EMPTY to be undefined. If you need an empty string value, write :name[]{EMPTY=\"\"}.\n\nNotes: - Single‑line exec: commands are not run through a shell. If you need shell features, invoke a shell explicitly, e.g., exec: bash -c 'echo \"Hello, $TARGET\"'. - In single‑line commands, variables in the command string are expanded before execution. For multi‑line scripts, variables are available to the script via the environment.\n\n\nConfiguration:\nmacros:\n  - name: greet\n    expansion: exec: bash -c 'echo \"Hello, $TARGET!\"'\nConversation:\n:greet[]{TARGET=\"World\"}\nWhen Lectic processes this, the directive will be replaced by the output of the exec command, which is “Hello, World!”.\n\n\n\n\nA few other environment variables are available by default.\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nMESSAGE_INDEX\nIndex (starting from one) of the message containing the macro\n\n\nMESSAGES_LENGTH\nTotal number of messages in the conversation\n\n\n\nThese might be useful for conditionally running only if the macro is, e.g. part of the most recent user message.\n\n\n\n\nMacros can interact with each other recursively. To support complex workflows, macros can define two separate expansion phases: pre and post.\n\npre: Expanded when the macro is first encountered (pre-order traversal). If pre returns content, the macro is replaced by that content, which is then recursively expanded. The original children are discarded.\npost: Expanded after the macro’s children have been processed (post-order traversal). The processed children are passed to post as the ARG variable.\n\nIf you define a macro with just expansion, it is treated as a post phase macro.\nHere’s how the phases work for a nested macro call like :outer[:inner[content]]:\n:outer[:inner[content]]\n   │\n   ▼\n┌─────────────────────────────────────────────────────┐\n│ 1. Run :outer's PRE                                 │\n│    - If it returns content → replace :outer,        │\n│      recursively expand the result, DONE            │\n│    - If it returns nothing → continue to children   │\n└─────────────────────────────────────────────────────┘\n   │\n   ▼\n┌─────────────────────────────────────────────────────┐\n│ 2. Process children: :inner[content]                │\n│    - Run :inner's PRE                               │\n│    - Process :inner's children (\"content\")          │\n│    - Run :inner's POST with children as ARG         │\n│    - Replace :inner with result                     │\n└─────────────────────────────────────────────────────┘\n   │\n   ▼\n┌─────────────────────────────────────────────────────┐\n│ 3. Run :outer's POST                                │\n│    - ARG = processed children (result of :inner)    │\n│    - Replace :outer with result                     │\n└─────────────────────────────────────────────────────┘\nThe key insight: pre lets you short-circuit (skip children entirely), or change the children of a directive, while post lets you wrap or transform the fully expanded results of the children.\n\n\nIf the pre script runs but produces no output (an empty string), Lectic treats this as a “pass-through”. The macro is NOT replaced; instead, Lectic proceeds to process the macro’s children and then runs the post phase.\nThis makes it easy to implement cache checks or conditional logic.\n\n\n\n\n\n\nTip\n\n\n\nIf you explicitly want to delete a node during the pre phase (stopping recursion and producing no output), you cannot return an empty string. Instead, return an empty HTML comment: &lt;!-- --&gt;. This stops recursion and renders as nothing.\n\n\n\n\n\nThis design allows for powerful compositions, such as a caching macro that wraps expensive operations.\nmacros:\n  - name: cache\n    # Check for cache hit. If found, cat the file.\n    # If not found, the script produces no output (empty string),\n    # so Lectic proceeds to expand the children.\n    pre: |\n      exec:#!/bin/bash\n      HASH=$(echo \"$ARG\" | md5sum | cut -d' ' -f1)\n      if [ -f \"/tmp/cache/$HASH\" ]; then\n        cat \"/tmp/cache/$HASH\"\n      fi\n    # If we reached post, it means pre didn't return anything (cache miss).\n    # We now have the result of the children in ARG. Save it and output it.\n    post: |\n      exec:#!/bin/bash\n      HASH=$(echo \"$ARG\" | md5sum | cut -d' ' -f1)\n      mkdir -p /tmp/cache\n      echo \"$ARG\" &gt; \"/tmp/cache/$HASH\"\n      echo \"$ARG\"\nUsage:\n:cache[:summarize[:cat[file.txt]]]\n\n:cache’s pre runs. If the cache exists for the raw text of the children, it returns the cached summary. Lectic replaces the :cache block with this text and is done.\nIf pre returns nothing (cache miss), Lectic enters the children.\n:cat expands to the file content.\n:summarize processes that content.\nFinally, :cache’s post runs. ARG contains the summary. It writes ARG to the cache and outputs it.",
    "crumbs": [
      "Automation",
      "Macros"
    ]
  },
  {
    "objectID": "automation/01_macros.html#defining-macros",
    "href": "automation/01_macros.html#defining-macros",
    "title": "Automation: Macros",
    "section": "",
    "text": "Macros are defined under the macros key. Each macro must have a name and an expansion. You can optionally provide an env map to set default environment variables for the expansion. You can also provide an optional description, which will be is shown in the LSP hover info for the macro.\nmacros:\n  - name: summarize\n    expansion: &gt;\n      Please provide a concise, single-paragraph summary of our\n      conversation so far, focusing on the key decisions made and\n      conclusions reached.\n\n  - name: build\n    env:\n      BUILD_DIR: ./dist\n    expansion: exec:echo \"Building in $BUILD_DIR\"\n\n\nThe expansion field can be a simple string, or it can load its content from a file or from the output of a command, just like the prompt field. For full semantics of file: and exec:, see External Prompts.\n\nFile Source: expansion: file:./prompts/summarize.txt\nCommand/Script Source:\n\nSingle line: expansion: exec:get-prompt-from-db --name summarize (executed directly, not via a shell)\nMulti‑line script: start with a shebang, e.g.\nexpansion: |\n  exec:#!/usr/bin/env bash\n  echo \"Hello, ${TARGET}!\"\nMulti‑line scripts are written to a temp file and executed with the interpreter given by the shebang.",
    "crumbs": [
      "Automation",
      "Macros"
    ]
  },
  {
    "objectID": "automation/01_macros.html#using-macros",
    "href": "automation/01_macros.html#using-macros",
    "title": "Automation: Macros",
    "section": "",
    "text": "To use a macro, you invoke it by writing the macro name as the directive name:\n\n:name[] expands the macro.\n:name[args] expands the macro and also passes args to the expansion as the ARG environment variable.\n\nWhen Lectic processes the file, it replaces the macro directive with the full text from its expansion field.\n\n\n\n\n\n\nNoteBuilt-in Macros\n\n\n\nLectic includes several built-in macros described in the next section. Because they are macros, they compose naturally with user-defined macros. For example, you can wrap :cmd in a caching macro: :cache[:cmd[expensive-command]].\n\n\nThis was a long and productive discussion. Could you wrap it up?\n\n:summarize[]",
    "crumbs": [
      "Automation",
      "Macros"
    ]
  },
  {
    "objectID": "automation/01_macros.html#built-in-macros-reference",
    "href": "automation/01_macros.html#built-in-macros-reference",
    "title": "Automation: Macros",
    "section": "",
    "text": "Lectic provides several built-in macros for common operations. These are always available without any configuration.\n\n\nRuns a shell command and expands to the output wrapped in XML.\nWhat's my current directory? :cmd[pwd]\nExpands to:\n&lt;stdout from=\"pwd\"&gt;/home/user/project&lt;/stdout&gt;\nIf the command fails, you get an error wrapper with both stdout and stderr. See External Content for full details on execution environment and error handling.\n\n\n\nExpands to the value of an environment variable. Useful for injecting configuration or paths without running a command.\nMy home directory is :env[HOME]\nIf the variable is not set, :env expands to an empty string.\n\n\n\nReturns the raw child text without expanding any macros inside it.\nHere's an example of macro syntax: :verbatim[:cmd[echo hello]]\nExpands to:\nHere's an example of macro syntax: :cmd[echo hello]\nThe inner :cmd is not executed — it appears literally in the output.\n\n\n\nOnly expands its children when processing the final (most recent) user message. In earlier messages, it expands to nothing.\nThis is useful for commands that should only run once, not be re-executed every time context is rebuilt:\n:once[:cmd[expensive-analysis-script]]\nWhen you add a new message and re-run Lectic, the :once directive in older messages will produce no output, while the one in your latest message will execute.\n\n\n\nExpands and evaluates its children (including any commands), but discards the output entirely. Useful for side effects.\n:discard[:cmd[echo \"logged\" &gt;&gt; activity.log]]\nThe command runs and writes to the log file, but nothing appears in the conversation. You can combine :once and :discard for cases where you only want the macro to run once, and you don’t want to pass the output to the LLM.\n\n\n\nCaptures its expanded children as an inline attachment stored in the assistant’s response block. Only processed in the final message.\n:attach[:cmd[git diff --staged]]\nSee External Content for full details on how inline attachments work and when to use them.",
    "crumbs": [
      "Automation",
      "Macros"
    ]
  },
  {
    "objectID": "automation/01_macros.html#the-macro-expansion-environment",
    "href": "automation/01_macros.html#the-macro-expansion-environment",
    "title": "Automation: Macros",
    "section": "",
    "text": "When a macro expands via exec, the script being executed can be pased information via environment variables.\n\n\nThe text inside the directive brackets is passed to the macro expansion as the ARG environment variable.\nThis works for both single-line exec: commands and multi-line exec: scripts.\n\n:name[hello] sets ARG=hello.\nIf you explicitly set an ARG attribute, it overrides the bracket content: :name[hello]{ARG=\"override\"}.\n\n\n\n\nYou can pass environment variables to a macro’s expansion by adding attributes to the macro directive. These attributes are injected into the environment of exec: expansions when they run.\n\n:name[]{FOO=\"bar\"} sets the variable FOO to bar.\n:name[]{EMPTY} sets the variable EMPTY to be undefined. If you need an empty string value, write :name[]{EMPTY=\"\"}.\n\nNotes: - Single‑line exec: commands are not run through a shell. If you need shell features, invoke a shell explicitly, e.g., exec: bash -c 'echo \"Hello, $TARGET\"'. - In single‑line commands, variables in the command string are expanded before execution. For multi‑line scripts, variables are available to the script via the environment.\n\n\nConfiguration:\nmacros:\n  - name: greet\n    expansion: exec: bash -c 'echo \"Hello, $TARGET!\"'\nConversation:\n:greet[]{TARGET=\"World\"}\nWhen Lectic processes this, the directive will be replaced by the output of the exec command, which is “Hello, World!”.\n\n\n\n\nA few other environment variables are available by default.\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nMESSAGE_INDEX\nIndex (starting from one) of the message containing the macro\n\n\nMESSAGES_LENGTH\nTotal number of messages in the conversation\n\n\n\nThese might be useful for conditionally running only if the macro is, e.g. part of the most recent user message.",
    "crumbs": [
      "Automation",
      "Macros"
    ]
  },
  {
    "objectID": "automation/01_macros.html#advanced-macros-phases-and-recursion",
    "href": "automation/01_macros.html#advanced-macros-phases-and-recursion",
    "title": "Automation: Macros",
    "section": "",
    "text": "Macros can interact with each other recursively. To support complex workflows, macros can define two separate expansion phases: pre and post.\n\npre: Expanded when the macro is first encountered (pre-order traversal). If pre returns content, the macro is replaced by that content, which is then recursively expanded. The original children are discarded.\npost: Expanded after the macro’s children have been processed (post-order traversal). The processed children are passed to post as the ARG variable.\n\nIf you define a macro with just expansion, it is treated as a post phase macro.\nHere’s how the phases work for a nested macro call like :outer[:inner[content]]:\n:outer[:inner[content]]\n   │\n   ▼\n┌─────────────────────────────────────────────────────┐\n│ 1. Run :outer's PRE                                 │\n│    - If it returns content → replace :outer,        │\n│      recursively expand the result, DONE            │\n│    - If it returns nothing → continue to children   │\n└─────────────────────────────────────────────────────┘\n   │\n   ▼\n┌─────────────────────────────────────────────────────┐\n│ 2. Process children: :inner[content]                │\n│    - Run :inner's PRE                               │\n│    - Process :inner's children (\"content\")          │\n│    - Run :inner's POST with children as ARG         │\n│    - Replace :inner with result                     │\n└─────────────────────────────────────────────────────┘\n   │\n   ▼\n┌─────────────────────────────────────────────────────┐\n│ 3. Run :outer's POST                                │\n│    - ARG = processed children (result of :inner)    │\n│    - Replace :outer with result                     │\n└─────────────────────────────────────────────────────┘\nThe key insight: pre lets you short-circuit (skip children entirely), or change the children of a directive, while post lets you wrap or transform the fully expanded results of the children.\n\n\nIf the pre script runs but produces no output (an empty string), Lectic treats this as a “pass-through”. The macro is NOT replaced; instead, Lectic proceeds to process the macro’s children and then runs the post phase.\nThis makes it easy to implement cache checks or conditional logic.\n\n\n\n\n\n\nTip\n\n\n\nIf you explicitly want to delete a node during the pre phase (stopping recursion and producing no output), you cannot return an empty string. Instead, return an empty HTML comment: &lt;!-- --&gt;. This stops recursion and renders as nothing.\n\n\n\n\n\nThis design allows for powerful compositions, such as a caching macro that wraps expensive operations.\nmacros:\n  - name: cache\n    # Check for cache hit. If found, cat the file.\n    # If not found, the script produces no output (empty string),\n    # so Lectic proceeds to expand the children.\n    pre: |\n      exec:#!/bin/bash\n      HASH=$(echo \"$ARG\" | md5sum | cut -d' ' -f1)\n      if [ -f \"/tmp/cache/$HASH\" ]; then\n        cat \"/tmp/cache/$HASH\"\n      fi\n    # If we reached post, it means pre didn't return anything (cache miss).\n    # We now have the result of the children in ARG. Save it and output it.\n    post: |\n      exec:#!/bin/bash\n      HASH=$(echo \"$ARG\" | md5sum | cut -d' ' -f1)\n      mkdir -p /tmp/cache\n      echo \"$ARG\" &gt; \"/tmp/cache/$HASH\"\n      echo \"$ARG\"\nUsage:\n:cache[:summarize[:cat[file.txt]]]\n\n:cache’s pre runs. If the cache exists for the raw text of the children, it returns the cached summary. Lectic replaces the :cache block with this text and is done.\nIf pre returns nothing (cache miss), Lectic enters the children.\n:cat expands to the file content.\n:summarize processes that content.\nFinally, :cache’s post runs. ARG contains the summary. It writes ARG to the cache and outputs it.",
    "crumbs": [
      "Automation",
      "Macros"
    ]
  },
  {
    "objectID": "automation/03_custom_subcommands.html",
    "href": "automation/03_custom_subcommands.html",
    "title": "Automation: Custom Subcommands",
    "section": "",
    "text": "Lectic’s CLI is extensible through “git-style” custom subcommands. If you create an executable named lectic-&lt;command&gt;, or lectic-&lt;command&gt;.&lt;file-extension&gt; and place it in your configuration directory, data directory, or PATH, you can invoke it as lectic &lt;command&gt;.\nThis allows you to wrap common workflows, build project-specific tools, and create shortcuts for complex Lectic invocations.\n\n\nWhen you run lectic foo args..., Lectic searches for an executable named lectic-foo or lectic-foo.* in the following locations, in order:\n\nConfiguration Directory: $LECTIC_CONFIG (defaults to ~/.config/lectic on Linux)\nData Directory: $LECTIC_DATA (defaults to ~/.local/share/lectic on Linux)\nSystem PATH: Any directory in your $PATH.\n\nThe first match found is executed. The subprocess receives the remaining arguments, inherits the standard input, output, and error streams, and has access to Lectic’s environment variables.\n\n\n\n\n\nCreate a file named lectic-hello in ~/.config/lectic/:\n#!/bin/bash\necho \"Hello from a custom subcommand!\"\necho \"My config dir is: $LECTIC_CONFIG\"\nMake it executable: chmod +x ~/.config/lectic/lectic-hello\nRun it:\nlectic hello\n\n\n\nLectic bundles a Bun runtime, so you can write subcommands in JavaScript or TypeScript without installing anything extra. Use lectic script as your shebang interpreter.\nCreate ~/.config/lectic/lectic-calc:\n#!/usr/bin/env -S lectic script\n\nconst args = process.argv.slice(2);\nif (args.length === 0) {\n  console.error(\"Usage: lectic calc &lt;expression&gt;\");\n  process.exit(1);\n}\n\n// Access standard Lectic environment variables\nconst configDir = process.env.LECTIC_CONFIG;\n\ntry {\n  console.log(eval(args.join(\" \")));\n} catch (e) {\n  console.error(\"Error:\", e.message);\n}\nMake it executable and run:\nlectic calc 1 + 2\n\n\n\n\n\n\nTipWhy lectic script?\n\n\n\nYou get Bun’s full capabilities without installing Bun separately. This includes built-in YAML parsing, HTTP servers, SQLite, fetch, and much more. See Bun’s documentation for what’s available.\nThis is especially useful for writing more complex subcommands that would be awkward in Bash.\n\n\n\n\n\n\nSubcommands receive the standard set of Lectic environment variables:\n\nLECTIC_CONFIG: Path to the configuration directory.\nLECTIC_DATA: Path to the data directory.\nLECTIC_CACHE: Path to the cache directory.\nLECTIC_STATE: Path to the state directory.\nLECTIC_TEMP: Path to the temporary directory.\n\nThese ensure your subcommands respect the user’s directory configuration.\n\n\n\nYou can add tab completion for your custom subcommands. The completion system supports plugging in custom completion functions.\n\n\nFirst, ensure you have enabled tab completion by sourcing the completion script in your shell configuration (e.g., ~/.bashrc):\nsource /path/to/lectic/extra/tab_complete/lectic_completion.bash\n(The path depends on how you installed Lectic. If you installed via Nix or an AppImage, you may need to locate this file in the repository or extract it.)\n\n\n\nTo provide completions for a subcommand lectic-foo, create a bash script that defines a completion function and registers it.\nThe script can be placed in: 1. ~/.config/lectic/completions/ 2. ~/.local/share/lectic/completions/ 3. Or alongside the executable itself, named lectic-foo.completion.bash.\nExample:\nCreate ~/.config/lectic/completions/foo.bash:\n_lectic_complete_foo() {\n  local cur\n  cur=\"${COMP_WORDS[COMP_CWORD]}\"\n  # Suggest 'bar' and 'baz'\n  COMPREPLY=( $(compgen -W \"bar baz\" -- \"${cur}\") )\n}\n\n# Register the function for the 'foo' subcommand\nlectic_register_completion foo _lectic_complete_foo\nNow, typing lectic foo &lt;TAB&gt; will suggest bar and baz.\n\n\n\n\n\n\nTip\n\n\n\nFor performance, define completions in a separate .completion.bash file rather than inside the subcommand script itself. This allows the shell to load completions without executing the subcommand.",
    "crumbs": [
      "Automation",
      "Subcommands"
    ]
  },
  {
    "objectID": "automation/03_custom_subcommands.html#how-it-works",
    "href": "automation/03_custom_subcommands.html#how-it-works",
    "title": "Automation: Custom Subcommands",
    "section": "",
    "text": "When you run lectic foo args..., Lectic searches for an executable named lectic-foo or lectic-foo.* in the following locations, in order:\n\nConfiguration Directory: $LECTIC_CONFIG (defaults to ~/.config/lectic on Linux)\nData Directory: $LECTIC_DATA (defaults to ~/.local/share/lectic on Linux)\nSystem PATH: Any directory in your $PATH.\n\nThe first match found is executed. The subprocess receives the remaining arguments, inherits the standard input, output, and error streams, and has access to Lectic’s environment variables.",
    "crumbs": [
      "Automation",
      "Subcommands"
    ]
  },
  {
    "objectID": "automation/03_custom_subcommands.html#examples",
    "href": "automation/03_custom_subcommands.html#examples",
    "title": "Automation: Custom Subcommands",
    "section": "",
    "text": "Create a file named lectic-hello in ~/.config/lectic/:\n#!/bin/bash\necho \"Hello from a custom subcommand!\"\necho \"My config dir is: $LECTIC_CONFIG\"\nMake it executable: chmod +x ~/.config/lectic/lectic-hello\nRun it:\nlectic hello\n\n\n\nLectic bundles a Bun runtime, so you can write subcommands in JavaScript or TypeScript without installing anything extra. Use lectic script as your shebang interpreter.\nCreate ~/.config/lectic/lectic-calc:\n#!/usr/bin/env -S lectic script\n\nconst args = process.argv.slice(2);\nif (args.length === 0) {\n  console.error(\"Usage: lectic calc &lt;expression&gt;\");\n  process.exit(1);\n}\n\n// Access standard Lectic environment variables\nconst configDir = process.env.LECTIC_CONFIG;\n\ntry {\n  console.log(eval(args.join(\" \")));\n} catch (e) {\n  console.error(\"Error:\", e.message);\n}\nMake it executable and run:\nlectic calc 1 + 2\n\n\n\n\n\n\nTipWhy lectic script?\n\n\n\nYou get Bun’s full capabilities without installing Bun separately. This includes built-in YAML parsing, HTTP servers, SQLite, fetch, and much more. See Bun’s documentation for what’s available.\nThis is especially useful for writing more complex subcommands that would be awkward in Bash.",
    "crumbs": [
      "Automation",
      "Subcommands"
    ]
  },
  {
    "objectID": "automation/03_custom_subcommands.html#environment-variables",
    "href": "automation/03_custom_subcommands.html#environment-variables",
    "title": "Automation: Custom Subcommands",
    "section": "",
    "text": "Subcommands receive the standard set of Lectic environment variables:\n\nLECTIC_CONFIG: Path to the configuration directory.\nLECTIC_DATA: Path to the data directory.\nLECTIC_CACHE: Path to the cache directory.\nLECTIC_STATE: Path to the state directory.\nLECTIC_TEMP: Path to the temporary directory.\n\nThese ensure your subcommands respect the user’s directory configuration.",
    "crumbs": [
      "Automation",
      "Subcommands"
    ]
  },
  {
    "objectID": "automation/03_custom_subcommands.html#tab-completion",
    "href": "automation/03_custom_subcommands.html#tab-completion",
    "title": "Automation: Custom Subcommands",
    "section": "",
    "text": "You can add tab completion for your custom subcommands. The completion system supports plugging in custom completion functions.\n\n\nFirst, ensure you have enabled tab completion by sourcing the completion script in your shell configuration (e.g., ~/.bashrc):\nsource /path/to/lectic/extra/tab_complete/lectic_completion.bash\n(The path depends on how you installed Lectic. If you installed via Nix or an AppImage, you may need to locate this file in the repository or extract it.)\n\n\n\nTo provide completions for a subcommand lectic-foo, create a bash script that defines a completion function and registers it.\nThe script can be placed in: 1. ~/.config/lectic/completions/ 2. ~/.local/share/lectic/completions/ 3. Or alongside the executable itself, named lectic-foo.completion.bash.\nExample:\nCreate ~/.config/lectic/completions/foo.bash:\n_lectic_complete_foo() {\n  local cur\n  cur=\"${COMP_WORDS[COMP_CWORD]}\"\n  # Suggest 'bar' and 'baz'\n  COMPREPLY=( $(compgen -W \"bar baz\" -- \"${cur}\") )\n}\n\n# Register the function for the 'foo' subcommand\nlectic_register_completion foo _lectic_complete_foo\nNow, typing lectic foo &lt;TAB&gt; will suggest bar and baz.\n\n\n\n\n\n\nTip\n\n\n\nFor performance, define completions in a separate .completion.bash file rather than inside the subcommand script itself. This allows the shell to load completions without executing the subcommand.",
    "crumbs": [
      "Automation",
      "Subcommands"
    ]
  },
  {
    "objectID": "cookbook/08_skills_subcommand.html",
    "href": "cookbook/08_skills_subcommand.html",
    "title": "Recipe: Agent Skills Support",
    "section": "",
    "text": "This recipe adds support for the Agent Skills format by building a custom lectic skills subcommand.\n\n\nAgent Skills is an open format for packaging reusable capabilities that LLMs can load on demand. A skill is a folder containing:\n\nSKILL.md: Instructions for the LLM (with YAML frontmatter for metadata)\nscripts/: Executable scripts the LLM can run\nreferences/: Documentation and examples the LLM can read\n\nThe format emphasizes “progressive disclosure”—the LLM sees only skill names and descriptions initially, then loads full instructions only when needed. This keeps prompts small while giving the LLM access to a large library of capabilities.\n\n\n\nWe’ll build a subcommand that gives your LLM access to a library of skills while keeping the base prompt small. It follows the skills spec’s “progressive disclosure” approach:\n\nDiscovery: load only each skill’s name + description.\nActivation: load the full SKILL.md instructions for one skill.\nResources: read reference files or run scripts only when needed.\n\n\n\n\nLectic resolves lectic &lt;command&gt; using git-style subcommands. We will create an executable named lectic-skills.ts (TypeScript), using Lectic’s built-in script interpreter.\nCopy the repository’s extra/lectic-skills.ts to your config directory:\ncp /path/to/lectic/extra/lectic-skills.ts ~/.config/lectic/lectic-skills.ts\nchmod +x ~/.config/lectic/lectic-skills.ts\nNow lectic skills should resolve to that file.\n\n\n\nThe subcommand accepts one or more “skill directory” paths.\nEach path may be either:\n\nA single skill root (it contains SKILL.md)\nA directory that contains multiple skill roots as immediate children\n\nExample layout:\nskills/\n  pdf-processing/\n    SKILL.md\n    scripts/\n    references/\n  code-review/\n    SKILL.md\n\n\n\nAdd an exec tool that bakes in the skill directories:\ntools:\n  - name: skills\n    exec: lectic skills ./skills $LECTIC_DATA/skills\n    usage: exec:lectic skills --prompt ./skills $LECTIC_DATA/skills\nThis creates a single tool (named skills) that the model can use to:\n\nactivate &lt;name&gt; (load instructions)\nread &lt;name&gt; &lt;path&gt; (load reference content)\nrun &lt;name&gt; &lt;script&gt; ... (execute bundled scripts)\n\nThe discovery list (name + description for each skill) is included in the tool usage text generated by --prompt, so the model usually does not need to call list.\n\n\n\n\nThe model activates a skill when relevant:\n\n&lt;tool-call with=\"skills\"&gt;\n&lt;arguments&gt;&lt;argv&gt;[\"activate\",\"pdf-processing\"]&lt;/argv&gt;&lt;/arguments&gt;\n...&lt;/tool-call&gt;\n\nIt reads references mentioned by SKILL.md:\n\n&lt;tool-call with=\"skills\"&gt;\n&lt;arguments&gt;&lt;argv&gt;[\"read\",\"pdf-processing\",\"references/REFERENCE.md\"]&lt;/argv&gt;&lt;/arguments&gt;\n...&lt;/tool-call&gt;\n\nIt runs a script bundled with the skill:\n\n&lt;tool-call with=\"skills\"&gt;\n&lt;arguments&gt;&lt;argv&gt;[\"run\",\"pdf-processing\",\"extract.py\",\"--input\",\"a.pdf\"]&lt;/argv&gt;&lt;/arguments&gt;\n...&lt;/tool-call&gt;\n\n\n\nThe subcommand is a small CLI wrapper around the skills folder format:\n\nIt scans the provided directories for SKILL.md files.\nIt parses the YAML frontmatter to build the discovery list.\nactivate returns the full skill instructions on demand.\nread enforces path containment (no .. escapes) and has a size cap.\nrun executes files in scripts/ only, with the skill root as the working directory.\n\n\n\n\nRunning skill scripts is equivalent to granting the LLM another exec tool. In real use, you should combine this with one or more of:\n\nA tool_use_pre confirmation hook\nA sandbox wrapper (Bubblewrap, nsjail, etc.)\n\nSee Custom Sandboxing for practical options.",
    "crumbs": [
      "Cookbook",
      "Agent Skills Support"
    ]
  },
  {
    "objectID": "cookbook/08_skills_subcommand.html#what-are-agent-skills",
    "href": "cookbook/08_skills_subcommand.html#what-are-agent-skills",
    "title": "Recipe: Agent Skills Support",
    "section": "",
    "text": "Agent Skills is an open format for packaging reusable capabilities that LLMs can load on demand. A skill is a folder containing:\n\nSKILL.md: Instructions for the LLM (with YAML frontmatter for metadata)\nscripts/: Executable scripts the LLM can run\nreferences/: Documentation and examples the LLM can read\n\nThe format emphasizes “progressive disclosure”—the LLM sees only skill names and descriptions initially, then loads full instructions only when needed. This keeps prompts small while giving the LLM access to a large library of capabilities.",
    "crumbs": [
      "Cookbook",
      "Agent Skills Support"
    ]
  },
  {
    "objectID": "cookbook/08_skills_subcommand.html#goal",
    "href": "cookbook/08_skills_subcommand.html#goal",
    "title": "Recipe: Agent Skills Support",
    "section": "",
    "text": "We’ll build a subcommand that gives your LLM access to a library of skills while keeping the base prompt small. It follows the skills spec’s “progressive disclosure” approach:\n\nDiscovery: load only each skill’s name + description.\nActivation: load the full SKILL.md instructions for one skill.\nResources: read reference files or run scripts only when needed.",
    "crumbs": [
      "Cookbook",
      "Agent Skills Support"
    ]
  },
  {
    "objectID": "cookbook/08_skills_subcommand.html#the-subcommand",
    "href": "cookbook/08_skills_subcommand.html#the-subcommand",
    "title": "Recipe: Agent Skills Support",
    "section": "",
    "text": "Lectic resolves lectic &lt;command&gt; using git-style subcommands. We will create an executable named lectic-skills.ts (TypeScript), using Lectic’s built-in script interpreter.\nCopy the repository’s extra/lectic-skills.ts to your config directory:\ncp /path/to/lectic/extra/lectic-skills.ts ~/.config/lectic/lectic-skills.ts\nchmod +x ~/.config/lectic/lectic-skills.ts\nNow lectic skills should resolve to that file.",
    "crumbs": [
      "Cookbook",
      "Agent Skills Support"
    ]
  },
  {
    "objectID": "cookbook/08_skills_subcommand.html#skill-directories",
    "href": "cookbook/08_skills_subcommand.html#skill-directories",
    "title": "Recipe: Agent Skills Support",
    "section": "",
    "text": "The subcommand accepts one or more “skill directory” paths.\nEach path may be either:\n\nA single skill root (it contains SKILL.md)\nA directory that contains multiple skill roots as immediate children\n\nExample layout:\nskills/\n  pdf-processing/\n    SKILL.md\n    scripts/\n    references/\n  code-review/\n    SKILL.md",
    "crumbs": [
      "Cookbook",
      "Agent Skills Support"
    ]
  },
  {
    "objectID": "cookbook/08_skills_subcommand.html#configure-it-as-an-exec-tool",
    "href": "cookbook/08_skills_subcommand.html#configure-it-as-an-exec-tool",
    "title": "Recipe: Agent Skills Support",
    "section": "",
    "text": "Add an exec tool that bakes in the skill directories:\ntools:\n  - name: skills\n    exec: lectic skills ./skills $LECTIC_DATA/skills\n    usage: exec:lectic skills --prompt ./skills $LECTIC_DATA/skills\nThis creates a single tool (named skills) that the model can use to:\n\nactivate &lt;name&gt; (load instructions)\nread &lt;name&gt; &lt;path&gt; (load reference content)\nrun &lt;name&gt; &lt;script&gt; ... (execute bundled scripts)\n\nThe discovery list (name + description for each skill) is included in the tool usage text generated by --prompt, so the model usually does not need to call list.",
    "crumbs": [
      "Cookbook",
      "Agent Skills Support"
    ]
  },
  {
    "objectID": "cookbook/08_skills_subcommand.html#example-workflow",
    "href": "cookbook/08_skills_subcommand.html#example-workflow",
    "title": "Recipe: Agent Skills Support",
    "section": "",
    "text": "The model activates a skill when relevant:\n\n&lt;tool-call with=\"skills\"&gt;\n&lt;arguments&gt;&lt;argv&gt;[\"activate\",\"pdf-processing\"]&lt;/argv&gt;&lt;/arguments&gt;\n...&lt;/tool-call&gt;\n\nIt reads references mentioned by SKILL.md:\n\n&lt;tool-call with=\"skills\"&gt;\n&lt;arguments&gt;&lt;argv&gt;[\"read\",\"pdf-processing\",\"references/REFERENCE.md\"]&lt;/argv&gt;&lt;/arguments&gt;\n...&lt;/tool-call&gt;\n\nIt runs a script bundled with the skill:\n\n&lt;tool-call with=\"skills\"&gt;\n&lt;arguments&gt;&lt;argv&gt;[\"run\",\"pdf-processing\",\"extract.py\",\"--input\",\"a.pdf\"]&lt;/argv&gt;&lt;/arguments&gt;\n...&lt;/tool-call&gt;",
    "crumbs": [
      "Cookbook",
      "Agent Skills Support"
    ]
  },
  {
    "objectID": "cookbook/08_skills_subcommand.html#how-its-built",
    "href": "cookbook/08_skills_subcommand.html#how-its-built",
    "title": "Recipe: Agent Skills Support",
    "section": "",
    "text": "The subcommand is a small CLI wrapper around the skills folder format:\n\nIt scans the provided directories for SKILL.md files.\nIt parses the YAML frontmatter to build the discovery list.\nactivate returns the full skill instructions on demand.\nread enforces path containment (no .. escapes) and has a size cap.\nrun executes files in scripts/ only, with the skill root as the working directory.",
    "crumbs": [
      "Cookbook",
      "Agent Skills Support"
    ]
  },
  {
    "objectID": "cookbook/08_skills_subcommand.html#safety-notes",
    "href": "cookbook/08_skills_subcommand.html#safety-notes",
    "title": "Recipe: Agent Skills Support",
    "section": "",
    "text": "Running skill scripts is equivalent to granting the LLM another exec tool. In real use, you should combine this with one or more of:\n\nA tool_use_pre confirmation hook\nA sandbox wrapper (Bubblewrap, nsjail, etc.)\n\nSee Custom Sandboxing for practical options.",
    "crumbs": [
      "Cookbook",
      "Agent Skills Support"
    ]
  },
  {
    "objectID": "cookbook/07_control_flow_macros.html",
    "href": "cookbook/07_control_flow_macros.html",
    "title": "Control Flow with Macros",
    "section": "",
    "text": "NoteAdvanced Recipe\n\n\n\nThis recipe demonstrates advanced macro techniques. Most Lectic workflows don’t need recursive macros or control flow constructs—a simple exec tool or script usually suffices. But if you want to see how far the macro system can go, read on.\n\n\nBecause Lectic’s macros support recursion and can execute scripts during the expansion phase, it is possible to build powerful control flow structures like conditionals, loops, and maps.\nThis guide demonstrates how to implement these constructs. While complex logic is often better handled by writing a custom tool or script, these examples show the flexibility of the macro system.\n\n\nThe key to control flow is the pre phase of macro expansion. (See Automation: Macros).\nBecause the result of a pre expansion is itself recursively expanded, a macro can return a new instance of itself with different arguments, effectively creating a loop.\nAdditionally, because pre expansions can run shell scripts (exec:), they can make decisions based on arguments or environment variables.\n\n\n\nA simple conditional macro evaluates a condition and outputs either its content (the “then” block) or an alternative (the “else” block).\nDefinition:\nmacros:\n  - name: if\n    post: |\n      exec:#!/bin/bash\n      if [ \"$ARG\" = \"true\" ]; then\n        echo \"$THEN\"\n      else\n        echo \"$ELSE\"\n      fi\nUsage:\n:if[true]{THEN=\"This is displayed if true\" ELSE=\"This is displayed if false\"}\n:if[false]{THEN=\"This is hidden if not true\" ELSE=\"This is shown instead\"}\n:if[:some_check[]]{THEN=\"This is hidden if not true\" ELSE=\"This is shown instead\"}\n\n\n\nThe previous example required passing the content as attributes (THEN=\"...\"), which is clumsy for large blocks of text. More importantly, if we want to conditionally run a command, we need to prevent it from executing at all unless the condition is met.\nIf we use the post phase, the children are expanded before the parent macro. To achieve “short-circuiting” (where the children are only expanded if the condition is true), we can use the pre phase of macro expansion.\nDefinition:\nmacros:\n  - name: when\n    # In the 'pre' phase, ARG contains the raw, unexpanded body text.\n    pre: |\n      exec:#!/bin/bash\n      if [ \"$CONDITION\" = \"true\" ]; then\n        # Return the body to be expanded\n        echo \"$ARG\"\n      else\n        # Return a comment (effectively deleting the block)\n        echo \"&lt;!-- skipped --&gt;\"\n      fi\nUsage:\n:when[\n  This content is only processed if the condition is met.\n  :cmd[echo \"Expensive operation running...\"]\n]{CONDITION=\"false\"}\nIn this example the expensive :cmd is never expanded or executed.\n\n\n\nBy having a macro call itself, we can create loops. We need a termination condition to stop the recursion (preventing an infinite loop).\nDefinition:\nmacros:\n  - name: countdown\n    pre: |\n      exec:#!/bin/bash\n      N=${ARG:-10}\n      if [ \"$N\" -gt 0 ]; then\n        echo \"$N...\"\n        # Recursive call with N-1\n        echo \":countdown[$((N-1))]\"\n      else\n        echo \"Liftoff!\"\n      fi\nUsage:\n:countdown[3]\nOutput:\n3...\n2...\n1...\nLiftoff!\n\n\n\nWe can iterate over a list of items and apply another macro to each one. This is useful for batch processing files, names, or data.\nThis implementation assumes a space-separated list of items.\nDefinition:\nmacros:\n  - name: map\n    pre: |\n      exec:#!/bin/bash\n      # Split ARG into array (space separated)\n      items=($ARG)\n      \n      # Termination: if no items, stop\n      if [ ${#items[@]} -eq 0 ]; then\n          echo \"&lt;!-- --&gt;\"\n          exit 0\n      fi\n      \n      # Head: The first item\n      first=${items[0]}\n      \n      # Tail: The rest of the items\n      rest=${items[@]:1}\n      \n      # 1. Apply the target macro to the first item\n      echo \":$MACRO[$first]\"\n      \n      # 2. Recurse on the rest (if any)\n      if [ -n \"$rest\" ]; then\n         echo \":map[$rest]{MACRO=$MACRO}\"\n      fi\nUsage:\nSuppose you have a macro greet defined:\nmacros:\n  - name: greet\n    expansion: \"Hello, $ARG! \"\nYou can map it over a list of names:\n:map[Alice Bob Charlie]{MACRO=\"greet\"}\nOutput:\nHello, Alice! Hello, Bob! Hello, Charlie! \n\n\n\nLet’s combine these concepts into a “Launch Sequence” generator. We want to check a list of systems, and if they are all go, initiate a countdown.\nConfiguration:\nmacros:\n  - name: launch_sequence\n    expansion: |\n      # Check systems\n      :map[Propulsion Guidance Life-Support]{MACRO=\"check_system\"}\n      \n      # Start countdown\n      :countdown[5]\n\n  - name: check_system\n    expansion: \"Checking $ARG... OK.\\n\"\n\n  - name: map\n    pre: |\n      exec:#!/bin/bash\n      items=($ARG)\n      if [ ${#items[@]} -eq 0 ]; then echo \"&lt;!-- --&gt;\"; exit 0; fi\n      first=${items[0]}\n      rest=${items[@]:1}\n      echo \":$MACRO[$first]\"\n      if [ -n \"$rest\" ]; then echo \":map[$rest]{MACRO=$MACRO}\"; fi\n\n  - name: countdown\n    pre: |\n      exec:#!/bin/bash\n      N=${ARG:-10}\n      if [ \"$N\" -gt 0 ]; then\n        echo \"$N...\"\n        echo \":countdown[$((N-1))]\"\n      else\n        echo \"Liftoff!\"\n      fi\nUsage:\n:launch_sequence[]\nOutput:\nChecking Propulsion... OK.\nChecking Guidance... OK.\nChecking Life-Support... OK.\n5...\n4...\n3...\n2...\n1...\nLiftoff!\n\n\n\n\n\n\nNoteRecursion Limit\n\n\n\nLectic has a recursion depth limit (default 100) to prevent infinite loops from crashing the process. If your loop needs to run more than 100 times, you should probably use an external script (exec:) instead of a recursive macro.",
    "crumbs": [
      "Cookbook",
      "Control Flow With Macros"
    ]
  },
  {
    "objectID": "cookbook/07_control_flow_macros.html#the-mechanism-recursion-pre",
    "href": "cookbook/07_control_flow_macros.html#the-mechanism-recursion-pre",
    "title": "Control Flow with Macros",
    "section": "",
    "text": "The key to control flow is the pre phase of macro expansion. (See Automation: Macros).\nBecause the result of a pre expansion is itself recursively expanded, a macro can return a new instance of itself with different arguments, effectively creating a loop.\nAdditionally, because pre expansions can run shell scripts (exec:), they can make decisions based on arguments or environment variables.",
    "crumbs": [
      "Cookbook",
      "Control Flow With Macros"
    ]
  },
  {
    "objectID": "cookbook/07_control_flow_macros.html#recipe-1-conditional-if",
    "href": "cookbook/07_control_flow_macros.html#recipe-1-conditional-if",
    "title": "Control Flow with Macros",
    "section": "",
    "text": "A simple conditional macro evaluates a condition and outputs either its content (the “then” block) or an alternative (the “else” block).\nDefinition:\nmacros:\n  - name: if\n    post: |\n      exec:#!/bin/bash\n      if [ \"$ARG\" = \"true\" ]; then\n        echo \"$THEN\"\n      else\n        echo \"$ELSE\"\n      fi\nUsage:\n:if[true]{THEN=\"This is displayed if true\" ELSE=\"This is displayed if false\"}\n:if[false]{THEN=\"This is hidden if not true\" ELSE=\"This is shown instead\"}\n:if[:some_check[]]{THEN=\"This is hidden if not true\" ELSE=\"This is shown instead\"}",
    "crumbs": [
      "Cookbook",
      "Control Flow With Macros"
    ]
  },
  {
    "objectID": "cookbook/07_control_flow_macros.html#recipe-2-short-circuiting-conditional-when",
    "href": "cookbook/07_control_flow_macros.html#recipe-2-short-circuiting-conditional-when",
    "title": "Control Flow with Macros",
    "section": "",
    "text": "The previous example required passing the content as attributes (THEN=\"...\"), which is clumsy for large blocks of text. More importantly, if we want to conditionally run a command, we need to prevent it from executing at all unless the condition is met.\nIf we use the post phase, the children are expanded before the parent macro. To achieve “short-circuiting” (where the children are only expanded if the condition is true), we can use the pre phase of macro expansion.\nDefinition:\nmacros:\n  - name: when\n    # In the 'pre' phase, ARG contains the raw, unexpanded body text.\n    pre: |\n      exec:#!/bin/bash\n      if [ \"$CONDITION\" = \"true\" ]; then\n        # Return the body to be expanded\n        echo \"$ARG\"\n      else\n        # Return a comment (effectively deleting the block)\n        echo \"&lt;!-- skipped --&gt;\"\n      fi\nUsage:\n:when[\n  This content is only processed if the condition is met.\n  :cmd[echo \"Expensive operation running...\"]\n]{CONDITION=\"false\"}\nIn this example the expensive :cmd is never expanded or executed.",
    "crumbs": [
      "Cookbook",
      "Control Flow With Macros"
    ]
  },
  {
    "objectID": "cookbook/07_control_flow_macros.html#recipe-3-recursion-loops-countdown",
    "href": "cookbook/07_control_flow_macros.html#recipe-3-recursion-loops-countdown",
    "title": "Control Flow with Macros",
    "section": "",
    "text": "By having a macro call itself, we can create loops. We need a termination condition to stop the recursion (preventing an infinite loop).\nDefinition:\nmacros:\n  - name: countdown\n    pre: |\n      exec:#!/bin/bash\n      N=${ARG:-10}\n      if [ \"$N\" -gt 0 ]; then\n        echo \"$N...\"\n        # Recursive call with N-1\n        echo \":countdown[$((N-1))]\"\n      else\n        echo \"Liftoff!\"\n      fi\nUsage:\n:countdown[3]\nOutput:\n3...\n2...\n1...\nLiftoff!",
    "crumbs": [
      "Cookbook",
      "Control Flow With Macros"
    ]
  },
  {
    "objectID": "cookbook/07_control_flow_macros.html#recipe-4-iteration-map",
    "href": "cookbook/07_control_flow_macros.html#recipe-4-iteration-map",
    "title": "Control Flow with Macros",
    "section": "",
    "text": "We can iterate over a list of items and apply another macro to each one. This is useful for batch processing files, names, or data.\nThis implementation assumes a space-separated list of items.\nDefinition:\nmacros:\n  - name: map\n    pre: |\n      exec:#!/bin/bash\n      # Split ARG into array (space separated)\n      items=($ARG)\n      \n      # Termination: if no items, stop\n      if [ ${#items[@]} -eq 0 ]; then\n          echo \"&lt;!-- --&gt;\"\n          exit 0\n      fi\n      \n      # Head: The first item\n      first=${items[0]}\n      \n      # Tail: The rest of the items\n      rest=${items[@]:1}\n      \n      # 1. Apply the target macro to the first item\n      echo \":$MACRO[$first]\"\n      \n      # 2. Recurse on the rest (if any)\n      if [ -n \"$rest\" ]; then\n         echo \":map[$rest]{MACRO=$MACRO}\"\n      fi\nUsage:\nSuppose you have a macro greet defined:\nmacros:\n  - name: greet\n    expansion: \"Hello, $ARG! \"\nYou can map it over a list of names:\n:map[Alice Bob Charlie]{MACRO=\"greet\"}\nOutput:\nHello, Alice! Hello, Bob! Hello, Charlie!",
    "crumbs": [
      "Cookbook",
      "Control Flow With Macros"
    ]
  },
  {
    "objectID": "cookbook/07_control_flow_macros.html#fun-example-the-launch-sequence",
    "href": "cookbook/07_control_flow_macros.html#fun-example-the-launch-sequence",
    "title": "Control Flow with Macros",
    "section": "",
    "text": "Let’s combine these concepts into a “Launch Sequence” generator. We want to check a list of systems, and if they are all go, initiate a countdown.\nConfiguration:\nmacros:\n  - name: launch_sequence\n    expansion: |\n      # Check systems\n      :map[Propulsion Guidance Life-Support]{MACRO=\"check_system\"}\n      \n      # Start countdown\n      :countdown[5]\n\n  - name: check_system\n    expansion: \"Checking $ARG... OK.\\n\"\n\n  - name: map\n    pre: |\n      exec:#!/bin/bash\n      items=($ARG)\n      if [ ${#items[@]} -eq 0 ]; then echo \"&lt;!-- --&gt;\"; exit 0; fi\n      first=${items[0]}\n      rest=${items[@]:1}\n      echo \":$MACRO[$first]\"\n      if [ -n \"$rest\" ]; then echo \":map[$rest]{MACRO=$MACRO}\"; fi\n\n  - name: countdown\n    pre: |\n      exec:#!/bin/bash\n      N=${ARG:-10}\n      if [ \"$N\" -gt 0 ]; then\n        echo \"$N...\"\n        echo \":countdown[$((N-1))]\"\n      else\n        echo \"Liftoff!\"\n      fi\nUsage:\n:launch_sequence[]\nOutput:\nChecking Propulsion... OK.\nChecking Guidance... OK.\nChecking Life-Support... OK.\n5...\n4...\n3...\n2...\n1...\nLiftoff!\n\n\n\n\n\n\nNoteRecursion Limit\n\n\n\nLectic has a recursion depth limit (default 100) to prevent infinite loops from crashing the process. If your loop needs to run more than 100 times, you should probably use an external script (exec:) instead of a recursive macro.",
    "crumbs": [
      "Cookbook",
      "Control Flow With Macros"
    ]
  },
  {
    "objectID": "cookbook/01_coding_assistant.html",
    "href": "cookbook/01_coding_assistant.html",
    "title": "Recipe: Coding Assistant",
    "section": "",
    "text": "This recipe shows how to set up an agentic coding assistant with shell tools, type checking, and a confirmation dialog before tool execution.\n\n\nWe’ll give the assistant access to:\n\nFile reading and writing\nRunning TypeScript compiler and linter\nExecuting shell commands (with confirmation)\n\n\n\nCreate a lectic.yaml in your project root:\ninterlocutor:\n  name: Assistant\n  prompt: |\n    You are a senior software engineer helping with this codebase.\n    \n    When making changes:\n    1. Read relevant files first to understand context\n    2. Make minimal, focused changes\n    3. Run tsc and eslint after edits to catch errors\n    4. Explain your reasoning\n  provider: anthropic\n  model: claude-sonnet-4-20250514\n  tools:\n    - exec: cat\n      name: read_file\n      usage: Read a file. Pass the file path as an argument.\n    - name: write_file\n      usage: Write content to a file. \n      exec: |\n        #!/bin/bash\n        cat &gt; \"$FILE_PATH\"\n      schema:\n        FILE_PATH: The path to write to.\n        CONTENT: The content to write (passed via stdin).\n    - exec: tsc --noEmit\n      name: typecheck\n      usage: Run the TypeScript compiler to check for type errors.\n    - exec: eslint\n      name: lint\n      usage: Run ESLint on files. Pass file paths as arguments.\n    - exec: bash -c\n      name: shell\n      usage: Run a shell command. Use for git, grep, find, etc.\n\nhooks:\n  - on: tool_use_pre\n    do: ~/.config/lectic/confirm.sh\n\n\n\nFor graphical environments, create ~/.config/lectic/confirm.sh:\n#!/bin/bash\n# Requires: zenity (GTK) or kdialog (KDE)\n\n# Skip confirmation for read-only tools\ncase \"$TOOL_NAME\" in\n  read_file|typecheck|lint)\n    exit 0\n    ;;\nesac\n\n# Show confirmation dialog\nzenity --question \\\n  --title=\"Allow tool use?\" \\\n  --text=\"Tool: $TOOL_NAME\\n\\nArguments:\\n$TOOL_ARGS\" \\\n  --width=400\n\nexit $?\nMake it executable: chmod +x ~/.config/lectic/confirm.sh\n\n\n\n\n\n\nNote\n\n\n\nThe confirmation hook runs as a subprocess without access to a terminal, so interactive terminal prompts (like read -p) won’t work. Use a GUI dialog tool like zenity, kdialog, or osascript on macOS.\n\n\n\n\n\n\nCreate a conversation file in your project:\n---\n# Uses lectic.yaml from project root\n---\n\nI need to add input validation to the `processUser` function in \nsrc/users.ts. It should reject empty names and invalid email formats.\nRun it:\nlectic -i task.lec\nThe assistant will:\n\nRead src/users.ts to understand the current implementation\nPropose changes (you’ll see a confirmation dialog for writes)\nRun tsc and eslint to verify the changes\nReport results\n\n\n\n\n\n\nRemove write and shell tools for a safer setup that can only read and analyze:\ntools:\n  - exec: cat\n    name: read_file\n  - exec: rg --json\n    name: search\n    usage: Search with ripgrep. Pass pattern and optional path.\n  - exec: tsc --noEmit\n    name: typecheck\n\n\n\nFor stronger isolation, use the bubblewrap sandbox included in the repository at extra/sandbox/bwrap-sandbox.sh:\ntools:\n  - exec: bash -c\n    name: shell\n    sandbox: ./extra/sandbox/bwrap-sandbox.sh\nThe sandbox script uses Bubblewrap to run commands in an isolated environment. It:\n\nCreates a temporary home directory that’s discarded after execution\nMounts the current working directory read-write\nProvides read-only access to essential system paths (/usr, /bin, etc.)\nBlocks network access by default\n\nYou can copy the script to your config directory and modify it to suit your needs — for example, to allow network access or mount additional paths.\n\n\n\nAdd a hook to notify you when the assistant finishes working:\nhooks:\n  - on: tool_use_pre\n    do: ~/.config/lectic/confirm.sh\n  - on: assistant_message\n    do: |\n      #!/bin/bash\n      if [[ \"$TOOL_USE_DONE\" == \"1\" ]]; then\n        notify-send \"Lectic\" \"Task complete\"\n      fi",
    "crumbs": [
      "Cookbook",
      "Coding Assistant"
    ]
  },
  {
    "objectID": "cookbook/01_coding_assistant.html#the-setup",
    "href": "cookbook/01_coding_assistant.html#the-setup",
    "title": "Recipe: Coding Assistant",
    "section": "",
    "text": "We’ll give the assistant access to:\n\nFile reading and writing\nRunning TypeScript compiler and linter\nExecuting shell commands (with confirmation)\n\n\n\nCreate a lectic.yaml in your project root:\ninterlocutor:\n  name: Assistant\n  prompt: |\n    You are a senior software engineer helping with this codebase.\n    \n    When making changes:\n    1. Read relevant files first to understand context\n    2. Make minimal, focused changes\n    3. Run tsc and eslint after edits to catch errors\n    4. Explain your reasoning\n  provider: anthropic\n  model: claude-sonnet-4-20250514\n  tools:\n    - exec: cat\n      name: read_file\n      usage: Read a file. Pass the file path as an argument.\n    - name: write_file\n      usage: Write content to a file. \n      exec: |\n        #!/bin/bash\n        cat &gt; \"$FILE_PATH\"\n      schema:\n        FILE_PATH: The path to write to.\n        CONTENT: The content to write (passed via stdin).\n    - exec: tsc --noEmit\n      name: typecheck\n      usage: Run the TypeScript compiler to check for type errors.\n    - exec: eslint\n      name: lint\n      usage: Run ESLint on files. Pass file paths as arguments.\n    - exec: bash -c\n      name: shell\n      usage: Run a shell command. Use for git, grep, find, etc.\n\nhooks:\n  - on: tool_use_pre\n    do: ~/.config/lectic/confirm.sh\n\n\n\nFor graphical environments, create ~/.config/lectic/confirm.sh:\n#!/bin/bash\n# Requires: zenity (GTK) or kdialog (KDE)\n\n# Skip confirmation for read-only tools\ncase \"$TOOL_NAME\" in\n  read_file|typecheck|lint)\n    exit 0\n    ;;\nesac\n\n# Show confirmation dialog\nzenity --question \\\n  --title=\"Allow tool use?\" \\\n  --text=\"Tool: $TOOL_NAME\\n\\nArguments:\\n$TOOL_ARGS\" \\\n  --width=400\n\nexit $?\nMake it executable: chmod +x ~/.config/lectic/confirm.sh\n\n\n\n\n\n\nNote\n\n\n\nThe confirmation hook runs as a subprocess without access to a terminal, so interactive terminal prompts (like read -p) won’t work. Use a GUI dialog tool like zenity, kdialog, or osascript on macOS.",
    "crumbs": [
      "Cookbook",
      "Coding Assistant"
    ]
  },
  {
    "objectID": "cookbook/01_coding_assistant.html#usage",
    "href": "cookbook/01_coding_assistant.html#usage",
    "title": "Recipe: Coding Assistant",
    "section": "",
    "text": "Create a conversation file in your project:\n---\n# Uses lectic.yaml from project root\n---\n\nI need to add input validation to the `processUser` function in \nsrc/users.ts. It should reject empty names and invalid email formats.\nRun it:\nlectic -i task.lec\nThe assistant will:\n\nRead src/users.ts to understand the current implementation\nPropose changes (you’ll see a confirmation dialog for writes)\nRun tsc and eslint to verify the changes\nReport results",
    "crumbs": [
      "Cookbook",
      "Coding Assistant"
    ]
  },
  {
    "objectID": "cookbook/01_coding_assistant.html#variations",
    "href": "cookbook/01_coding_assistant.html#variations",
    "title": "Recipe: Coding Assistant",
    "section": "",
    "text": "Remove write and shell tools for a safer setup that can only read and analyze:\ntools:\n  - exec: cat\n    name: read_file\n  - exec: rg --json\n    name: search\n    usage: Search with ripgrep. Pass pattern and optional path.\n  - exec: tsc --noEmit\n    name: typecheck\n\n\n\nFor stronger isolation, use the bubblewrap sandbox included in the repository at extra/sandbox/bwrap-sandbox.sh:\ntools:\n  - exec: bash -c\n    name: shell\n    sandbox: ./extra/sandbox/bwrap-sandbox.sh\nThe sandbox script uses Bubblewrap to run commands in an isolated environment. It:\n\nCreates a temporary home directory that’s discarded after execution\nMounts the current working directory read-write\nProvides read-only access to essential system paths (/usr, /bin, etc.)\nBlocks network access by default\n\nYou can copy the script to your config directory and modify it to suit your needs — for example, to allow network access or mount additional paths.\n\n\n\nAdd a hook to notify you when the assistant finishes working:\nhooks:\n  - on: tool_use_pre\n    do: ~/.config/lectic/confirm.sh\n  - on: assistant_message\n    do: |\n      #!/bin/bash\n      if [[ \"$TOOL_USE_DONE\" == \"1\" ]]; then\n        notify-send \"Lectic\" \"Task complete\"\n      fi",
    "crumbs": [
      "Cookbook",
      "Coding Assistant"
    ]
  },
  {
    "objectID": "cookbook/02_commit_messages.html",
    "href": "cookbook/02_commit_messages.html",
    "title": "Recipe: Git Commit Messages",
    "section": "",
    "text": "This recipe creates a custom lectic commit subcommand that generates commit messages from your staged changes.\n\n\nLectic looks for executables named lectic-&lt;command&gt; in your config directory, data directory, or PATH. Create ~/.config/lectic/lectic-commit:\n#!/bin/bash\nset -euo pipefail\n\n# Check for staged changes\nif git diff --cached --quiet; then\n  echo \"No staged changes\" &gt;&2\n  exit 1\nfi\n\nlectic -f ~/.config/lectic/commit-prompt.lec -S\nMake it executable:\nchmod +x ~/.config/lectic/lectic-commit\n\n\n\nCreate ~/.config/lectic/commit-prompt.lec:\n---\ninterlocutor:\n  name: Assistant\n  prompt: |\n    You write git commit messages following the Conventional Commits\n    specification. Output ONLY the commit message, nothing else.\n    \n    Format:\n    &lt;type&gt;[optional scope]: &lt;description&gt;\n    \n    [optional body]\n    \n    Types: feat, fix, docs, style, refactor, perf, test, chore\n    \n    Rules:\n    - Subject line max 50 characters\n    - Use imperative mood (\"add\" not \"added\")\n    - Body wraps at 72 characters\n    - Explain what and why, not how\n  provider: anthropic\n  model: claude-3-haiku-20240307\n  max_tokens: 500\n---\n\nWrite a commit message for this diff:\n\n:cmd[git diff --cached]\nThe :cmd[git diff --cached] directive runs git diff --cached and includes the output in the context sent to the LLM.\n\n\n\nStage your changes and run:\ngit add -p\nlectic commit\nOutput:\nfeat(auth): add password strength validation\n\nImplement zxcvbn-based password strength checking during registration.\nReject passwords scoring below 3 and display feedback to users.\n\n\nlectic commit | git commit -F -\n\n\n\nlectic commit | git commit -eF -\n\n\n\n\n\n\nModify the prompt to show recent history:\nRecent commits for context:\n:cmd[git log --oneline -5]\n\nWrite a commit message for this diff:\n:cmd[git diff --cached]\n\n\n\nCreate multiple prompt files for different projects:\n\ncommit-prompt-conventional.lec — Conventional Commits\ncommit-prompt-gitmoji.lec — Gitmoji style\ncommit-prompt-simple.lec — Plain descriptions\n\nThen modify the subcommand to accept an argument:\n#!/bin/bash\nset -euo pipefail\n\nSTYLE=\"${1:-conventional}\"\nPROMPT=\"$HOME/.config/lectic/commit-prompt-$STYLE.lec\"\n\nif [[ ! -f \"$PROMPT\" ]]; then\n  echo \"Unknown style: $STYLE\" &gt;&2\n  exit 1\nfi\n\nif git diff --cached --quiet; then\n  echo \"No staged changes\" &gt;&2\n  exit 1\nfi\n\nlectic -f \"$PROMPT\" -S\nUsage: lectic commit gitmoji\n\n\n\nGenerate messages for each file separately:\n#!/bin/bash\nfor file in $(git diff --cached --name-only); do\n  echo \"=== $file ===\"\n  # Create a temporary prompt with just this file's diff\n  cat &gt; /tmp/commit-single.lec &lt;&lt; EOF\n---\ninterlocutor:\n  name: Assistant\n  prompt: Write a conventional commit message for this change. Output only the message.\n  model: claude-3-haiku-20240307\n  max_tokens: 200\n---\n\n:cmd[git diff --cached -- \"$file\"]\nEOF\n  lectic -f /tmp/commit-single.lec -S\n  echo\ndone",
    "crumbs": [
      "Cookbook",
      "Commit Messages"
    ]
  },
  {
    "objectID": "cookbook/02_commit_messages.html#the-subcommand",
    "href": "cookbook/02_commit_messages.html#the-subcommand",
    "title": "Recipe: Git Commit Messages",
    "section": "",
    "text": "Lectic looks for executables named lectic-&lt;command&gt; in your config directory, data directory, or PATH. Create ~/.config/lectic/lectic-commit:\n#!/bin/bash\nset -euo pipefail\n\n# Check for staged changes\nif git diff --cached --quiet; then\n  echo \"No staged changes\" &gt;&2\n  exit 1\nfi\n\nlectic -f ~/.config/lectic/commit-prompt.lec -S\nMake it executable:\nchmod +x ~/.config/lectic/lectic-commit",
    "crumbs": [
      "Cookbook",
      "Commit Messages"
    ]
  },
  {
    "objectID": "cookbook/02_commit_messages.html#the-prompt-template",
    "href": "cookbook/02_commit_messages.html#the-prompt-template",
    "title": "Recipe: Git Commit Messages",
    "section": "",
    "text": "Create ~/.config/lectic/commit-prompt.lec:\n---\ninterlocutor:\n  name: Assistant\n  prompt: |\n    You write git commit messages following the Conventional Commits\n    specification. Output ONLY the commit message, nothing else.\n    \n    Format:\n    &lt;type&gt;[optional scope]: &lt;description&gt;\n    \n    [optional body]\n    \n    Types: feat, fix, docs, style, refactor, perf, test, chore\n    \n    Rules:\n    - Subject line max 50 characters\n    - Use imperative mood (\"add\" not \"added\")\n    - Body wraps at 72 characters\n    - Explain what and why, not how\n  provider: anthropic\n  model: claude-3-haiku-20240307\n  max_tokens: 500\n---\n\nWrite a commit message for this diff:\n\n:cmd[git diff --cached]\nThe :cmd[git diff --cached] directive runs git diff --cached and includes the output in the context sent to the LLM.",
    "crumbs": [
      "Cookbook",
      "Commit Messages"
    ]
  },
  {
    "objectID": "cookbook/02_commit_messages.html#usage",
    "href": "cookbook/02_commit_messages.html#usage",
    "title": "Recipe: Git Commit Messages",
    "section": "",
    "text": "Stage your changes and run:\ngit add -p\nlectic commit\nOutput:\nfeat(auth): add password strength validation\n\nImplement zxcvbn-based password strength checking during registration.\nReject passwords scoring below 3 and display feedback to users.\n\n\nlectic commit | git commit -F -\n\n\n\nlectic commit | git commit -eF -",
    "crumbs": [
      "Cookbook",
      "Commit Messages"
    ]
  },
  {
    "objectID": "cookbook/02_commit_messages.html#variations",
    "href": "cookbook/02_commit_messages.html#variations",
    "title": "Recipe: Git Commit Messages",
    "section": "",
    "text": "Modify the prompt to show recent history:\nRecent commits for context:\n:cmd[git log --oneline -5]\n\nWrite a commit message for this diff:\n:cmd[git diff --cached]\n\n\n\nCreate multiple prompt files for different projects:\n\ncommit-prompt-conventional.lec — Conventional Commits\ncommit-prompt-gitmoji.lec — Gitmoji style\ncommit-prompt-simple.lec — Plain descriptions\n\nThen modify the subcommand to accept an argument:\n#!/bin/bash\nset -euo pipefail\n\nSTYLE=\"${1:-conventional}\"\nPROMPT=\"$HOME/.config/lectic/commit-prompt-$STYLE.lec\"\n\nif [[ ! -f \"$PROMPT\" ]]; then\n  echo \"Unknown style: $STYLE\" &gt;&2\n  exit 1\nfi\n\nif git diff --cached --quiet; then\n  echo \"No staged changes\" &gt;&2\n  exit 1\nfi\n\nlectic -f \"$PROMPT\" -S\nUsage: lectic commit gitmoji\n\n\n\nGenerate messages for each file separately:\n#!/bin/bash\nfor file in $(git diff --cached --name-only); do\n  echo \"=== $file ===\"\n  # Create a temporary prompt with just this file's diff\n  cat &gt; /tmp/commit-single.lec &lt;&lt; EOF\n---\ninterlocutor:\n  name: Assistant\n  prompt: Write a conventional commit message for this change. Output only the message.\n  model: claude-3-haiku-20240307\n  max_tokens: 200\n---\n\n:cmd[git diff --cached -- \"$file\"]\nEOF\n  lectic -f /tmp/commit-single.lec -S\n  echo\ndone",
    "crumbs": [
      "Cookbook",
      "Commit Messages"
    ]
  }
]